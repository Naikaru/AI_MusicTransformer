{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Music Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naikaru/AI_MusicTransformer/blob/master/Music_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9opKSK2RSDRg",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Artificial Intelligence Project ~ Music Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05hD19W0hSCP",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgImNNGWyTPd",
        "colab_type": "code",
        "outputId": "0c20c62d-d7cc-4699-d1a6-53abfd9850a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "gdrive_root = '/gdrive/My Drive'\n",
        "gdrive_data = '/gdrive/My Drive/my_data'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ror_UJUp7wlO",
        "colab_type": "code",
        "outputId": "b906a280-3e8e-463b-957c-1dc882adea52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwhwdGvU5snk",
        "colab_type": "code",
        "outputId": "6e8bdfbc-c1c7-493c-e21e-5bf090194c40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jsD-2R6OHtW",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3cHKI6gOGB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_model = 512\n",
        "nhead = 8\n",
        "dim_feedforward = 1024\n",
        "dropout = 0.2\n",
        "num_layer = 6\n",
        "batch_size = 8\n",
        "sequence_length = 1024\n",
        "warmup_steps = 4000\n",
        "pad_token = 1   \n",
        "# vocabulary_size = 388 # depends on the dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsOdPRHj4iw-",
        "colab_type": "code",
        "outputId": "710f2065-0c2b-4a53-facf-3e4597a3ad4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Import library for encoder function\n",
        "\n",
        "!ls /gdrive/My\\ Drive/my_data/library/processor.py\n",
        "!cat '/gdrive/My Drive/my_data/library/processor.py'\n",
        "\n",
        "sys.path.append('/gdrive/My Drive/my_data/library')\n",
        "\n",
        "from processor import encode_midi, decode_midi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/gdrive/My Drive/my_data/library/processor.py'\n",
            "import pretty_midi\n",
            "\n",
            "\n",
            "RANGE_NOTE_ON = 128\n",
            "RANGE_NOTE_OFF = 128\n",
            "RANGE_VEL = 32\n",
            "RANGE_TIME_SHIFT = 100\n",
            "\n",
            "START_IDX = {\n",
            "    'note_on': 0,\n",
            "    'note_off': RANGE_NOTE_ON,\n",
            "    'time_shift': RANGE_NOTE_ON + RANGE_NOTE_OFF,\n",
            "    'velocity': RANGE_NOTE_ON + RANGE_NOTE_OFF + RANGE_TIME_SHIFT\n",
            "}\n",
            "\n",
            "\n",
            "class SustainAdapter:\n",
            "    def __init__(self, time, type):\n",
            "        self.start =  time\n",
            "        self.type = type\n",
            "\n",
            "\n",
            "class SustainDownManager:\n",
            "    def __init__(self, start, end):\n",
            "        self.start = start\n",
            "        self.end = end\n",
            "        self.managed_notes = []\n",
            "        self._note_dict = {} # key: pitch, value: note.start\n",
            "\n",
            "    def add_managed_note(self, note: pretty_midi.Note):\n",
            "        self.managed_notes.append(note)\n",
            "\n",
            "    def transposition_notes(self):\n",
            "        for note in reversed(self.managed_notes):\n",
            "            try:\n",
            "                note.end = self._note_dict[note.pitch]\n",
            "            except KeyError:\n",
            "                note.end = max(self.end, note.end)\n",
            "            self._note_dict[note.pitch] = note.start\n",
            "\n",
            "\n",
            "# Divided note by note_on, note_off\n",
            "class SplitNote:\n",
            "    def __init__(self, type, time, value, velocity):\n",
            "        ## type: note_on, note_off\n",
            "        self.type = type\n",
            "        self.time = time\n",
            "        self.velocity = velocity\n",
            "        self.value = value\n",
            "\n",
            "    def __repr__(self):\n",
            "        return '<[SNote] time: {} type: {}, value: {}, velocity: {}>'\\\n",
            "            .format(self.time, self.type, self.value, self.velocity)\n",
            "\n",
            "\n",
            "class Event:\n",
            "    def __init__(self, event_type, value):\n",
            "        self.type = event_type\n",
            "        self.value = value\n",
            "\n",
            "    def __repr__(self):\n",
            "        return '<Event type: {}, value: {}>'.format(self.type, self.value)\n",
            "\n",
            "    def to_int(self):\n",
            "        return START_IDX[self.type] + self.value\n",
            "\n",
            "    @staticmethod\n",
            "    def from_int(int_value):\n",
            "        info = Event._type_check(int_value)\n",
            "        return Event(info['type'], info['value'])\n",
            "\n",
            "    @staticmethod\n",
            "    def _type_check(int_value):\n",
            "        range_note_on = range(0, RANGE_NOTE_ON)\n",
            "        range_note_off = range(RANGE_NOTE_ON, RANGE_NOTE_ON+RANGE_NOTE_OFF)\n",
            "        range_time_shift = range(RANGE_NOTE_ON+RANGE_NOTE_OFF,RANGE_NOTE_ON+RANGE_NOTE_OFF+RANGE_TIME_SHIFT)\n",
            "\n",
            "        valid_value = int_value\n",
            "\n",
            "        if int_value in range_note_on:\n",
            "            return {'type': 'note_on', 'value': valid_value}\n",
            "        elif int_value in range_note_off:\n",
            "            valid_value -= RANGE_NOTE_ON\n",
            "            return {'type': 'note_off', 'value': valid_value}\n",
            "        elif int_value in range_time_shift:\n",
            "            valid_value -= (RANGE_NOTE_ON + RANGE_NOTE_OFF)\n",
            "            return {'type': 'time_shift', 'value': valid_value}\n",
            "        else:\n",
            "            valid_value -= (RANGE_NOTE_ON + RANGE_NOTE_OFF + RANGE_TIME_SHIFT)\n",
            "            return {'type': 'velocity', 'value': valid_value}\n",
            "\n",
            "\n",
            "def _divide_note(notes):\n",
            "    result_array = []\n",
            "    notes.sort(key=lambda x: x.start)\n",
            "\n",
            "    for note in notes:\n",
            "        on = SplitNote('note_on', note.start, note.pitch, note.velocity)\n",
            "        off = SplitNote('note_off', note.end, note.pitch, None)\n",
            "        result_array += [on, off]\n",
            "    return result_array\n",
            "\n",
            "\n",
            "def _merge_note(snote_sequence):\n",
            "    note_on_dict = {}\n",
            "    result_array = []\n",
            "\n",
            "    for snote in snote_sequence:\n",
            "        # print(note_on_dict)\n",
            "        if snote.type == 'note_on':\n",
            "            note_on_dict[snote.value] = snote\n",
            "        elif snote.type == 'note_off':\n",
            "            try:\n",
            "                on = note_on_dict[snote.value]\n",
            "                off = snote\n",
            "                if off.time - on.time == 0:\n",
            "                    continue\n",
            "                result = pretty_midi.Note(on.velocity, snote.value, on.time, off.time)\n",
            "                result_array.append(result)\n",
            "            except:\n",
            "                print('info removed pitch: {}'.format(snote.value))\n",
            "    return result_array\n",
            "\n",
            "\n",
            "def _snote2events(snote: SplitNote, prev_vel: int):\n",
            "    result = []\n",
            "    if snote.velocity is not None:\n",
            "        modified_velocity = snote.velocity // 4\n",
            "        if prev_vel != modified_velocity:\n",
            "            result.append(Event(event_type='velocity', value=modified_velocity))\n",
            "    result.append(Event(event_type=snote.type, value=snote.value))\n",
            "    return result\n",
            "\n",
            "\n",
            "def _event_seq2snote_seq(event_sequence):\n",
            "    timeline = 0\n",
            "    velocity = 0\n",
            "    snote_seq = []\n",
            "\n",
            "    for event in event_sequence:\n",
            "        if event.type == 'time_shift':\n",
            "            timeline += ((event.value+1) / 100)\n",
            "        if event.type == 'velocity':\n",
            "            velocity = event.value * 4\n",
            "        else:\n",
            "            snote = SplitNote(event.type, timeline, event.value, velocity)\n",
            "            snote_seq.append(snote)\n",
            "    return snote_seq\n",
            "\n",
            "\n",
            "def _make_time_sift_events(prev_time, post_time):\n",
            "    time_interval = int(round((post_time - prev_time) * 100))\n",
            "    results = []\n",
            "    while time_interval >= RANGE_TIME_SHIFT:\n",
            "        results.append(Event(event_type='time_shift', value=RANGE_TIME_SHIFT-1))\n",
            "        time_interval -= RANGE_TIME_SHIFT\n",
            "    if time_interval == 0:\n",
            "        return results\n",
            "    else:\n",
            "        return results + [Event(event_type='time_shift', value=time_interval-1)]\n",
            "\n",
            "\n",
            "def _control_preprocess(ctrl_changes):\n",
            "    sustains = []\n",
            "\n",
            "    manager = None\n",
            "    for ctrl in ctrl_changes:\n",
            "        if ctrl.value >= 64 and manager is None:\n",
            "            # sustain down\n",
            "            manager = SustainDownManager(start=ctrl.time, end=None)\n",
            "        elif ctrl.value < 64 and manager is not None:\n",
            "            # sustain up\n",
            "            manager.end = ctrl.time\n",
            "            sustains.append(manager)\n",
            "            manager = None\n",
            "        elif ctrl.value < 64 and len(sustains) > 0:\n",
            "            sustains[-1].end = ctrl.time\n",
            "    return sustains\n",
            "\n",
            "\n",
            "def _note_preprocess(susteins, notes):\n",
            "    note_stream = []\n",
            "\n",
            "    for sustain in susteins:\n",
            "        for note_idx, note in enumerate(notes):\n",
            "            if note.start < sustain.start:\n",
            "                note_stream.append(note)\n",
            "            elif note.start > sustain.end:\n",
            "                notes = notes[note_idx:]\n",
            "                sustain.transposition_notes()\n",
            "                break\n",
            "            else:\n",
            "                sustain.add_managed_note(note)\n",
            "\n",
            "    for sustain in susteins:\n",
            "        note_stream += sustain.managed_notes\n",
            "\n",
            "    note_stream.sort(key= lambda x: x.start)\n",
            "    return note_stream\n",
            "\n",
            "\n",
            "def encode_midi(file_path):\n",
            "    events = []\n",
            "    notes = []\n",
            "    mid = pretty_midi.PrettyMIDI(midi_file=file_path)\n",
            "\n",
            "    for inst in mid.instruments:\n",
            "        inst_notes = inst.notes\n",
            "        # ctrl.number is the number of sustain control. If you want to know abour the number type of control,\n",
            "        # see https://www.midi.org/specifications-old/item/table-3-control-change-messages-data-bytes-2\n",
            "        ctrls = _control_preprocess([ctrl for ctrl in inst.control_changes if ctrl.number == 64])\n",
            "        notes += _note_preprocess(ctrls, inst_notes)\n",
            "\n",
            "    dnotes = _divide_note(notes)\n",
            "\n",
            "    # print(dnotes)\n",
            "    dnotes.sort(key=lambda x: x.time)\n",
            "    # print('sorted:')\n",
            "    # print(dnotes)\n",
            "    cur_time = 0\n",
            "    cur_vel = 0\n",
            "    for snote in dnotes:\n",
            "        events += _make_time_sift_events(prev_time=cur_time, post_time=snote.time)\n",
            "        events += _snote2events(snote=snote, prev_vel=cur_vel)\n",
            "        # events += _make_time_sift_events(prev_time=cur_time, post_time=snote.time)\n",
            "\n",
            "        cur_time = snote.time\n",
            "        cur_vel = snote.velocity\n",
            "\n",
            "    return [e.to_int() for e in events]\n",
            "\n",
            "\n",
            "def decode_midi(idx_array, file_path=None):\n",
            "    event_sequence = [Event.from_int(idx) for idx in idx_array]\n",
            "    # print(event_sequence)\n",
            "    snote_seq = _event_seq2snote_seq(event_sequence)\n",
            "    note_seq = _merge_note(snote_seq)\n",
            "    note_seq.sort(key=lambda x:x.start)\n",
            "\n",
            "    mid = pretty_midi.PrettyMIDI()\n",
            "    # if want to change instument, see https://www.midi.org/specifications/item/gm-level-1-sound-set\n",
            "    instument = pretty_midi.Instrument(1, False, \"Developed By Yang-Kichang\")\n",
            "    instument.notes = note_seq\n",
            "\n",
            "    mid.instruments.append(instument)\n",
            "    if file_path is not None:\n",
            "        mid.write(file_path)\n",
            "    return mid\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    encoded = encode_midi('bin/ADIG04.mid')\n",
            "    print(encoded)\n",
            "    decided = decode_midi(encoded,file_path='bin/test.mid')\n",
            "\n",
            "    ins = pretty_midi.PrettyMIDI('bin/ADIG04.mid')\n",
            "    print(ins)\n",
            "    print(ins.instruments[0])\n",
            "    for i in ins.instruments:\n",
            "        print(i.control_changes)\n",
            "        print(i.notes)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHOUe1Tz90ZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_midi_files(midi_dir_path, save_dir_path, extension):\n",
        "  #create directory for saving files\n",
        "  os.makedirs(save_dir_path, exist_ok=True)\n",
        "  #get all midi files from midi_directory\n",
        "  for file in os.listdir(midi_dir_path):\n",
        "    if file.endswith(tuple(extension)):\n",
        "        print(os.path.join(midi_dir_path, file))\n",
        "        print(file + ' is being processed', flush=True)\n",
        "        try:\n",
        "          encoded_file = encode_midi(midi_dir_path+file)\n",
        "        except KeyboardInterrupt:\n",
        "            print(' Stopped by keyboard')\n",
        "            return\n",
        "        except EOFError:\n",
        "            print('EOF Error')\n",
        "            return\n",
        "        with open(save_dir_path+file+'.encoded', 'wb') as f:\n",
        "            pickle.dump(encoded_file, f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3_Fd7tRDOOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "midi_dir_path = '/gdrive/My Drive/my_data/midi_files/'\n",
        "save_dir_path = '/gdrive/My Drive/my_data/encoded_midi/'\n",
        "train_dir_path = '/gdrive/My Drive/my_data/training_set/'\n",
        "test_dir_path = '/gdrive/My Drive/my_data/test_set/'\n",
        "extension = ['.mid', '.midi']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngsTYWHspwyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode_midi_files(midi_dir_path, save_dir_path, extension)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lMJCA75_fu9M",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "\n",
        "def create_dataset(save_dir_path, split_ratio=0.9):\n",
        "    dataset = [file for file in os.listdir(save_dir_path)]\n",
        "    np.random.shuffle(dataset)\n",
        "\n",
        "    train_set = dataset[:int(len(dataset) * split_ratio)]    \n",
        "    test_set = dataset[int(len(dataset) * split_ratio):]\n",
        "    \n",
        "    shutil.rmtree(train_dir_path)\n",
        "    shutil.rmtree(test_dir_path)\n",
        "\n",
        "    os.makedirs(train_dir_path, exist_ok=True)\n",
        "    os.makedirs(test_dir_path, exist_ok=True)\n",
        "\n",
        "    for file in os.listdir(save_dir_path):\n",
        "        if os.stat(save_dir_path+file).st_size != 0:\n",
        "            if file in test_set:\n",
        "                shutil.copyfile(save_dir_path+file, test_dir_path+file)\n",
        "            else:\n",
        "                shutil.copyfile(save_dir_path+file, train_dir_path+file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urSdAaicuJ2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create_dataset(save_dir_path, split_ratio=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LHbi7-HWsfmC",
        "colab": {}
      },
      "source": [
        "def load_dataset(train_dir_path, test_dir_path):\n",
        "    #load all encoded file\n",
        "    train_set = [file for file in os.listdir(train_dir_path)]\n",
        "    test_set = [file for file in os.listdir(test_dir_path)]\n",
        "\n",
        "    return train_set, test_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFNzOq0nsgMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_data, test_data = load_dataset(train_dir_path, test_dir_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7c3CzQEfwUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(dataset, dir_path, sequence_length=1024, batch_size=8):\n",
        "    sequence_length += 1\n",
        "    batch_midi = []\n",
        "    while len(batch_midi) < batch_size:\n",
        "        file = random.choice(dataset)\n",
        "        #if the midi contains more sequence that the sequence length\n",
        "        try:\n",
        "            with open(dir_path+file, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "        except:\n",
        "            print(dir_path+file + \" file not found.\")\n",
        "        if sequence_length <= len(data):\n",
        "            begin_index = random.randrange(0, len(data) - sequence_length)\n",
        "            data = data[begin_index:begin_index + sequence_length]\n",
        "            batch_midi.append(data)\n",
        "    batch_midi = torch.Tensor(batch_midi)\n",
        "    inputs = batch_midi[:, :-1]\n",
        "    labels = batch_midi[:, 1:]\n",
        "    return inputs, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMQXmq1XoUkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x, y = generate_batch(train_data, train_dir_path, sequence_length=sequence_length, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0dZR6UVPNkd",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waBqGI-AP7qo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MusicMultiheadAttention(torch.nn.MultiheadAttention):\n",
        "    def __init__(self, embed_dim, nhead, dropout=0.1, bias=True, add_bias_kv=False, \n",
        "                 add_zero_attn=False, kdim=None, vdim=None):\n",
        "        \n",
        "        torch.nn.MultiheadAttention.__init__(self, embed_dim, nhead, dropout=0.1, \n",
        "                                             bias=True, add_bias_kv=False, \n",
        "                                             add_zero_attn=False, kdim=None, vdim=None)\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        self.weights_q = torch.nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.weights_k = torch.nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.weights_v = torch.nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.weights_o = torch.nn.Linear(self.embed_dim, self.embed_dim)\n",
        "\n",
        "    def forward(self, query, key, value, key_padding_mask=None,\n",
        "                need_weights=True, attn_mask=None):\n",
        "        Q, K, V = self.transform_input(query, key, value)\n",
        "        # Reshaping the matrices \n",
        "        # Each L × D query, key, and value matrix is then split into H L × D \n",
        "        # h_D parts or attention heads, indexed by h, and with dimension D_h = D/H\n",
        "        Q = self.matrix_to_heads(Q)\n",
        "        K = self.matrix_to_heads(K)\n",
        "        V = self.matrix_to_heads(V)\n",
        "        \n",
        "        # learning a separate relative position embedding Er of shape (H, L, Dh)\n",
        "        Er = torch.randn([self.num_heads, query.size(1), self.head_dim], requires_grad=False).to(device)\n",
        "\n",
        "        # we transpose the two last dimensions of Er to realize Q*Er^T \n",
        "        QEr = torch.matmul(Q, torch.transpose(Er,1,2))\n",
        "        # QEr of shape (B, H, L, L)     \n",
        "        # QEr = torch.einsum('bhld,ld->bhll', [Q, Er])\n",
        "\n",
        "        # 1. Pad a dummy column vector of length L before the leftmost column.\n",
        "        QEr = torch.nn.functional.pad(QEr, (1,0), mode=\"constant\", value=0)\n",
        "\n",
        "        # 2. Reshape the matrix to have shape (L+1, L). \n",
        "        QEr = torch.reshape(QEr, [QEr.size(0), QEr.size(1), QEr.size(3), QEr.size(2)])\n",
        "        \n",
        "        # 3. Slice that matrix to retain only the last l rows and all the columns, \n",
        "        # resulting in a (L, L) matrix again, but now absolute-by-absolute indexed, \n",
        "        # which is the S rel that we need.\n",
        "        S_rel = QEr[:,:,1:,:]\n",
        "\n",
        "        z_attention = self.attention(Q, K, V, S_rel, attn_mask)\n",
        "        z_attention = self.weights_o(z_attention)\n",
        "        # Masking can be added and Dropout ?\n",
        "\n",
        "        return z_attention\n",
        "\n",
        "    def attention(self, Q, K, V, S, mask):\n",
        "        # Dh = self.head_dim // self.num_heads\n",
        "        logits = torch.add(torch.matmul(Q, torch.transpose(K, 2, 3)), S) / math.sqrt((self.head_dim // self.num_heads))\n",
        "        # print(\"logits : \", logits.size())\n",
        "        # print(\"mask : \", mask.size())\n",
        "        if mask is not None:\n",
        "        #    mask = mask.unsqueeze(1) #shape of mask must be broadcastable with shape of underlying tensor\n",
        "            logits = logits.masked_fill(mask == 0, -1e9) #masked_fill fills elements of scores with -1e9 where mask == 0\n",
        "        #if mask is not None:\n",
        "        #    logits += (mask.to(torch.int64) * -1e9).to(logits.dtype)        \n",
        "            \n",
        "        activation = F.softmax(logits, -1)\n",
        "        attention = torch.matmul(activation, V)\n",
        "        attention = torch.reshape(attention, (attention.size(0), -1, self.embed_dim))\n",
        "        return attention\n",
        "    \n",
        "    def matrix_to_heads(self, qkv):\n",
        "        '''\n",
        "            Takes a query/key/value (qkv) matrix and reshapes it to  B * H * L * D_h heads \n",
        "            with dimension D_h = D/H\n",
        "        '''\n",
        "        batch_size_q = qkv.size(0)\n",
        "        #qkv = torch.reshape(qkv, (batch_size_q, qkv.size(0), self.num_heads, self.head_dim))\n",
        "        qkv = torch.reshape(qkv, (batch_size_q, self.num_heads, qkv.size(1), self.head_dim))\n",
        "        return qkv\n",
        "\n",
        "    def transform_input(self, query, key, value):\n",
        "        '''\n",
        "            Transforming the input vector, X, of LxD dimension \n",
        "            into \n",
        "                queries: Q = XW^Q \n",
        "                keys:    K = XW^K\n",
        "            and values:  V = XW^V\n",
        "            which are all DxD square matrices.\n",
        "        '''\n",
        "        return self.weights_q(query), self.weights_k(key), self.weights_v(value)\n",
        "    \n",
        "\n",
        "class MusicTransformerEncoderLayer(torch.nn.TransformerEncoderLayer):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048,\n",
        "                 dropout=0.1, activation=\"relu\"):\n",
        "        torch.nn.TransformerEncoderLayer.__init__(self, d_model, nhead)\n",
        "        self.d_model = d_model\n",
        "        # OverRide\n",
        "        self.self_attn = MusicMultiheadAttention(d_model, nhead)\n",
        "\n",
        "class MusicTransformerDecoderLayer(torch.nn.TransformerDecoderLayer):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048,\n",
        "                 dropout=0.1, activation=\"relu\"):\n",
        "        torch.nn.TransformerDecoderLayer.__init__(self, d_model,nhead)\n",
        "        self.d_model = d_model\n",
        "        # OverRide\n",
        "        self.self_attn = MusicMultiheadAttention(d_model, nhead)\n",
        "\n",
        "class MusicTransformerEncoder(torch.nn.TransformerEncoder):\n",
        "    def __init__(self, encoder_layer, vocabulary_size=390, num_encoder_layers=6, normalization=None):\n",
        "        super().__init__(encoder_layer, num_encoder_layers, normalization)\n",
        "        self.d_model = encoder_layer.d_model\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.dropout = torch.nn.Dropout(encoder_layer.dropout.p)\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings=self.vocabulary_size, embedding_dim=self.d_model)\n",
        "    \n",
        "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
        "        \n",
        "        pos_encoding = DynamicPositionEmbedding(self.d_model, src.size(1))\n",
        "        \n",
        "        src = math.sqrt(self.d_model) * self.embedding(src.to(torch.long).to(device))\n",
        "        src = pos_encoding(src)\n",
        "        src = self.dropout(src)\n",
        "\n",
        "        return super().forward(src, mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "\n",
        "class MusicTransformerDecoder(torch.nn.TransformerDecoder):\n",
        "    def __init__(self, decoder_layer, vocabulary_size=390, num_decoder_layers=6, normalization=None):\n",
        "        super().__init__(decoder_layer, num_decoder_layers, normalization)\n",
        "        self.d_model = decoder_layer.d_model\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.dropout = torch.nn.Dropout(decoder_layer.dropout.p)\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings=self.vocabulary_size, embedding_dim=self.d_model)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, \n",
        "                memory_mask=None, tgt_key_padding_mask=None,\n",
        "                memory_key_padding_mask=None):        \n",
        "                \n",
        "        pos_encoding = DynamicPositionEmbedding(self.d_model, tgt.size(1))\n",
        "\n",
        "        tgt = pos_encoding(math.sqrt(self.d_model) * self.embedding(tgt.to(torch.long).to(device)))\n",
        "        tgt = self.dropout(tgt)\n",
        "\n",
        "        return super().forward(tgt, memory, tgt_mask=tgt_mask, \n",
        "                memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                memory_key_padding_mask=memory_key_padding_mask)\n",
        "\n",
        "\n",
        "class MusicTransformer(torch.nn.modules.Transformer):\n",
        "    def __init__(self, d_model=512, nhead=8, vocabulary_size=388,\n",
        "                 num_encoder_layers=6, num_decoder_layers=6, \n",
        "                 dim_feedforward=2048, dropout=0.1, activation=\"relu\", \n",
        "                 custom_encoder=None, custom_decoder=None):\n",
        "        \n",
        "        super().__init__(d_model=d_model, nhead=nhead, \n",
        "                         num_encoder_layers=num_encoder_layers,\n",
        "                         num_decoder_layers=num_decoder_layers, \n",
        "                         dim_feedforward=dim_feedforward, dropout=dropout, activation=activation, \n",
        "                         custom_encoder=custom_encoder, custom_decoder=custom_decoder)\n",
        "        \n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        ###        \n",
        "        self.fc = torch.nn.Linear(self.d_model, self.vocabulary_size)\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None,\n",
        "                memory_mask=None, src_key_padding_mask=None,\n",
        "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "\n",
        "        #if src.size(1) != tgt.size(1):\n",
        "        #    raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
        "\n",
        "        #if src.size(2) != self.d_model or tgt.size(2) != self.d_model:\n",
        "        #    raise RuntimeError(\"the feature number of src and tgt must be equal to d_model\")\n",
        "            \n",
        "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
        "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
        "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                              memory_key_padding_mask=memory_key_padding_mask)\n",
        "\n",
        "        output = self.fc(output)        \n",
        "        \n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DynamicPositionEmbedding(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, max_seq=1024):\n",
        "        super().__init__()\n",
        "        embed_sinusoid_list = np.array([[\n",
        "            [\n",
        "                math.sin(\n",
        "                    pos * math.exp(-math.log(10000) * i/embedding_dim) *\n",
        "                    math.exp(math.log(10000)/embedding_dim * (i % 2)) + 0.5 * math.pi * (i % 2)\n",
        "                )\n",
        "                for i in range(embedding_dim)\n",
        "            ]\n",
        "            for pos in range(max_seq)\n",
        "        ]])\n",
        "        self.positional_embedding = embed_sinusoid_list\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + torch.from_numpy(self.positional_embedding[:, :x.size(1), :]).to(x.device, dtype=x.dtype)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PositionalEncoder(torch.nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len=1024):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = \\\n",
        "                    math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
        "                pe[pos, i + 1] = \\\n",
        "                    math.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = x * math.sqrt(self.d_model)\n",
        "            seq_len = x.size(1)\n",
        "            self.pe.to(device)\n",
        "            print(\"self.pe : \", self.pe.device.type)\n",
        "            pe = self.pe[:, :seq_len]\n",
        "            print(\"pe : \", pe.device.type)\n",
        "            x = x + pe\n",
        "            return x   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfU5uhNwKx6u",
        "colab_type": "code",
        "outputId": "64341abd-0037-48c7-8394-2bcd6aa854dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "def tensorFromSequence(sequence):\n",
        "    \"\"\"\n",
        "    Generate tensors from the sequence in numpy.\n",
        "    \"\"\"\n",
        "    output = torch.tensor(sequence).long()\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def PrepareData(npz_file, split='train', L=1024):\n",
        "    \"\"\"\n",
        "    Function to prepare the data into pairs (input, target).\n",
        "    Adds [PAD], [SOS] and [EOS] tokens into the data,\n",
        "    where [PAD]=1, [SOS]=2, [EOS]=3.\n",
        "    Limits the sequence to length of L.\n",
        "    \"\"\"\n",
        "    print(\"Preparing data for\",split,\"split...\")\n",
        "    # Load in the data\n",
        "    full_data = np.load(npz_file, fix_imports=True, encoding=\"latin1\", allow_pickle=True)\n",
        "    data = full_data[split]\n",
        "\n",
        "    # Extract the vocab from file\n",
        "    vocab = GenerateVocab(npz_file)\n",
        "    # Generate new vocab to map to later\n",
        "    new_vocab = np.arange(len(vocab))\n",
        "\n",
        "    # Initialize the tokens\n",
        "    pad_token = np.array([[1]])\n",
        "\n",
        "    # Repeat for all samples in data\n",
        "    pairs = []\n",
        "    for samples in data:\n",
        "        # Serialise the dataset so that the resulting sequence is\n",
        "        # S_1 A_1 T_1, B_2 S_2 A_2 T_2 B_2, ...\n",
        "\n",
        "        # Generate input\n",
        "        input_seq = samples.flatten()\n",
        "\n",
        "        # Cut off the samples so that it has length of 1024\n",
        "        if(len(input_seq) >= L):\n",
        "            # input_seq = input_seq[:L-1]\n",
        "            input_seq = input_seq[:L]\n",
        "\n",
        "        # Set the NaN values to 0 and reshape accordingly\n",
        "        input_seq = np.nan_to_num(input_seq.reshape(1,input_seq.size))\n",
        "\n",
        "        # Generate target\n",
        "        output_seq = input_seq[:,1:]\n",
        "\n",
        "        # For both sequences, pad to sequence length L\n",
        "        pad_array = pad_token * np.ones((1,L-input_seq.shape[1]))\n",
        "        input_seq = np.append(input_seq, pad_array,axis=1)\n",
        "        pad_array = pad_token * np.ones((1,L-output_seq.shape[1]))\n",
        "        output_seq = np.append(output_seq, pad_array,axis=1)\n",
        "\n",
        "        # Map the pitch value to int values below vocab size\n",
        "        for i, val in enumerate(vocab):\n",
        "            input_seq[input_seq==val] = new_vocab[i]\n",
        "            output_seq[output_seq==val] = new_vocab[i]\n",
        "\n",
        "        # Make it into a pair\n",
        "        pair = [input_seq, output_seq]\n",
        "\n",
        "        # Combine all pairs into one big list of pairs\n",
        "        pairs.append(pair)\n",
        "\n",
        "    print(\"Generated data pairs.\")\n",
        "    return np.array(pairs)\n",
        "\n",
        "def GenerateVocab(npz_file):\n",
        "    \"\"\"\n",
        "    Generate vocabulary for the dataset including the custom tokens.\n",
        "    \"\"\"\n",
        "    full_data = np.load(npz_file, fix_imports=True, encoding=\"latin1\", allow_pickle=True)\n",
        "    train_data = full_data['train']\n",
        "    validation_data = full_data['valid']\n",
        "    test_data = full_data['test']\n",
        "\n",
        "    combined_data = np.concatenate((train_data, validation_data, test_data))\n",
        "\n",
        "    vocab = np.nan\n",
        "    for sequences in combined_data:\n",
        "        vocab = np.append(vocab,np.unique(sequences))\n",
        "\n",
        "    vocab = np.unique(vocab)\n",
        "    vocab = vocab[~np.isnan(vocab)]\n",
        "    vocab = np.append([0,1],vocab)\n",
        "    return vocab \n",
        " \n",
        "src_data = '/gdrive/My Drive/my_data/library/Jsb16thSeparated.npz'\n",
        "# Generate the vocabulary from the data\n",
        "vocab = GenerateVocab(src_data)\n",
        "vocabulary_size = len(vocab)\n",
        "pad_token = 1\n",
        "\n",
        "# Setup the dataset for training split and validation split\n",
        "train_data = PrepareData(src_data ,'train', int(sequence_length))\n",
        "valid_data = PrepareData(src_data ,'valid', int(sequence_length))\n",
        "\n",
        "def batched_learning(train,batch_size):\n",
        "    for i in range(0, len(train), batch_size):\n",
        "        train1 = train[i:i + batch_size]\n",
        "        yield train1[:,0],train1[:,1]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing data for train split...\n",
            "Generated data pairs.\n",
            "Preparing data for valid split...\n",
            "Generated data pairs.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjktKnA0OSFh",
        "colab_type": "code",
        "outputId": "e8cf1e85-4dd6-48de-a6c9-3d3cec5aa51d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocabulary_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbnfRnF2P0d3",
        "colab_type": "text"
      },
      "source": [
        "##3 - Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtKgQYKuP4-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "# vocabulary_size depends on the midi encodding\n",
        "# ~> 388(+2) for encoded_midi / epiano compt \n",
        "# ~> 46(+2) for encoded_midi / epiano compt                 \n",
        "\n",
        "# DEFINING THE MODEL\n",
        "\n",
        "vocabulary_size = 390\n",
        "\n",
        "normalization = torch.nn.LayerNorm(d_model)\n",
        "\n",
        "custom_encoder_layer = MusicTransformerEncoderLayer(d_model=d_model, nhead=nhead, \n",
        "                                               dim_feedforward=dim_feedforward, \n",
        "                                               dropout=dropout, activation=\"relu\")\n",
        "\n",
        "custom_decoder_layer = MusicTransformerDecoderLayer(d_model=d_model, nhead=nhead, \n",
        "                                               dim_feedforward=dim_feedforward, \n",
        "                                               dropout=dropout, activation=\"relu\")\n",
        "\n",
        "custom_encoder = MusicTransformerEncoder(custom_encoder_layer, vocabulary_size, num_layer, normalization)\n",
        "custom_decoder = MusicTransformerDecoder(custom_decoder_layer, vocabulary_size, num_layer, normalization)\n",
        "\n",
        "model = MusicTransformer(d_model=d_model, nhead=nhead, \n",
        "                         vocabulary_size=vocabulary_size, \n",
        "                         num_encoder_layers=num_layer, \n",
        "                         num_decoder_layers=num_layer, \n",
        "                         dim_feedforward=dim_feedforward, \n",
        "                         dropout=dropout, activation=\"relu\", \n",
        "                         custom_encoder=custom_encoder, \n",
        "                         custom_decoder=custom_decoder)\n",
        "# Give model to the current device (hopefully cuda)\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "# Adam optimizer [20] with β 1 = 0.9, β 2 = 0.98 and \u000f = 10 −9\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-09, weight_decay=1e-4)\n",
        "\n",
        "# Define a scheduler to vary the learning rate\n",
        "\n",
        "class Scheduler:\n",
        "    def __init__(self, optimizer, d_model=d_model, warmup_steps=4000):\n",
        "        self.optimizer = optimizer\n",
        "        self.d_model = d_model\n",
        "        self.step_num = 1\n",
        "        self.l_rate = 0\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def step(self):        \n",
        "        # increment step\n",
        "        self.step_num += 1\n",
        "\n",
        "        # compute new learning rate        \n",
        "        self.l_rate = self.d_model**(-.5) * min(self.step_num**(-.5), self.step_num * self.warmup_steps**(-1.5))\n",
        "\n",
        "        # update optimizer learning rate\n",
        "        for p in optimizer.param_groups:\n",
        "            p['lr'] = self.l_rate\n",
        "\n",
        "        # update the weights in the network\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "# See if it is possible to do it using lr_scheduler.LambdaLR lr_scheduler.StepLR\n",
        "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)\n",
        "scheduler = Scheduler(optimizer, d_model, warmup_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np2MoiTMJ8KF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Mask Generation from https://github.com/COMP6248-Reproducability-Challenge/music-transformer-comp6248/blob/master/MaskGen.py\n",
        "\n",
        "# Filename: MaskGen.py\n",
        "# Date Created: 15-Mar-2019 2:42:12 pm\n",
        "# Description: Functions used to generate masks w.r.t. given inputs.\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def nopeak_mask(size):\n",
        "    np_mask = np.triu(np.ones((1, size, size)), k=1).astype('uint8')\n",
        "    np_mask =  Variable(torch.from_numpy(np_mask) == 0).to(device)\n",
        "    return np_mask\n",
        "\n",
        "\n",
        "def create_masks(src, trg, pad_token):\n",
        "    src_mask = (src != pad_token).unsqueeze(-2).to(device)\n",
        "\n",
        "    if trg is not None:\n",
        "        trg_mask = (trg != pad_token).unsqueeze(-2).to(device)\n",
        "        size = trg.size(1) # get seq_len for matrix\n",
        "        np_mask = nopeak_mask(size)\n",
        "        trg_mask = trg_mask & np_mask\n",
        "    else:\n",
        "        trg_mask = None\n",
        "    return src_mask, trg_mask\n",
        "\n",
        "\n",
        "def count_nonpad_tokens(target):\n",
        "    nonpads = (target != 1).squeeze()\n",
        "    ntokens = torch.sum(nonpads)\n",
        "    return ntokens        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOSMK5LN1_TB",
        "colab_type": "code",
        "outputId": "4d3a6652-9f72-452d-95a0-39eac336d9f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "train_data, test_data = load_dataset(train_dir_path, test_dir_path)\n",
        "\n",
        "ckpt_dir = os.path.join(gdrive_root, '/my_data/library/checkpoints/')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "  os.makedirs(ckpt_dir)\n",
        "  \n",
        "best_loss = 10.\n",
        "#ckpt_path = os.path.join(ckpt_dir, 'MT-'+ str(datetime.datetime.now()) +'.pt')\n",
        "#ckpt_path = os.path.join(ckpt_dir, 'MT.pt')\n",
        "model_name = 'midi_encoded_6-1'\n",
        "ckpt_path = '/gdrive/My Drive/my_data/library/checkpoints/train-nlayer_'+model_name+'.pt'\n",
        "ckpt_backup_path = '/gdrive/My Drive/my_data/library/checkpoints/train-backup-nlayer_'+model_name+'.pt'\n",
        "ckpt_fullbackup_path = '/gdrive/My Drive/my_data/library/checkpoints/train-fullbackup-nlayer_'+model_name+'.pt'\n",
        "if os.path.exists(ckpt_path):\n",
        "    ckpt = torch.load(ckpt_path)\n",
        "    try:\n",
        "      model.load_state_dict(ckpt['my_model'])\n",
        "      optimizer.load_state_dict(ckpt['optimizer'])\n",
        "      best_acc = ckpt['best_loss']\n",
        "    except RuntimeError as e:\n",
        "        print('wrong checkpoint')\n",
        "    else:    \n",
        "      print('checkpoint is loaded !')\n",
        "      print('current best loss : %.2f' % best_loss)\n",
        "\n",
        "train_writer = SummaryWriter()\n",
        "test_writer = SummaryWriter()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XYfecvLVYkh",
        "colab_type": "code",
        "outputId": "4004107e-eb10-4940-8f65-adf7e414402e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Training\n",
        "max_epochs = 100\n",
        "n_iter = 0\n",
        "\n",
        "# each 50 iterations we are going to compare the losses\n",
        "total_train_loss = []\n",
        "total_valid_loss = []\n",
        "\n",
        "for e in range(max_epochs):\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    for b_train in range(len(train_data) // batch_size):\n",
        "        \n",
        "        n_iter += 1\n",
        "        # train phase\n",
        "        # feed data into the network and get outputs.\n",
        "        # feed data into the network and get outputs.\n",
        "        inputs, target = generate_batch(train_data, train_dir_path, sequence_length=sequence_length, batch_size=batch_size)\n",
        "              \n",
        "        # Train on GPU\n",
        "        inputs.to(device)\n",
        "        target.to(device)\n",
        "        ys = target.contiguous().view(-1).to(torch.long).to(device)\n",
        "        \n",
        "        # Create mask for both input and target sequences\n",
        "        input_mask, target_mask = create_masks(torch.reshape(inputs, (batch_size, 1, -1)), torch.reshape(target, (batch_size, 1, -1)), pad_token)        \n",
        "        \n",
        "        # feed data into the network and get outputs.\n",
        "        preds_idx = model(inputs, target, input_mask, target_mask)\n",
        "        \n",
        "        # Flush out gradients computed at the previous step before computing gradients at the current step. \n",
        "        #       Otherwise, gradients would accumulate.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # calculate loss\n",
        "        loss = F.cross_entropy(preds_idx.contiguous().view(preds_idx.size(-1), -1).transpose(0,1), ys, ignore_index = pad_token, size_average = False) / (count_nonpad_tokens(ys))\n",
        "\n",
        "        # accumulates the gradient and backprogate loss.\n",
        "        loss.backward()\n",
        "\n",
        "        # performs a parameter update based on the current gradient\n",
        "        scheduler.step()    \n",
        "        \n",
        "        print('\\n====================================================')\n",
        "        print('Epoch/Batch: {}/{}'.format(e, b_train))\n",
        "        print('Train >>>> Loss: {:6.6}'.format(loss))\n",
        "\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "    print('\\n**************************************************')\n",
        "    print(\"\\n*** Test *** \")\n",
        "    \n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    valid_loss = []\n",
        "    with torch.no_grad():\n",
        "        for b_test in range(len(test_data) // batch_size):        \n",
        "            inputs, target = generate_batch(test_data, test_dir_path, sequence_length=sequence_length, batch_size=batch_size)\n",
        "                  \n",
        "            # Train on GPU\n",
        "            inputs.to(device)\n",
        "            target.to(device)\n",
        "            ys = target.contiguous().view(-1).to(torch.long).to(device)\n",
        "            \n",
        "            # Create mask for both input and target sequences\n",
        "            input_mask, target_mask = create_masks(torch.reshape(inputs, (batch_size, 1,-1)), torch.reshape(target, (batch_size, 1, -1)), pad_token)        \n",
        "\n",
        "            # Feed Forward\n",
        "            preds_validate = model(inputs, target, input_mask, target_mask)\n",
        "            loss = F.cross_entropy(preds_validate.contiguous().view(preds_validate.size(-1), -1).transpose(0,1), ys, \\\n",
        "                                    ignore_index = pad_token, size_average = False) / (count_nonpad_tokens(ys))\n",
        "            valid_loss.append(loss.item())\n",
        "\n",
        "    avg_train_loss = np.mean(train_loss)\n",
        "    avg_valid_loss = np.mean(valid_loss)\n",
        "\n",
        "    total_train_loss.append(avg_train_loss)\n",
        "    total_valid_loss.append(avg_valid_loss)\n",
        "\n",
        "    print(\"[Average Train Loss]: {:6.6}\".format(avg_train_loss))\n",
        "    print(\"[Average Testing Loss]: {:6.6}\".format(avg_valid_loss))\n",
        "\n",
        "    # save checkpoint whenever there is improvement in performance\n",
        "    if avg_valid_loss < best_loss:\n",
        "        best_loss = avg_valid_loss\n",
        "        # Note: optimizer also has states ! don't forget to save them as well.\n",
        "        ckpt = {'my_model':model.state_dict(),\n",
        "                'optimizer':optimizer.state_dict(),\n",
        "                'best_loss':best_loss}\n",
        "        torch.save(ckpt, ckpt_path)\n",
        "        print('checkpoint is saved !')\n",
        "\n",
        "    train_writer.add_scalar('loss/train', avg_train_loss, global_step=n_iter)\n",
        "    test_writer.add_scalar('loss/valid', avg_valid_loss, global_step=n_iter)\n",
        "    print('\\n**************************************************')\n",
        "    # torch.cuda.empty_cache()\n",
        "    # torch.save(model.state_dict(), '/gdrive/My Drive/my_data/library/checkpoints/train-{}.pt'.format(e))\n",
        "    ckpt = {'my_model':model.state_dict(),\n",
        "            'optimizer':optimizer.state_dict(),\n",
        "            'best_loss':best_loss}\n",
        "    torch.save(model.state_dict(), ckpt_backup_path)\n",
        "    torch.save(ckpt, ckpt_fullbackup_path)\n",
        "\n",
        "train_writer = SummaryWriter()\n",
        "test_writer = SummaryWriter()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/0\n",
            "Train >>>> Loss: 6.51958\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/1\n",
            "Train >>>> Loss: 6.52127\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/2\n",
            "Train >>>> Loss: 6.53951\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/3\n",
            "Train >>>> Loss: 6.52333\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/4\n",
            "Train >>>> Loss: 6.49981\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/5\n",
            "Train >>>> Loss: 6.50868\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/6\n",
            "Train >>>> Loss: 6.53136\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/7\n",
            "Train >>>> Loss: 6.50404\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/8\n",
            "Train >>>> Loss: 6.50997\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/9\n",
            "Train >>>> Loss: 6.52364\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/10\n",
            "Train >>>> Loss: 6.51185\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/11\n",
            "Train >>>> Loss: 6.50966\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/12\n",
            "Train >>>> Loss: 6.50863\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/13\n",
            "Train >>>> Loss: 6.50677\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/14\n",
            "Train >>>> Loss: 6.51717\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/15\n",
            "Train >>>> Loss: 6.51741\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/16\n",
            "Train >>>> Loss: 6.47248\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/17\n",
            "Train >>>> Loss: 6.4661\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/18\n",
            "Train >>>> Loss: 6.48398\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/19\n",
            "Train >>>> Loss: 6.48152\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/20\n",
            "Train >>>> Loss: 6.47292\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/21\n",
            "Train >>>> Loss: 6.46893\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/22\n",
            "Train >>>> Loss: 6.4796\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/23\n",
            "Train >>>> Loss: 6.45524\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/24\n",
            "Train >>>> Loss: 6.46117\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/25\n",
            "Train >>>> Loss: 6.45128\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/26\n",
            "Train >>>> Loss: 6.46007\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/27\n",
            "Train >>>> Loss: 6.45616\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/28\n",
            "Train >>>> Loss: 6.45807\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/29\n",
            "Train >>>> Loss: 6.42843\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/30\n",
            "Train >>>> Loss: 6.44391\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/31\n",
            "Train >>>> Loss: 6.43814\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/32\n",
            "Train >>>> Loss: 6.44386\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/33\n",
            "Train >>>> Loss: 6.40975\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/34\n",
            "Train >>>> Loss: 6.42402\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 0/35\n",
            "Train >>>> Loss: 6.43041\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 6.48163\n",
            "[Average Testing Loss]: 6.37935\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/0\n",
            "Train >>>> Loss: 6.43154\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/1\n",
            "Train >>>> Loss: 6.39803\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/2\n",
            "Train >>>> Loss: 6.40553\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/3\n",
            "Train >>>> Loss: 6.39345\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/4\n",
            "Train >>>> Loss: 6.37846\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/5\n",
            "Train >>>> Loss: 6.39775\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/6\n",
            "Train >>>> Loss: 6.38579\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/7\n",
            "Train >>>> Loss: 6.3712\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/8\n",
            "Train >>>> Loss: 6.38099\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/9\n",
            "Train >>>> Loss: 6.36974\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/10\n",
            "Train >>>> Loss: 6.37539\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/11\n",
            "Train >>>> Loss: 6.33727\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/12\n",
            "Train >>>> Loss: 6.35324\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/13\n",
            "Train >>>> Loss: 6.34436\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/14\n",
            "Train >>>> Loss: 6.34081\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/15\n",
            "Train >>>> Loss: 6.34182\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/16\n",
            "Train >>>> Loss: 6.32999\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/17\n",
            "Train >>>> Loss: 6.31434\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/18\n",
            "Train >>>> Loss: 6.29557\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/19\n",
            "Train >>>> Loss: 6.28964\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/20\n",
            "Train >>>> Loss: 6.29447\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/21\n",
            "Train >>>> Loss: 6.29182\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/22\n",
            "Train >>>> Loss: 6.27843\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/23\n",
            "Train >>>> Loss: 6.27506\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/24\n",
            "Train >>>> Loss: 6.25202\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/25\n",
            "Train >>>> Loss: 6.24887\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/26\n",
            "Train >>>> Loss: 6.23485\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/27\n",
            "Train >>>> Loss: 6.24186\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/28\n",
            "Train >>>> Loss: 6.22813\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/29\n",
            "Train >>>> Loss: 6.21851\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/30\n",
            "Train >>>> Loss: 6.21906\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/31\n",
            "Train >>>> Loss: 6.21858\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/32\n",
            "Train >>>> Loss: 6.20111\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/33\n",
            "Train >>>> Loss: 6.20198\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/34\n",
            "Train >>>> Loss: 6.19134\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 1/35\n",
            "Train >>>> Loss: 6.18199\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 6.30592\n",
            "[Average Testing Loss]: 6.12123\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/0\n",
            "Train >>>> Loss: 6.18502\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/1\n",
            "Train >>>> Loss: 6.1813\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/2\n",
            "Train >>>> Loss: 6.15857\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/3\n",
            "Train >>>> Loss: 6.17334\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/4\n",
            "Train >>>> Loss: 6.15966\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/5\n",
            "Train >>>> Loss: 6.1543\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/6\n",
            "Train >>>> Loss: 6.14597\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/7\n",
            "Train >>>> Loss: 6.1457\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/8\n",
            "Train >>>> Loss: 6.1336\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/9\n",
            "Train >>>> Loss: 6.1249\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/10\n",
            "Train >>>> Loss: 6.1309\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/11\n",
            "Train >>>> Loss: 6.1133\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/12\n",
            "Train >>>> Loss: 6.10735\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/13\n",
            "Train >>>> Loss: 6.11739\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/14\n",
            "Train >>>> Loss: 6.11071\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/15\n",
            "Train >>>> Loss: 6.10586\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/16\n",
            "Train >>>> Loss: 6.09512\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/17\n",
            "Train >>>> Loss: 6.09957\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/18\n",
            "Train >>>> Loss: 6.09863\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/19\n",
            "Train >>>> Loss: 6.09459\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/20\n",
            "Train >>>> Loss: 6.09379\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/21\n",
            "Train >>>> Loss: 6.08254\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/22\n",
            "Train >>>> Loss: 6.07882\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/23\n",
            "Train >>>> Loss: 6.07718\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/24\n",
            "Train >>>> Loss: 6.08087\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/25\n",
            "Train >>>> Loss: 6.07086\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/26\n",
            "Train >>>> Loss: 6.07808\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/27\n",
            "Train >>>> Loss: 6.06152\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/28\n",
            "Train >>>> Loss: 6.06644\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/29\n",
            "Train >>>> Loss: 6.06366\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/30\n",
            "Train >>>> Loss: 6.05997\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/31\n",
            "Train >>>> Loss: 6.0621\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/32\n",
            "Train >>>> Loss: 6.05766\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/33\n",
            "Train >>>> Loss: 6.05502\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/34\n",
            "Train >>>> Loss: 6.05564\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 2/35\n",
            "Train >>>> Loss: 6.04997\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 6.10361\n",
            "[Average Testing Loss]: 6.00355\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/0\n",
            "Train >>>> Loss: 6.0468\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/1\n",
            "Train >>>> Loss: 6.04264\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/2\n",
            "Train >>>> Loss: 6.04886\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/3\n",
            "Train >>>> Loss: 6.05052\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/4\n",
            "Train >>>> Loss: 6.04783\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/5\n",
            "Train >>>> Loss: 6.04963\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/6\n",
            "Train >>>> Loss: 6.04046\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/7\n",
            "Train >>>> Loss: 6.04122\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/8\n",
            "Train >>>> Loss: 6.04345\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/9\n",
            "Train >>>> Loss: 6.04095\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/10\n",
            "Train >>>> Loss: 6.03263\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/11\n",
            "Train >>>> Loss: 6.03944\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/12\n",
            "Train >>>> Loss: 6.03355\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/13\n",
            "Train >>>> Loss: 6.02716\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/14\n",
            "Train >>>> Loss: 6.03411\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/15\n",
            "Train >>>> Loss: 6.03149\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/16\n",
            "Train >>>> Loss: 6.03306\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/17\n",
            "Train >>>> Loss: 6.03138\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/18\n",
            "Train >>>> Loss: 6.02895\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/19\n",
            "Train >>>> Loss: 6.03382\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/20\n",
            "Train >>>> Loss: 6.02334\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/21\n",
            "Train >>>> Loss:  6.022\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/22\n",
            "Train >>>> Loss: 6.02644\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/23\n",
            "Train >>>> Loss: 6.0331\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/24\n",
            "Train >>>> Loss: 6.0288\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/25\n",
            "Train >>>> Loss: 6.02856\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/26\n",
            "Train >>>> Loss: 6.02351\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/27\n",
            "Train >>>> Loss: 6.02564\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/28\n",
            "Train >>>> Loss: 6.02807\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/29\n",
            "Train >>>> Loss: 6.02166\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/30\n",
            "Train >>>> Loss: 6.02268\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/31\n",
            "Train >>>> Loss: 6.00977\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/32\n",
            "Train >>>> Loss: 6.03077\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/33\n",
            "Train >>>> Loss: 6.03222\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/34\n",
            "Train >>>> Loss: 6.01878\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 3/35\n",
            "Train >>>> Loss: 6.02861\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 6.03283\n",
            "[Average Testing Loss]: 5.97941\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/0\n",
            "Train >>>> Loss: 6.02211\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/1\n",
            "Train >>>> Loss: 6.01912\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/2\n",
            "Train >>>> Loss: 6.01702\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/3\n",
            "Train >>>> Loss: 6.02045\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/4\n",
            "Train >>>> Loss: 6.0239\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/5\n",
            "Train >>>> Loss: 6.01929\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/6\n",
            "Train >>>> Loss: 6.01673\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/7\n",
            "Train >>>> Loss: 6.02346\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/8\n",
            "Train >>>> Loss: 6.01812\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/9\n",
            "Train >>>> Loss: 6.02187\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/10\n",
            "Train >>>> Loss: 6.01463\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/11\n",
            "Train >>>> Loss: 6.01567\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/12\n",
            "Train >>>> Loss: 6.01655\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/13\n",
            "Train >>>> Loss: 6.01571\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/14\n",
            "Train >>>> Loss: 6.01142\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/15\n",
            "Train >>>> Loss: 6.01795\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/16\n",
            "Train >>>> Loss: 6.01772\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/17\n",
            "Train >>>> Loss: 6.02065\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/18\n",
            "Train >>>> Loss: 6.01874\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/19\n",
            "Train >>>> Loss: 6.01553\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/20\n",
            "Train >>>> Loss: 6.00814\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/21\n",
            "Train >>>> Loss: 6.02113\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/22\n",
            "Train >>>> Loss: 6.02383\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/23\n",
            "Train >>>> Loss: 6.01086\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/24\n",
            "Train >>>> Loss: 6.01627\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/25\n",
            "Train >>>> Loss: 6.01899\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/26\n",
            "Train >>>> Loss: 6.02104\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/27\n",
            "Train >>>> Loss: 6.01565\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/28\n",
            "Train >>>> Loss: 6.01246\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/29\n",
            "Train >>>> Loss: 6.01272\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/30\n",
            "Train >>>> Loss: 6.01323\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/31\n",
            "Train >>>> Loss: 6.01352\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/32\n",
            "Train >>>> Loss: 6.01203\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/33\n",
            "Train >>>> Loss: 6.00906\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/34\n",
            "Train >>>> Loss: 6.01038\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 4/35\n",
            "Train >>>> Loss: 6.01915\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 6.01681\n",
            "[Average Testing Loss]: 5.97125\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/0\n",
            "Train >>>> Loss: 6.00945\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/1\n",
            "Train >>>> Loss: 6.00837\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/2\n",
            "Train >>>> Loss: 6.01336\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/3\n",
            "Train >>>> Loss: 6.00774\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/4\n",
            "Train >>>> Loss: 6.00761\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/5\n",
            "Train >>>> Loss: 6.01103\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/6\n",
            "Train >>>> Loss: 6.00654\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/7\n",
            "Train >>>> Loss: 6.00946\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/8\n",
            "Train >>>> Loss: 6.01137\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/9\n",
            "Train >>>> Loss: 6.00642\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/10\n",
            "Train >>>> Loss: 6.0087\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/11\n",
            "Train >>>> Loss: 6.00949\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/12\n",
            "Train >>>> Loss: 6.0122\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/13\n",
            "Train >>>> Loss: 6.01323\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/14\n",
            "Train >>>> Loss: 6.00647\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/15\n",
            "Train >>>> Loss: 6.01027\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/16\n",
            "Train >>>> Loss: 6.01314\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/17\n",
            "Train >>>> Loss: 6.01078\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/18\n",
            "Train >>>> Loss: 6.00304\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/19\n",
            "Train >>>> Loss: 6.01398\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/20\n",
            "Train >>>> Loss: 6.00619\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/21\n",
            "Train >>>> Loss: 6.00349\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/22\n",
            "Train >>>> Loss: 6.00415\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/23\n",
            "Train >>>> Loss: 6.00942\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/24\n",
            "Train >>>> Loss: 6.00948\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/25\n",
            "Train >>>> Loss: 6.00284\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/26\n",
            "Train >>>> Loss: 6.01122\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/27\n",
            "Train >>>> Loss: 6.01054\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/28\n",
            "Train >>>> Loss: 6.00559\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/29\n",
            "Train >>>> Loss: 6.00302\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/30\n",
            "Train >>>> Loss: 6.00037\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/31\n",
            "Train >>>> Loss: 6.00491\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/32\n",
            "Train >>>> Loss: 6.00527\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/33\n",
            "Train >>>> Loss: 6.00307\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/34\n",
            "Train >>>> Loss: 6.00756\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 5/35\n",
            "Train >>>> Loss: 6.00376\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 6.00788\n",
            "[Average Testing Loss]: 5.96704\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/0\n",
            "Train >>>> Loss: 6.00624\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/1\n",
            "Train >>>> Loss: 6.00404\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/2\n",
            "Train >>>> Loss: 6.00035\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/3\n",
            "Train >>>> Loss: 5.99933\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/4\n",
            "Train >>>> Loss: 6.00471\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/5\n",
            "Train >>>> Loss: 6.0053\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/6\n",
            "Train >>>> Loss: 6.00887\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/7\n",
            "Train >>>> Loss: 6.00345\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/8\n",
            "Train >>>> Loss: 6.00428\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/9\n",
            "Train >>>> Loss: 6.00567\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/10\n",
            "Train >>>> Loss: 5.99759\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/11\n",
            "Train >>>> Loss: 6.00501\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/12\n",
            "Train >>>> Loss: 6.0064\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/13\n",
            "Train >>>> Loss: 6.00004\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/14\n",
            "Train >>>> Loss: 5.99806\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/15\n",
            "Train >>>> Loss: 5.99703\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/16\n",
            "Train >>>> Loss: 5.99683\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/17\n",
            "Train >>>> Loss: 5.99768\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/18\n",
            "Train >>>> Loss: 5.99217\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/19\n",
            "Train >>>> Loss: 5.9984\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/20\n",
            "Train >>>> Loss: 5.99345\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/21\n",
            "Train >>>> Loss: 5.99693\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/22\n",
            "Train >>>> Loss: 5.99575\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/23\n",
            "Train >>>> Loss: 5.99503\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/24\n",
            "Train >>>> Loss: 6.00361\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/25\n",
            "Train >>>> Loss: 5.9934\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/26\n",
            "Train >>>> Loss: 5.98896\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/27\n",
            "Train >>>> Loss: 5.99701\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/28\n",
            "Train >>>> Loss: 5.99487\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/29\n",
            "Train >>>> Loss: 5.99435\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/30\n",
            "Train >>>> Loss: 5.99457\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/31\n",
            "Train >>>> Loss: 5.99508\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/32\n",
            "Train >>>> Loss: 5.99222\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/33\n",
            "Train >>>> Loss: 5.99226\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/34\n",
            "Train >>>> Loss: 5.98954\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 6/35\n",
            "Train >>>> Loss: 6.0011\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.9986\n",
            "[Average Testing Loss]: 5.94428\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/0\n",
            "Train >>>> Loss: 5.98655\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/1\n",
            "Train >>>> Loss: 5.99422\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/2\n",
            "Train >>>> Loss: 5.98398\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/3\n",
            "Train >>>> Loss: 5.98609\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/4\n",
            "Train >>>> Loss: 5.98798\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/5\n",
            "Train >>>> Loss: 5.98158\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/6\n",
            "Train >>>> Loss: 5.98132\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/7\n",
            "Train >>>> Loss: 5.9811\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/8\n",
            "Train >>>> Loss: 5.97748\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/9\n",
            "Train >>>> Loss: 5.97314\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/10\n",
            "Train >>>> Loss: 5.97691\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/11\n",
            "Train >>>> Loss: 5.9688\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/12\n",
            "Train >>>> Loss: 5.97161\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/13\n",
            "Train >>>> Loss: 5.96953\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/14\n",
            "Train >>>> Loss: 5.96663\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/15\n",
            "Train >>>> Loss: 5.94919\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/16\n",
            "Train >>>> Loss: 5.96345\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/17\n",
            "Train >>>> Loss: 5.95723\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/18\n",
            "Train >>>> Loss: 5.94237\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/19\n",
            "Train >>>> Loss: 5.93954\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/20\n",
            "Train >>>> Loss: 5.9775\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/21\n",
            "Train >>>> Loss: 5.94037\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/22\n",
            "Train >>>> Loss: 5.96864\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/23\n",
            "Train >>>> Loss: 5.94812\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/24\n",
            "Train >>>> Loss: 5.96984\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/25\n",
            "Train >>>> Loss: 5.96281\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/26\n",
            "Train >>>> Loss: 5.9682\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/27\n",
            "Train >>>> Loss: 5.94362\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/28\n",
            "Train >>>> Loss: 5.94862\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/29\n",
            "Train >>>> Loss: 5.95618\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/30\n",
            "Train >>>> Loss: 5.92547\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/31\n",
            "Train >>>> Loss: 5.95051\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/32\n",
            "Train >>>> Loss: 5.9543\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/33\n",
            "Train >>>> Loss: 5.94341\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/34\n",
            "Train >>>> Loss: 5.92086\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 7/35\n",
            "Train >>>> Loss: 5.94259\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.96277\n",
            "[Average Testing Loss]: 5.88814\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/0\n",
            "Train >>>> Loss: 5.92658\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/1\n",
            "Train >>>> Loss: 5.93468\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/2\n",
            "Train >>>> Loss:  5.912\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/3\n",
            "Train >>>> Loss: 5.96841\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/4\n",
            "Train >>>> Loss: 5.90348\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/5\n",
            "Train >>>> Loss: 5.92304\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/6\n",
            "Train >>>> Loss: 5.92892\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/7\n",
            "Train >>>> Loss: 5.91274\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/8\n",
            "Train >>>> Loss: 5.93043\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/9\n",
            "Train >>>> Loss: 5.91688\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/10\n",
            "Train >>>> Loss: 5.93333\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/11\n",
            "Train >>>> Loss: 5.92341\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/12\n",
            "Train >>>> Loss: 5.93019\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/13\n",
            "Train >>>> Loss: 5.93873\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/14\n",
            "Train >>>> Loss: 5.88553\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/15\n",
            "Train >>>> Loss: 5.91048\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/16\n",
            "Train >>>> Loss: 5.92804\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/17\n",
            "Train >>>> Loss: 5.92553\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/18\n",
            "Train >>>> Loss: 5.90886\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/19\n",
            "Train >>>> Loss: 5.90773\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/20\n",
            "Train >>>> Loss: 5.91779\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/21\n",
            "Train >>>> Loss: 5.90691\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/22\n",
            "Train >>>> Loss: 5.90804\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/23\n",
            "Train >>>> Loss: 5.91182\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/24\n",
            "Train >>>> Loss: 5.93413\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/25\n",
            "Train >>>> Loss: 5.94166\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/26\n",
            "Train >>>> Loss: 5.9216\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/27\n",
            "Train >>>> Loss: 5.91816\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/28\n",
            "Train >>>> Loss: 5.94385\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/29\n",
            "Train >>>> Loss: 5.92781\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/30\n",
            "Train >>>> Loss: 5.9204\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/31\n",
            "Train >>>> Loss: 5.91204\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/32\n",
            "Train >>>> Loss: 5.92179\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/33\n",
            "Train >>>> Loss: 5.91149\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/34\n",
            "Train >>>> Loss: 5.92134\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 8/35\n",
            "Train >>>> Loss: 5.92532\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.92203\n",
            "[Average Testing Loss]: 5.88696\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/0\n",
            "Train >>>> Loss: 5.92134\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/1\n",
            "Train >>>> Loss: 5.88487\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/2\n",
            "Train >>>> Loss: 5.96559\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/3\n",
            "Train >>>> Loss: 5.91217\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/4\n",
            "Train >>>> Loss: 5.92494\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/5\n",
            "Train >>>> Loss: 5.91003\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/6\n",
            "Train >>>> Loss: 5.89367\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/7\n",
            "Train >>>> Loss: 5.95228\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/8\n",
            "Train >>>> Loss: 5.92778\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/9\n",
            "Train >>>> Loss: 5.94076\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/10\n",
            "Train >>>> Loss: 5.91231\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/11\n",
            "Train >>>> Loss: 5.88869\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/12\n",
            "Train >>>> Loss: 5.92192\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/13\n",
            "Train >>>> Loss: 5.90146\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/14\n",
            "Train >>>> Loss: 5.94836\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/15\n",
            "Train >>>> Loss: 5.89291\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/16\n",
            "Train >>>> Loss: 5.90623\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/17\n",
            "Train >>>> Loss: 5.89104\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/18\n",
            "Train >>>> Loss: 5.93259\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/19\n",
            "Train >>>> Loss: 5.92463\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/20\n",
            "Train >>>> Loss: 5.88717\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/21\n",
            "Train >>>> Loss: 5.94498\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/22\n",
            "Train >>>> Loss: 5.91832\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/23\n",
            "Train >>>> Loss: 5.90441\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/24\n",
            "Train >>>> Loss: 5.89655\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/25\n",
            "Train >>>> Loss: 5.88585\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/26\n",
            "Train >>>> Loss: 5.90705\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/27\n",
            "Train >>>> Loss: 5.88829\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/28\n",
            "Train >>>> Loss: 5.93274\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/29\n",
            "Train >>>> Loss: 5.90226\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/30\n",
            "Train >>>> Loss: 5.89988\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/31\n",
            "Train >>>> Loss: 5.93281\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/32\n",
            "Train >>>> Loss: 5.9093\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/33\n",
            "Train >>>> Loss: 5.89884\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/34\n",
            "Train >>>> Loss: 5.89541\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 9/35\n",
            "Train >>>> Loss: 5.92836\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.91349\n",
            "[Average Testing Loss]: 5.86665\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/0\n",
            "Train >>>> Loss: 5.92571\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/1\n",
            "Train >>>> Loss: 5.91193\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/2\n",
            "Train >>>> Loss: 5.89906\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/3\n",
            "Train >>>> Loss: 5.93695\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/4\n",
            "Train >>>> Loss: 5.90277\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/5\n",
            "Train >>>> Loss: 5.90925\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/6\n",
            "Train >>>> Loss: 5.90539\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/7\n",
            "Train >>>> Loss: 5.9681\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/8\n",
            "Train >>>> Loss: 5.89279\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/9\n",
            "Train >>>> Loss: 5.91898\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/10\n",
            "Train >>>> Loss: 5.94874\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/11\n",
            "Train >>>> Loss: 5.90876\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/12\n",
            "Train >>>> Loss: 5.88557\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/13\n",
            "Train >>>> Loss: 5.94592\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/14\n",
            "Train >>>> Loss: 5.89123\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/15\n",
            "Train >>>> Loss: 5.92958\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/16\n",
            "Train >>>> Loss: 5.90087\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/17\n",
            "Train >>>> Loss: 5.91625\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/18\n",
            "Train >>>> Loss: 5.96466\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/19\n",
            "Train >>>> Loss: 5.89239\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/20\n",
            "Train >>>> Loss: 5.91877\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/21\n",
            "Train >>>> Loss: 5.91835\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/22\n",
            "Train >>>> Loss: 5.92105\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/23\n",
            "Train >>>> Loss: 5.91142\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/24\n",
            "Train >>>> Loss: 5.93893\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/25\n",
            "Train >>>> Loss: 5.90147\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/26\n",
            "Train >>>> Loss: 5.90096\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/27\n",
            "Train >>>> Loss: 5.91008\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/28\n",
            "Train >>>> Loss: 5.89326\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/29\n",
            "Train >>>> Loss: 5.90559\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/30\n",
            "Train >>>> Loss: 5.91865\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/31\n",
            "Train >>>> Loss: 5.93163\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/32\n",
            "Train >>>> Loss: 5.91499\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/33\n",
            "Train >>>> Loss: 5.91036\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/34\n",
            "Train >>>> Loss: 5.88006\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 10/35\n",
            "Train >>>> Loss: 5.90072\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.91475\n",
            "[Average Testing Loss]: 5.8892\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/0\n",
            "Train >>>> Loss: 5.88779\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/1\n",
            "Train >>>> Loss: 5.94667\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/2\n",
            "Train >>>> Loss: 5.90789\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/3\n",
            "Train >>>> Loss: 5.92573\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/4\n",
            "Train >>>> Loss: 5.90868\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/5\n",
            "Train >>>> Loss: 5.90728\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/6\n",
            "Train >>>> Loss: 5.91228\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/7\n",
            "Train >>>> Loss: 5.91735\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/8\n",
            "Train >>>> Loss: 5.88318\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/9\n",
            "Train >>>> Loss: 5.93278\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/10\n",
            "Train >>>> Loss: 5.89304\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/11\n",
            "Train >>>> Loss: 5.88859\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/12\n",
            "Train >>>> Loss: 5.90471\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/13\n",
            "Train >>>> Loss: 5.92887\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/14\n",
            "Train >>>> Loss: 5.89998\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/15\n",
            "Train >>>> Loss: 5.90637\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/16\n",
            "Train >>>> Loss: 5.90911\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/17\n",
            "Train >>>> Loss: 5.93269\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/18\n",
            "Train >>>> Loss: 5.87619\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/19\n",
            "Train >>>> Loss: 5.89376\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/20\n",
            "Train >>>> Loss: 5.95011\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/21\n",
            "Train >>>> Loss: 5.95692\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/22\n",
            "Train >>>> Loss: 5.91366\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/23\n",
            "Train >>>> Loss: 5.88591\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/24\n",
            "Train >>>> Loss: 5.88833\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/25\n",
            "Train >>>> Loss: 5.92955\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/26\n",
            "Train >>>> Loss: 5.87391\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/27\n",
            "Train >>>> Loss: 5.91045\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/28\n",
            "Train >>>> Loss: 5.89544\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/29\n",
            "Train >>>> Loss: 5.90759\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/30\n",
            "Train >>>> Loss: 5.88886\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/31\n",
            "Train >>>> Loss: 5.90771\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/32\n",
            "Train >>>> Loss: 5.92599\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/33\n",
            "Train >>>> Loss: 5.89653\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/34\n",
            "Train >>>> Loss: 5.89174\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 11/35\n",
            "Train >>>> Loss: 5.92048\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.9085\n",
            "[Average Testing Loss]: 5.87437\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/0\n",
            "Train >>>> Loss: 5.94234\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/1\n",
            "Train >>>> Loss: 5.90549\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/2\n",
            "Train >>>> Loss: 5.89329\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/3\n",
            "Train >>>> Loss: 5.92738\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/4\n",
            "Train >>>> Loss: 5.84655\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/5\n",
            "Train >>>> Loss: 5.9084\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/6\n",
            "Train >>>> Loss: 5.94686\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/7\n",
            "Train >>>> Loss: 5.93831\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/8\n",
            "Train >>>> Loss: 5.91326\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/9\n",
            "Train >>>> Loss: 5.90835\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/10\n",
            "Train >>>> Loss: 5.92974\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/11\n",
            "Train >>>> Loss: 5.90712\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/12\n",
            "Train >>>> Loss: 5.92102\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/13\n",
            "Train >>>> Loss: 5.93361\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/14\n",
            "Train >>>> Loss: 5.91896\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/15\n",
            "Train >>>> Loss: 5.89399\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/16\n",
            "Train >>>> Loss: 5.92206\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/17\n",
            "Train >>>> Loss: 5.91414\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/18\n",
            "Train >>>> Loss: 5.91715\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/19\n",
            "Train >>>> Loss: 5.90855\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/20\n",
            "Train >>>> Loss: 5.93529\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/21\n",
            "Train >>>> Loss: 5.91503\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/22\n",
            "Train >>>> Loss: 5.89548\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/23\n",
            "Train >>>> Loss:    5.9\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/24\n",
            "Train >>>> Loss: 5.91225\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/25\n",
            "Train >>>> Loss: 5.93675\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/26\n",
            "Train >>>> Loss: 5.91466\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/27\n",
            "Train >>>> Loss: 5.90479\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/28\n",
            "Train >>>> Loss: 5.91926\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/29\n",
            "Train >>>> Loss: 5.87347\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/30\n",
            "Train >>>> Loss: 5.90523\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/31\n",
            "Train >>>> Loss: 5.89839\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/32\n",
            "Train >>>> Loss: 5.8829\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/33\n",
            "Train >>>> Loss: 5.89865\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/34\n",
            "Train >>>> Loss: 5.88805\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 12/35\n",
            "Train >>>> Loss: 5.9334\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.91139\n",
            "[Average Testing Loss]: 5.86395\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/0\n",
            "Train >>>> Loss: 5.86231\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/1\n",
            "Train >>>> Loss: 5.88238\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/2\n",
            "Train >>>> Loss: 5.90295\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/3\n",
            "Train >>>> Loss: 5.90365\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/4\n",
            "Train >>>> Loss: 5.92693\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/5\n",
            "Train >>>> Loss: 5.92766\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/6\n",
            "Train >>>> Loss: 5.9114\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/7\n",
            "Train >>>> Loss: 5.90982\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/8\n",
            "Train >>>> Loss: 5.89803\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/9\n",
            "Train >>>> Loss: 5.88891\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/10\n",
            "Train >>>> Loss: 5.89575\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/11\n",
            "Train >>>> Loss: 5.92243\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/12\n",
            "Train >>>> Loss: 5.91047\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/13\n",
            "Train >>>> Loss: 5.90939\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/14\n",
            "Train >>>> Loss: 5.89462\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/15\n",
            "Train >>>> Loss: 5.88918\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/16\n",
            "Train >>>> Loss: 5.93309\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/17\n",
            "Train >>>> Loss: 5.88378\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/18\n",
            "Train >>>> Loss: 5.92214\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/19\n",
            "Train >>>> Loss: 5.87135\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/20\n",
            "Train >>>> Loss: 5.91562\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/21\n",
            "Train >>>> Loss: 5.89039\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/22\n",
            "Train >>>> Loss: 5.88215\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/23\n",
            "Train >>>> Loss: 5.88752\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/24\n",
            "Train >>>> Loss: 5.91184\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/25\n",
            "Train >>>> Loss: 5.90633\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/26\n",
            "Train >>>> Loss: 5.92234\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/27\n",
            "Train >>>> Loss: 5.97003\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/28\n",
            "Train >>>> Loss: 5.90231\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/29\n",
            "Train >>>> Loss: 5.90244\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/30\n",
            "Train >>>> Loss: 5.90574\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/31\n",
            "Train >>>> Loss: 5.89836\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/32\n",
            "Train >>>> Loss: 5.89353\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/33\n",
            "Train >>>> Loss: 5.89677\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/34\n",
            "Train >>>> Loss: 5.92314\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 13/35\n",
            "Train >>>> Loss: 5.93559\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.90529\n",
            "[Average Testing Loss]: 5.86076\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/0\n",
            "Train >>>> Loss: 5.91971\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/1\n",
            "Train >>>> Loss: 5.88386\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/2\n",
            "Train >>>> Loss: 5.91279\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/3\n",
            "Train >>>> Loss: 5.91203\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/4\n",
            "Train >>>> Loss: 5.91442\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/5\n",
            "Train >>>> Loss: 5.90671\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/6\n",
            "Train >>>> Loss: 5.89934\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/7\n",
            "Train >>>> Loss: 5.86546\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/8\n",
            "Train >>>> Loss: 5.87327\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/9\n",
            "Train >>>> Loss: 5.92215\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/10\n",
            "Train >>>> Loss: 5.88264\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/11\n",
            "Train >>>> Loss: 5.86871\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/12\n",
            "Train >>>> Loss: 5.88399\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/13\n",
            "Train >>>> Loss: 5.92443\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/14\n",
            "Train >>>> Loss: 5.85637\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/15\n",
            "Train >>>> Loss: 5.89327\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/16\n",
            "Train >>>> Loss: 5.89727\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/17\n",
            "Train >>>> Loss: 5.87318\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/18\n",
            "Train >>>> Loss: 5.92045\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/19\n",
            "Train >>>> Loss: 5.86905\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/20\n",
            "Train >>>> Loss: 5.92935\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/21\n",
            "Train >>>> Loss: 5.89708\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/22\n",
            "Train >>>> Loss: 5.91576\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/23\n",
            "Train >>>> Loss: 5.88385\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/24\n",
            "Train >>>> Loss: 5.89668\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/25\n",
            "Train >>>> Loss: 5.90404\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/26\n",
            "Train >>>> Loss: 5.92915\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/27\n",
            "Train >>>> Loss: 5.8873\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/28\n",
            "Train >>>> Loss: 5.91848\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/29\n",
            "Train >>>> Loss: 5.90387\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/30\n",
            "Train >>>> Loss: 5.9038\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/31\n",
            "Train >>>> Loss: 5.93387\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/32\n",
            "Train >>>> Loss: 5.91479\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/33\n",
            "Train >>>> Loss: 5.86952\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/34\n",
            "Train >>>> Loss: 5.8924\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 14/35\n",
            "Train >>>> Loss: 5.92116\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.89945\n",
            "[Average Testing Loss]: 5.87205\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/0\n",
            "Train >>>> Loss: 5.86568\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/1\n",
            "Train >>>> Loss: 5.90282\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/2\n",
            "Train >>>> Loss: 5.8968\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/3\n",
            "Train >>>> Loss: 5.89559\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/4\n",
            "Train >>>> Loss: 5.88245\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/5\n",
            "Train >>>> Loss: 5.94596\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/6\n",
            "Train >>>> Loss: 5.88898\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/7\n",
            "Train >>>> Loss: 5.87729\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/8\n",
            "Train >>>> Loss: 5.88672\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/9\n",
            "Train >>>> Loss: 5.87332\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/10\n",
            "Train >>>> Loss: 5.89413\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/11\n",
            "Train >>>> Loss: 5.89078\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/12\n",
            "Train >>>> Loss: 5.85196\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/13\n",
            "Train >>>> Loss: 5.92487\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/14\n",
            "Train >>>> Loss: 5.88941\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/15\n",
            "Train >>>> Loss: 5.89286\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/16\n",
            "Train >>>> Loss: 5.92986\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/17\n",
            "Train >>>> Loss: 5.8936\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/18\n",
            "Train >>>> Loss: 5.87428\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/19\n",
            "Train >>>> Loss: 5.91712\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/20\n",
            "Train >>>> Loss: 5.87964\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/21\n",
            "Train >>>> Loss: 5.90607\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/22\n",
            "Train >>>> Loss: 5.93014\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/23\n",
            "Train >>>> Loss: 5.90813\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/24\n",
            "Train >>>> Loss: 5.91336\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/25\n",
            "Train >>>> Loss: 5.89679\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/26\n",
            "Train >>>> Loss: 5.9076\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/27\n",
            "Train >>>> Loss: 5.85738\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/28\n",
            "Train >>>> Loss: 5.8971\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/29\n",
            "Train >>>> Loss: 5.90488\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/30\n",
            "Train >>>> Loss: 5.89087\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/31\n",
            "Train >>>> Loss: 5.90166\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/32\n",
            "Train >>>> Loss: 5.89844\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/33\n",
            "Train >>>> Loss: 5.89413\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/34\n",
            "Train >>>> Loss: 5.89018\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 15/35\n",
            "Train >>>> Loss: 5.89843\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.89581\n",
            "[Average Testing Loss]: 5.86078\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/0\n",
            "Train >>>> Loss: 5.89717\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/1\n",
            "Train >>>> Loss: 5.91629\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/2\n",
            "Train >>>> Loss: 5.91646\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/3\n",
            "Train >>>> Loss: 5.89255\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/4\n",
            "Train >>>> Loss: 5.88846\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/5\n",
            "Train >>>> Loss: 5.90769\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/6\n",
            "Train >>>> Loss: 5.88678\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/7\n",
            "Train >>>> Loss: 5.90777\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/8\n",
            "Train >>>> Loss: 5.9136\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/9\n",
            "Train >>>> Loss: 5.89218\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/10\n",
            "Train >>>> Loss: 5.90088\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/11\n",
            "Train >>>> Loss: 5.90896\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/12\n",
            "Train >>>> Loss: 5.92046\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/13\n",
            "Train >>>> Loss: 5.91471\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/14\n",
            "Train >>>> Loss: 5.88325\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/15\n",
            "Train >>>> Loss: 5.91788\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/16\n",
            "Train >>>> Loss: 5.87564\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/17\n",
            "Train >>>> Loss: 5.91959\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/18\n",
            "Train >>>> Loss: 5.91701\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/19\n",
            "Train >>>> Loss: 5.9055\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/20\n",
            "Train >>>> Loss: 5.90001\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/21\n",
            "Train >>>> Loss: 5.88207\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/22\n",
            "Train >>>> Loss: 5.90949\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/23\n",
            "Train >>>> Loss: 5.90057\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/24\n",
            "Train >>>> Loss: 5.89865\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/25\n",
            "Train >>>> Loss: 5.89644\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/26\n",
            "Train >>>> Loss: 5.87069\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/27\n",
            "Train >>>> Loss: 5.9016\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/28\n",
            "Train >>>> Loss: 5.89398\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/29\n",
            "Train >>>> Loss: 5.88826\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/30\n",
            "Train >>>> Loss: 5.87752\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/31\n",
            "Train >>>> Loss: 5.90153\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/32\n",
            "Train >>>> Loss: 5.85931\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/33\n",
            "Train >>>> Loss: 5.89502\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/34\n",
            "Train >>>> Loss: 5.89394\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 16/35\n",
            "Train >>>> Loss: 5.88901\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.89836\n",
            "[Average Testing Loss]: 5.85806\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/0\n",
            "Train >>>> Loss: 5.84619\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/1\n",
            "Train >>>> Loss: 5.91019\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/2\n",
            "Train >>>> Loss: 5.88986\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/3\n",
            "Train >>>> Loss: 5.83931\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/4\n",
            "Train >>>> Loss: 5.87874\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/5\n",
            "Train >>>> Loss: 5.88153\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/6\n",
            "Train >>>> Loss: 5.88189\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/7\n",
            "Train >>>> Loss: 5.90488\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/8\n",
            "Train >>>> Loss: 5.86752\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/9\n",
            "Train >>>> Loss: 5.8817\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/10\n",
            "Train >>>> Loss: 5.88266\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/11\n",
            "Train >>>> Loss: 5.8367\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/12\n",
            "Train >>>> Loss: 5.88565\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/13\n",
            "Train >>>> Loss: 5.90481\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/14\n",
            "Train >>>> Loss: 5.88331\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/15\n",
            "Train >>>> Loss: 5.93641\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/16\n",
            "Train >>>> Loss: 5.88772\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/17\n",
            "Train >>>> Loss: 5.9149\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/18\n",
            "Train >>>> Loss: 5.91743\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/19\n",
            "Train >>>> Loss: 5.93449\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/20\n",
            "Train >>>> Loss: 5.89943\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/21\n",
            "Train >>>> Loss: 5.93025\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/22\n",
            "Train >>>> Loss: 5.87811\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/23\n",
            "Train >>>> Loss: 5.91414\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/24\n",
            "Train >>>> Loss: 5.89053\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/25\n",
            "Train >>>> Loss: 5.88766\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/26\n",
            "Train >>>> Loss: 5.91757\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/27\n",
            "Train >>>> Loss: 5.9208\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/28\n",
            "Train >>>> Loss: 5.91938\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/29\n",
            "Train >>>> Loss: 5.8812\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/30\n",
            "Train >>>> Loss: 5.91033\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/31\n",
            "Train >>>> Loss: 5.8746\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/32\n",
            "Train >>>> Loss: 5.86076\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/33\n",
            "Train >>>> Loss: 5.87257\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/34\n",
            "Train >>>> Loss: 5.89291\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 17/35\n",
            "Train >>>> Loss: 5.91243\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.89246\n",
            "[Average Testing Loss]: 5.87175\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/0\n",
            "Train >>>> Loss: 5.87372\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/1\n",
            "Train >>>> Loss: 5.8731\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/2\n",
            "Train >>>> Loss: 5.90407\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/3\n",
            "Train >>>> Loss: 5.87765\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/4\n",
            "Train >>>> Loss: 5.89753\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/5\n",
            "Train >>>> Loss: 5.93777\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/6\n",
            "Train >>>> Loss: 5.8523\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/7\n",
            "Train >>>> Loss: 5.90584\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/8\n",
            "Train >>>> Loss: 5.89578\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/9\n",
            "Train >>>> Loss: 5.86228\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/10\n",
            "Train >>>> Loss: 5.91612\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/11\n",
            "Train >>>> Loss: 5.91715\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/12\n",
            "Train >>>> Loss: 5.91956\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/13\n",
            "Train >>>> Loss: 5.93423\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/14\n",
            "Train >>>> Loss: 5.86858\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/15\n",
            "Train >>>> Loss: 5.90524\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/16\n",
            "Train >>>> Loss: 5.93008\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/17\n",
            "Train >>>> Loss: 5.90655\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/18\n",
            "Train >>>> Loss: 5.87633\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/19\n",
            "Train >>>> Loss: 5.91859\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/20\n",
            "Train >>>> Loss: 5.90755\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/21\n",
            "Train >>>> Loss: 5.89493\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/22\n",
            "Train >>>> Loss: 5.88963\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/23\n",
            "Train >>>> Loss: 5.91284\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/24\n",
            "Train >>>> Loss: 5.86751\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/25\n",
            "Train >>>> Loss: 5.8748\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/26\n",
            "Train >>>> Loss: 5.91528\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/27\n",
            "Train >>>> Loss: 5.83849\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/28\n",
            "Train >>>> Loss: 5.9103\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/29\n",
            "Train >>>> Loss: 5.90889\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/30\n",
            "Train >>>> Loss: 5.88417\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/31\n",
            "Train >>>> Loss: 5.88138\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/32\n",
            "Train >>>> Loss: 5.91014\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/33\n",
            "Train >>>> Loss: 5.93756\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/34\n",
            "Train >>>> Loss: 5.90293\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 18/35\n",
            "Train >>>> Loss: 5.88757\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.89712\n",
            "[Average Testing Loss]: 5.87832\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/0\n",
            "Train >>>> Loss: 5.87837\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/1\n",
            "Train >>>> Loss: 5.91835\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/2\n",
            "Train >>>> Loss: 5.90375\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/3\n",
            "Train >>>> Loss: 5.8682\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/4\n",
            "Train >>>> Loss: 5.90394\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/5\n",
            "Train >>>> Loss: 5.86335\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/6\n",
            "Train >>>> Loss: 5.90905\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/7\n",
            "Train >>>> Loss: 5.91015\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/8\n",
            "Train >>>> Loss: 5.92235\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/9\n",
            "Train >>>> Loss: 5.90346\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/10\n",
            "Train >>>> Loss: 5.88742\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/11\n",
            "Train >>>> Loss: 5.9219\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/12\n",
            "Train >>>> Loss: 5.92728\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/13\n",
            "Train >>>> Loss: 5.9212\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/14\n",
            "Train >>>> Loss: 5.87548\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/15\n",
            "Train >>>> Loss: 5.86021\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/16\n",
            "Train >>>> Loss: 5.87703\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/17\n",
            "Train >>>> Loss: 5.90487\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/18\n",
            "Train >>>> Loss: 5.91767\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/19\n",
            "Train >>>> Loss: 5.84408\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/20\n",
            "Train >>>> Loss: 5.86587\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/21\n",
            "Train >>>> Loss:  5.913\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/22\n",
            "Train >>>> Loss: 5.90611\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/23\n",
            "Train >>>> Loss: 5.89791\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/24\n",
            "Train >>>> Loss: 5.87513\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/25\n",
            "Train >>>> Loss: 5.88867\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/26\n",
            "Train >>>> Loss: 5.90297\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/27\n",
            "Train >>>> Loss: 5.83827\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/28\n",
            "Train >>>> Loss: 5.88355\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/29\n",
            "Train >>>> Loss: 5.89999\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/30\n",
            "Train >>>> Loss: 5.89487\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/31\n",
            "Train >>>> Loss: 5.90912\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/32\n",
            "Train >>>> Loss: 5.87727\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/33\n",
            "Train >>>> Loss: 5.90538\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/34\n",
            "Train >>>> Loss: 5.84639\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 19/35\n",
            "Train >>>> Loss: 5.88331\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.89183\n",
            "[Average Testing Loss]: 5.85437\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/0\n",
            "Train >>>> Loss: 5.89767\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/1\n",
            "Train >>>> Loss: 5.85333\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/2\n",
            "Train >>>> Loss: 5.88123\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/3\n",
            "Train >>>> Loss: 5.9487\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/4\n",
            "Train >>>> Loss: 5.87598\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/5\n",
            "Train >>>> Loss: 5.89588\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/6\n",
            "Train >>>> Loss: 5.87349\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/7\n",
            "Train >>>> Loss: 5.88318\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/8\n",
            "Train >>>> Loss: 5.91355\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/9\n",
            "Train >>>> Loss: 5.94312\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/10\n",
            "Train >>>> Loss: 5.90881\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/11\n",
            "Train >>>> Loss: 5.90031\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/12\n",
            "Train >>>> Loss: 5.90903\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/13\n",
            "Train >>>> Loss: 5.8999\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/14\n",
            "Train >>>> Loss: 5.91034\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/15\n",
            "Train >>>> Loss: 5.91785\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/16\n",
            "Train >>>> Loss: 5.90993\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/17\n",
            "Train >>>> Loss: 5.89533\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/18\n",
            "Train >>>> Loss: 5.86291\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/19\n",
            "Train >>>> Loss: 5.8788\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/20\n",
            "Train >>>> Loss: 5.93179\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/21\n",
            "Train >>>> Loss: 5.8742\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/22\n",
            "Train >>>> Loss: 5.9208\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/23\n",
            "Train >>>> Loss: 5.89133\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/24\n",
            "Train >>>> Loss: 5.92224\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/25\n",
            "Train >>>> Loss: 5.88181\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/26\n",
            "Train >>>> Loss: 5.89942\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/27\n",
            "Train >>>> Loss: 5.85717\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/28\n",
            "Train >>>> Loss: 5.8868\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/29\n",
            "Train >>>> Loss: 5.87708\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/30\n",
            "Train >>>> Loss:  5.959\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/31\n",
            "Train >>>> Loss: 5.92042\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/32\n",
            "Train >>>> Loss: 5.89086\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/33\n",
            "Train >>>> Loss:  5.897\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/34\n",
            "Train >>>> Loss: 5.87811\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 20/35\n",
            "Train >>>> Loss: 5.87085\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.89773\n",
            "[Average Testing Loss]: 5.8647\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/0\n",
            "Train >>>> Loss: 5.89785\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/1\n",
            "Train >>>> Loss: 5.89388\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/2\n",
            "Train >>>> Loss: 5.86099\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/3\n",
            "Train >>>> Loss: 5.87577\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/4\n",
            "Train >>>> Loss: 5.90269\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/5\n",
            "Train >>>> Loss: 5.90587\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/6\n",
            "Train >>>> Loss: 5.9186\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/7\n",
            "Train >>>> Loss: 5.89468\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/8\n",
            "Train >>>> Loss: 5.92114\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/9\n",
            "Train >>>> Loss: 5.91057\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/10\n",
            "Train >>>> Loss: 5.89393\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/11\n",
            "Train >>>> Loss: 5.90295\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/12\n",
            "Train >>>> Loss: 5.91636\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/13\n",
            "Train >>>> Loss: 5.91285\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/14\n",
            "Train >>>> Loss: 5.85866\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/15\n",
            "Train >>>> Loss: 5.91847\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/16\n",
            "Train >>>> Loss: 5.89803\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/17\n",
            "Train >>>> Loss: 5.87308\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/18\n",
            "Train >>>> Loss: 5.89998\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/19\n",
            "Train >>>> Loss: 5.88251\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/20\n",
            "Train >>>> Loss: 5.86988\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/21\n",
            "Train >>>> Loss: 5.85947\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/22\n",
            "Train >>>> Loss: 5.9436\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/23\n",
            "Train >>>> Loss: 5.91851\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/24\n",
            "Train >>>> Loss: 5.91896\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/25\n",
            "Train >>>> Loss: 5.89948\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/26\n",
            "Train >>>> Loss: 5.89525\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/27\n",
            "Train >>>> Loss: 5.88129\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/28\n",
            "Train >>>> Loss: 5.87456\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/29\n",
            "Train >>>> Loss: 5.92154\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/30\n",
            "Train >>>> Loss: 5.87492\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/31\n",
            "Train >>>> Loss: 5.87806\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/32\n",
            "Train >>>> Loss: 5.92785\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/33\n",
            "Train >>>> Loss: 5.88293\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/34\n",
            "Train >>>> Loss: 5.86846\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 21/35\n",
            "Train >>>> Loss: 5.88289\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.89546\n",
            "[Average Testing Loss]: 5.86456\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/0\n",
            "Train >>>> Loss: 5.87817\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/1\n",
            "Train >>>> Loss: 5.85089\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/2\n",
            "Train >>>> Loss: 5.93003\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/3\n",
            "Train >>>> Loss: 5.88169\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/4\n",
            "Train >>>> Loss: 5.87395\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/5\n",
            "Train >>>> Loss: 5.88969\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/6\n",
            "Train >>>> Loss: 5.94359\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/7\n",
            "Train >>>> Loss: 5.93277\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/8\n",
            "Train >>>> Loss: 5.87775\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/9\n",
            "Train >>>> Loss: 5.88677\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/10\n",
            "Train >>>> Loss: 5.93984\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/11\n",
            "Train >>>> Loss: 5.90069\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/12\n",
            "Train >>>> Loss: 5.92753\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/13\n",
            "Train >>>> Loss: 5.9012\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/14\n",
            "Train >>>> Loss: 5.91466\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/15\n",
            "Train >>>> Loss: 5.90682\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/16\n",
            "Train >>>> Loss: 5.88475\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/17\n",
            "Train >>>> Loss: 5.89283\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/18\n",
            "Train >>>> Loss: 5.89144\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/19\n",
            "Train >>>> Loss:  5.901\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/20\n",
            "Train >>>> Loss: 5.86243\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/21\n",
            "Train >>>> Loss: 5.88008\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/22\n",
            "Train >>>> Loss: 5.87699\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/23\n",
            "Train >>>> Loss: 5.93573\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/24\n",
            "Train >>>> Loss: 5.85867\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/25\n",
            "Train >>>> Loss: 5.86569\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/26\n",
            "Train >>>> Loss: 5.9025\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/27\n",
            "Train >>>> Loss: 5.93281\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/28\n",
            "Train >>>> Loss: 5.87153\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/29\n",
            "Train >>>> Loss: 5.89197\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/30\n",
            "Train >>>> Loss: 5.93175\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/31\n",
            "Train >>>> Loss: 5.89746\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/32\n",
            "Train >>>> Loss: 5.90163\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/33\n",
            "Train >>>> Loss: 5.9288\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/34\n",
            "Train >>>> Loss: 5.90002\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 22/35\n",
            "Train >>>> Loss: 5.89873\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.89841\n",
            "[Average Testing Loss]: 5.87601\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/0\n",
            "Train >>>> Loss: 5.90333\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/1\n",
            "Train >>>> Loss: 5.92951\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/2\n",
            "Train >>>> Loss: 5.88615\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/3\n",
            "Train >>>> Loss: 5.93356\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/4\n",
            "Train >>>> Loss: 5.9153\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/5\n",
            "Train >>>> Loss: 5.88481\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/6\n",
            "Train >>>> Loss: 5.88187\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/7\n",
            "Train >>>> Loss: 5.90924\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/8\n",
            "Train >>>> Loss: 5.86328\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/9\n",
            "Train >>>> Loss: 5.88888\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/10\n",
            "Train >>>> Loss: 5.95598\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/11\n",
            "Train >>>> Loss: 5.90343\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/12\n",
            "Train >>>> Loss: 5.86177\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/13\n",
            "Train >>>> Loss: 5.8946\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/14\n",
            "Train >>>> Loss: 5.93374\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/15\n",
            "Train >>>> Loss: 5.86372\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/16\n",
            "Train >>>> Loss: 5.90451\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/17\n",
            "Train >>>> Loss: 5.88417\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/18\n",
            "Train >>>> Loss: 5.87294\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/19\n",
            "Train >>>> Loss: 5.91604\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/20\n",
            "Train >>>> Loss: 5.85949\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/21\n",
            "Train >>>> Loss: 5.89372\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/22\n",
            "Train >>>> Loss: 5.88863\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/23\n",
            "Train >>>> Loss: 5.86893\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/24\n",
            "Train >>>> Loss: 5.88002\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/25\n",
            "Train >>>> Loss: 5.88989\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/26\n",
            "Train >>>> Loss: 5.89828\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/27\n",
            "Train >>>> Loss: 5.90653\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/28\n",
            "Train >>>> Loss: 5.91613\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/29\n",
            "Train >>>> Loss: 5.88441\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/30\n",
            "Train >>>> Loss: 5.91157\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/31\n",
            "Train >>>> Loss: 5.86037\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/32\n",
            "Train >>>> Loss: 5.8666\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/33\n",
            "Train >>>> Loss: 5.87855\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/34\n",
            "Train >>>> Loss: 5.90241\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 23/35\n",
            "Train >>>> Loss: 5.88096\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.8937\n",
            "[Average Testing Loss]: 5.8555\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/0\n",
            "Train >>>> Loss: 5.87771\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/1\n",
            "Train >>>> Loss: 5.87418\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/2\n",
            "Train >>>> Loss: 5.90897\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/3\n",
            "Train >>>> Loss: 5.89573\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/4\n",
            "Train >>>> Loss: 5.88572\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/5\n",
            "Train >>>> Loss: 5.88144\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/6\n",
            "Train >>>> Loss: 5.9065\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/7\n",
            "Train >>>> Loss: 5.87783\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/8\n",
            "Train >>>> Loss: 5.88762\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/9\n",
            "Train >>>> Loss: 5.92004\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/10\n",
            "Train >>>> Loss: 5.88423\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/11\n",
            "Train >>>> Loss: 5.90477\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/12\n",
            "Train >>>> Loss: 5.88366\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/13\n",
            "Train >>>> Loss: 5.87061\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/14\n",
            "Train >>>> Loss: 5.90343\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/15\n",
            "Train >>>> Loss: 5.90198\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/16\n",
            "Train >>>> Loss: 5.92864\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/17\n",
            "Train >>>> Loss: 5.91349\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/18\n",
            "Train >>>> Loss: 5.88497\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/19\n",
            "Train >>>> Loss: 5.86037\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/20\n",
            "Train >>>> Loss: 5.87125\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/21\n",
            "Train >>>> Loss: 5.89144\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/22\n",
            "Train >>>> Loss: 5.87978\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/23\n",
            "Train >>>> Loss: 5.85361\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/24\n",
            "Train >>>> Loss: 5.87362\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/25\n",
            "Train >>>> Loss: 5.94779\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/26\n",
            "Train >>>> Loss: 5.90353\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/27\n",
            "Train >>>> Loss: 5.88509\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/28\n",
            "Train >>>> Loss: 5.88626\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/29\n",
            "Train >>>> Loss: 5.91405\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/30\n",
            "Train >>>> Loss: 5.88412\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/31\n",
            "Train >>>> Loss: 5.89009\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/32\n",
            "Train >>>> Loss: 5.91269\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/33\n",
            "Train >>>> Loss: 5.89501\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/34\n",
            "Train >>>> Loss: 5.91153\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 24/35\n",
            "Train >>>> Loss: 5.87286\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.89235\n",
            "[Average Testing Loss]: 5.8794\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/0\n",
            "Train >>>> Loss: 5.90567\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/1\n",
            "Train >>>> Loss: 5.88184\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/2\n",
            "Train >>>> Loss: 5.85938\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/3\n",
            "Train >>>> Loss: 5.84593\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/4\n",
            "Train >>>> Loss: 5.89333\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/5\n",
            "Train >>>> Loss: 5.92414\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/6\n",
            "Train >>>> Loss: 5.87824\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/7\n",
            "Train >>>> Loss: 5.90745\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/8\n",
            "Train >>>> Loss: 5.89544\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/9\n",
            "Train >>>> Loss: 5.87853\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/10\n",
            "Train >>>> Loss: 5.90892\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/11\n",
            "Train >>>> Loss: 5.86062\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/12\n",
            "Train >>>> Loss: 5.87426\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/13\n",
            "Train >>>> Loss: 5.9032\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/14\n",
            "Train >>>> Loss: 5.89516\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/15\n",
            "Train >>>> Loss: 5.90266\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/16\n",
            "Train >>>> Loss: 5.87376\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/17\n",
            "Train >>>> Loss: 5.87265\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/18\n",
            "Train >>>> Loss: 5.88394\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/19\n",
            "Train >>>> Loss: 5.90096\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/20\n",
            "Train >>>> Loss: 5.89611\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/21\n",
            "Train >>>> Loss: 5.91358\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/22\n",
            "Train >>>> Loss: 5.89803\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/23\n",
            "Train >>>> Loss: 5.89204\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/24\n",
            "Train >>>> Loss: 5.85386\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/25\n",
            "Train >>>> Loss: 5.87054\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/26\n",
            "Train >>>> Loss: 5.8851\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/27\n",
            "Train >>>> Loss: 5.89391\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/28\n",
            "Train >>>> Loss: 5.87515\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/29\n",
            "Train >>>> Loss: 5.90477\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/30\n",
            "Train >>>> Loss: 5.88768\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/31\n",
            "Train >>>> Loss: 5.87054\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/32\n",
            "Train >>>> Loss: 5.90554\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/33\n",
            "Train >>>> Loss: 5.85636\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/34\n",
            "Train >>>> Loss: 5.86206\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 25/35\n",
            "Train >>>> Loss: 5.88773\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.88608\n",
            "[Average Testing Loss]: 5.86867\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/0\n",
            "Train >>>> Loss: 5.90814\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/1\n",
            "Train >>>> Loss: 5.92654\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/2\n",
            "Train >>>> Loss: 5.89355\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/3\n",
            "Train >>>> Loss: 5.89856\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/4\n",
            "Train >>>> Loss: 5.90984\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/5\n",
            "Train >>>> Loss: 5.91255\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/6\n",
            "Train >>>> Loss: 5.90345\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/7\n",
            "Train >>>> Loss: 5.89103\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/8\n",
            "Train >>>> Loss: 5.92862\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/9\n",
            "Train >>>> Loss: 5.87797\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/10\n",
            "Train >>>> Loss: 5.92199\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/11\n",
            "Train >>>> Loss: 5.90374\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/12\n",
            "Train >>>> Loss: 5.91611\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/13\n",
            "Train >>>> Loss: 5.91534\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/14\n",
            "Train >>>> Loss: 5.91552\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/15\n",
            "Train >>>> Loss: 5.88161\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/16\n",
            "Train >>>> Loss: 5.91956\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/17\n",
            "Train >>>> Loss: 5.87217\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/18\n",
            "Train >>>> Loss: 5.88753\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/19\n",
            "Train >>>> Loss: 5.84589\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/20\n",
            "Train >>>> Loss: 5.89722\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/21\n",
            "Train >>>> Loss: 5.92232\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/22\n",
            "Train >>>> Loss: 5.90525\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/23\n",
            "Train >>>> Loss: 5.88767\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/24\n",
            "Train >>>> Loss: 5.9101\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/25\n",
            "Train >>>> Loss: 5.88156\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/26\n",
            "Train >>>> Loss: 5.89353\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/27\n",
            "Train >>>> Loss: 5.88375\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/28\n",
            "Train >>>> Loss: 5.91693\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/29\n",
            "Train >>>> Loss: 5.85667\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/30\n",
            "Train >>>> Loss: 5.92845\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/31\n",
            "Train >>>> Loss: 5.87928\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/32\n",
            "Train >>>> Loss: 5.9381\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/33\n",
            "Train >>>> Loss: 5.84279\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/34\n",
            "Train >>>> Loss: 5.92774\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 26/35\n",
            "Train >>>> Loss: 5.85085\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.89866\n",
            "[Average Testing Loss]: 5.84258\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/0\n",
            "Train >>>> Loss: 5.8906\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/1\n",
            "Train >>>> Loss: 5.85535\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/2\n",
            "Train >>>> Loss: 5.85816\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/3\n",
            "Train >>>> Loss: 5.91553\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/4\n",
            "Train >>>> Loss: 5.8747\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/5\n",
            "Train >>>> Loss: 5.9047\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/6\n",
            "Train >>>> Loss: 5.86614\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/7\n",
            "Train >>>> Loss: 5.8803\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/8\n",
            "Train >>>> Loss: 5.91522\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/9\n",
            "Train >>>> Loss: 5.93325\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/10\n",
            "Train >>>> Loss: 5.85444\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/11\n",
            "Train >>>> Loss: 5.89887\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/12\n",
            "Train >>>> Loss: 5.90398\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/13\n",
            "Train >>>> Loss: 5.91296\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/14\n",
            "Train >>>> Loss: 5.88009\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/15\n",
            "Train >>>> Loss: 5.89664\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/16\n",
            "Train >>>> Loss: 5.9081\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/17\n",
            "Train >>>> Loss: 5.87029\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/18\n",
            "Train >>>> Loss: 5.88312\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/19\n",
            "Train >>>> Loss: 5.84468\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/20\n",
            "Train >>>> Loss: 5.90938\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/21\n",
            "Train >>>> Loss: 5.87831\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/22\n",
            "Train >>>> Loss: 5.86869\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/23\n",
            "Train >>>> Loss: 5.95958\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/24\n",
            "Train >>>> Loss: 5.87948\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/25\n",
            "Train >>>> Loss: 5.87502\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/26\n",
            "Train >>>> Loss: 5.90694\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/27\n",
            "Train >>>> Loss: 5.88104\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/28\n",
            "Train >>>> Loss: 5.84549\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/29\n",
            "Train >>>> Loss: 5.90437\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/30\n",
            "Train >>>> Loss: 5.86962\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/31\n",
            "Train >>>> Loss: 5.84653\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/32\n",
            "Train >>>> Loss: 5.90613\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/33\n",
            "Train >>>> Loss: 5.90468\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/34\n",
            "Train >>>> Loss: 5.87848\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 27/35\n",
            "Train >>>> Loss: 5.87435\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.88709\n",
            "[Average Testing Loss]: 5.84917\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/0\n",
            "Train >>>> Loss: 5.8871\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/1\n",
            "Train >>>> Loss: 5.91286\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/2\n",
            "Train >>>> Loss: 5.86193\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/3\n",
            "Train >>>> Loss: 5.85936\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/4\n",
            "Train >>>> Loss: 5.91267\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/5\n",
            "Train >>>> Loss: 5.86651\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/6\n",
            "Train >>>> Loss: 5.9288\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/7\n",
            "Train >>>> Loss: 5.91784\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/8\n",
            "Train >>>> Loss: 5.85206\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/9\n",
            "Train >>>> Loss: 5.89469\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/10\n",
            "Train >>>> Loss: 5.88806\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/11\n",
            "Train >>>> Loss: 5.86682\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/12\n",
            "Train >>>> Loss: 5.91866\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/13\n",
            "Train >>>> Loss: 5.90609\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/14\n",
            "Train >>>> Loss: 5.84419\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/15\n",
            "Train >>>> Loss: 5.88715\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/16\n",
            "Train >>>> Loss: 5.88631\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/17\n",
            "Train >>>> Loss: 5.88725\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/18\n",
            "Train >>>> Loss: 5.88558\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/19\n",
            "Train >>>> Loss: 5.89846\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/20\n",
            "Train >>>> Loss: 5.89949\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/21\n",
            "Train >>>> Loss: 5.89376\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/22\n",
            "Train >>>> Loss: 5.91385\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/23\n",
            "Train >>>> Loss: 5.90204\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/24\n",
            "Train >>>> Loss: 5.86023\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/25\n",
            "Train >>>> Loss: 5.89623\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/26\n",
            "Train >>>> Loss: 5.91654\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/27\n",
            "Train >>>> Loss: 5.88375\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/28\n",
            "Train >>>> Loss: 5.89236\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/29\n",
            "Train >>>> Loss: 5.94342\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/30\n",
            "Train >>>> Loss: 5.87434\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/31\n",
            "Train >>>> Loss: 5.88261\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/32\n",
            "Train >>>> Loss: 5.90692\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/33\n",
            "Train >>>> Loss: 5.90947\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/34\n",
            "Train >>>> Loss: 5.91839\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 28/35\n",
            "Train >>>> Loss: 5.87422\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.8925\n",
            "[Average Testing Loss]: 5.86361\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/0\n",
            "Train >>>> Loss: 5.8895\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/1\n",
            "Train >>>> Loss: 5.88944\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/2\n",
            "Train >>>> Loss: 5.89575\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/3\n",
            "Train >>>> Loss: 5.8913\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/4\n",
            "Train >>>> Loss: 5.90673\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/5\n",
            "Train >>>> Loss: 5.87437\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/6\n",
            "Train >>>> Loss: 5.90996\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/7\n",
            "Train >>>> Loss: 5.86598\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/8\n",
            "Train >>>> Loss: 5.8944\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/9\n",
            "Train >>>> Loss: 5.88509\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/10\n",
            "Train >>>> Loss: 5.89438\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/11\n",
            "Train >>>> Loss: 5.89767\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/12\n",
            "Train >>>> Loss: 5.85755\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/13\n",
            "Train >>>> Loss: 5.9162\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/14\n",
            "Train >>>> Loss: 5.88886\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/15\n",
            "Train >>>> Loss: 5.90624\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/16\n",
            "Train >>>> Loss: 5.87921\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/17\n",
            "Train >>>> Loss: 5.87121\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/18\n",
            "Train >>>> Loss: 5.90468\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/19\n",
            "Train >>>> Loss: 5.85257\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/20\n",
            "Train >>>> Loss: 5.92661\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/21\n",
            "Train >>>> Loss: 5.91122\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/22\n",
            "Train >>>> Loss: 5.91762\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/23\n",
            "Train >>>> Loss: 5.9032\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/24\n",
            "Train >>>> Loss: 5.89461\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/25\n",
            "Train >>>> Loss: 5.85864\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/26\n",
            "Train >>>> Loss: 5.89479\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/27\n",
            "Train >>>> Loss: 5.84562\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/28\n",
            "Train >>>> Loss: 5.89516\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/29\n",
            "Train >>>> Loss: 5.89811\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/30\n",
            "Train >>>> Loss: 5.88847\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/31\n",
            "Train >>>> Loss: 5.88354\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/32\n",
            "Train >>>> Loss: 5.8737\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/33\n",
            "Train >>>> Loss: 5.87699\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/34\n",
            "Train >>>> Loss: 5.92371\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 29/35\n",
            "Train >>>> Loss: 5.88677\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.89027\n",
            "[Average Testing Loss]: 5.86679\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/0\n",
            "Train >>>> Loss: 5.90458\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/1\n",
            "Train >>>> Loss: 5.87297\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/2\n",
            "Train >>>> Loss: 5.91584\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/3\n",
            "Train >>>> Loss: 5.89951\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/4\n",
            "Train >>>> Loss: 5.86359\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/5\n",
            "Train >>>> Loss: 5.87158\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/6\n",
            "Train >>>> Loss: 5.95016\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/7\n",
            "Train >>>> Loss: 5.90039\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/8\n",
            "Train >>>> Loss: 5.86607\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/9\n",
            "Train >>>> Loss: 5.89563\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/10\n",
            "Train >>>> Loss: 5.9152\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/11\n",
            "Train >>>> Loss: 5.8838\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/12\n",
            "Train >>>> Loss: 5.90536\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/13\n",
            "Train >>>> Loss: 5.89732\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/14\n",
            "Train >>>> Loss: 5.87854\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/15\n",
            "Train >>>> Loss: 5.86718\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/16\n",
            "Train >>>> Loss: 5.91712\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/17\n",
            "Train >>>> Loss: 5.92798\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/18\n",
            "Train >>>> Loss: 5.87221\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/19\n",
            "Train >>>> Loss: 5.90906\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/20\n",
            "Train >>>> Loss: 5.89603\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/21\n",
            "Train >>>> Loss: 5.89752\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/22\n",
            "Train >>>> Loss: 5.91014\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/23\n",
            "Train >>>> Loss: 5.87245\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/24\n",
            "Train >>>> Loss: 5.86353\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/25\n",
            "Train >>>> Loss: 5.86631\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/26\n",
            "Train >>>> Loss: 5.88121\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/27\n",
            "Train >>>> Loss: 5.89631\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/28\n",
            "Train >>>> Loss: 5.87752\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/29\n",
            "Train >>>> Loss: 5.93831\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/30\n",
            "Train >>>> Loss: 5.91595\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/31\n",
            "Train >>>> Loss: 5.87556\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/32\n",
            "Train >>>> Loss: 5.85605\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/33\n",
            "Train >>>> Loss: 5.87206\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/34\n",
            "Train >>>> Loss: 5.91394\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 30/35\n",
            "Train >>>> Loss: 5.85469\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.89171\n",
            "[Average Testing Loss]: 5.84407\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/0\n",
            "Train >>>> Loss: 5.92691\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/1\n",
            "Train >>>> Loss: 5.89554\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/2\n",
            "Train >>>> Loss: 5.90751\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/3\n",
            "Train >>>> Loss: 5.9099\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/4\n",
            "Train >>>> Loss: 5.8992\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/5\n",
            "Train >>>> Loss: 5.88027\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/6\n",
            "Train >>>> Loss: 5.88917\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/7\n",
            "Train >>>> Loss: 5.90549\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/8\n",
            "Train >>>> Loss: 5.89827\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/9\n",
            "Train >>>> Loss: 5.89376\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/10\n",
            "Train >>>> Loss: 5.89894\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/11\n",
            "Train >>>> Loss: 5.9201\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/12\n",
            "Train >>>> Loss: 5.86001\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/13\n",
            "Train >>>> Loss: 5.87972\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/14\n",
            "Train >>>> Loss: 5.88553\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/15\n",
            "Train >>>> Loss: 5.87563\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/16\n",
            "Train >>>> Loss: 5.86675\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/17\n",
            "Train >>>> Loss: 5.9129\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/18\n",
            "Train >>>> Loss: 5.82533\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/19\n",
            "Train >>>> Loss: 5.9022\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/20\n",
            "Train >>>> Loss: 5.95041\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/21\n",
            "Train >>>> Loss: 5.83892\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/22\n",
            "Train >>>> Loss: 5.89673\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/23\n",
            "Train >>>> Loss: 5.90768\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/24\n",
            "Train >>>> Loss: 5.88477\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/25\n",
            "Train >>>> Loss: 5.90811\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/26\n",
            "Train >>>> Loss: 5.84425\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/27\n",
            "Train >>>> Loss: 5.87054\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/28\n",
            "Train >>>> Loss: 5.87733\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/29\n",
            "Train >>>> Loss: 5.93542\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/30\n",
            "Train >>>> Loss: 5.85528\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/31\n",
            "Train >>>> Loss: 5.90309\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/32\n",
            "Train >>>> Loss: 5.91703\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/33\n",
            "Train >>>> Loss: 5.87244\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/34\n",
            "Train >>>> Loss: 5.86893\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 31/35\n",
            "Train >>>> Loss: 5.86511\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.8897\n",
            "[Average Testing Loss]: 5.89052\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/0\n",
            "Train >>>> Loss: 5.90006\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/1\n",
            "Train >>>> Loss: 5.88713\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/2\n",
            "Train >>>> Loss: 5.89977\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/3\n",
            "Train >>>> Loss: 5.87206\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/4\n",
            "Train >>>> Loss: 5.88675\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/5\n",
            "Train >>>> Loss: 5.84077\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/6\n",
            "Train >>>> Loss: 5.86454\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/7\n",
            "Train >>>> Loss: 5.87485\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/8\n",
            "Train >>>> Loss: 5.8913\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/9\n",
            "Train >>>> Loss: 5.86219\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/10\n",
            "Train >>>> Loss: 5.89679\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/11\n",
            "Train >>>> Loss: 5.87203\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/12\n",
            "Train >>>> Loss: 5.88433\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/13\n",
            "Train >>>> Loss: 5.86709\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/14\n",
            "Train >>>> Loss: 5.88744\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/15\n",
            "Train >>>> Loss: 5.91655\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/16\n",
            "Train >>>> Loss: 5.88716\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/17\n",
            "Train >>>> Loss: 5.88465\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/18\n",
            "Train >>>> Loss: 5.86587\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/19\n",
            "Train >>>> Loss: 5.93756\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/20\n",
            "Train >>>> Loss: 5.85989\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/21\n",
            "Train >>>> Loss: 5.89238\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/22\n",
            "Train >>>> Loss: 5.86604\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/23\n",
            "Train >>>> Loss: 5.90481\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/24\n",
            "Train >>>> Loss: 5.86737\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/25\n",
            "Train >>>> Loss: 5.89312\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/26\n",
            "Train >>>> Loss: 5.83987\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/27\n",
            "Train >>>> Loss: 5.87069\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/28\n",
            "Train >>>> Loss: 5.86923\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/29\n",
            "Train >>>> Loss: 5.87202\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/30\n",
            "Train >>>> Loss: 5.88826\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/31\n",
            "Train >>>> Loss: 5.86999\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/32\n",
            "Train >>>> Loss: 5.9206\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/33\n",
            "Train >>>> Loss: 5.91592\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/34\n",
            "Train >>>> Loss: 5.91617\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 32/35\n",
            "Train >>>> Loss: 5.89483\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.88389\n",
            "[Average Testing Loss]: 5.87753\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/0\n",
            "Train >>>> Loss: 5.89069\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/1\n",
            "Train >>>> Loss: 5.84539\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/2\n",
            "Train >>>> Loss: 5.91016\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/3\n",
            "Train >>>> Loss: 5.88355\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/4\n",
            "Train >>>> Loss: 5.84692\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/5\n",
            "Train >>>> Loss: 5.89342\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/6\n",
            "Train >>>> Loss: 5.87487\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/7\n",
            "Train >>>> Loss: 5.85909\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/8\n",
            "Train >>>> Loss: 5.91854\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/9\n",
            "Train >>>> Loss: 5.90202\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/10\n",
            "Train >>>> Loss: 5.91551\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/11\n",
            "Train >>>> Loss: 5.93668\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/12\n",
            "Train >>>> Loss: 5.86825\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/13\n",
            "Train >>>> Loss: 5.87174\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/14\n",
            "Train >>>> Loss: 5.87247\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/15\n",
            "Train >>>> Loss: 5.91879\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/16\n",
            "Train >>>> Loss: 5.90592\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/17\n",
            "Train >>>> Loss: 5.85997\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/18\n",
            "Train >>>> Loss: 5.84785\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/19\n",
            "Train >>>> Loss: 5.88564\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/20\n",
            "Train >>>> Loss: 5.90029\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/21\n",
            "Train >>>> Loss: 5.88637\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/22\n",
            "Train >>>> Loss: 5.90904\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/23\n",
            "Train >>>> Loss: 5.9022\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/24\n",
            "Train >>>> Loss: 5.88122\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/25\n",
            "Train >>>> Loss: 5.87696\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/26\n",
            "Train >>>> Loss: 5.87903\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/27\n",
            "Train >>>> Loss: 5.90417\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/28\n",
            "Train >>>> Loss: 5.89468\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/29\n",
            "Train >>>> Loss: 5.90812\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/30\n",
            "Train >>>> Loss: 5.87068\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/31\n",
            "Train >>>> Loss: 5.84946\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/32\n",
            "Train >>>> Loss: 5.86474\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/33\n",
            "Train >>>> Loss: 5.9066\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/34\n",
            "Train >>>> Loss: 5.85537\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 33/35\n",
            "Train >>>> Loss: 5.88706\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.88565\n",
            "[Average Testing Loss]: 5.88392\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/0\n",
            "Train >>>> Loss: 5.86748\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/1\n",
            "Train >>>> Loss: 5.85724\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/2\n",
            "Train >>>> Loss: 5.88923\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/3\n",
            "Train >>>> Loss: 5.89021\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/4\n",
            "Train >>>> Loss: 5.86561\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/5\n",
            "Train >>>> Loss: 5.87076\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/6\n",
            "Train >>>> Loss: 5.85779\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/7\n",
            "Train >>>> Loss: 5.88867\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/8\n",
            "Train >>>> Loss: 5.89663\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/9\n",
            "Train >>>> Loss: 5.89966\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/10\n",
            "Train >>>> Loss: 5.87951\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/11\n",
            "Train >>>> Loss: 5.86537\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/12\n",
            "Train >>>> Loss: 5.90485\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/13\n",
            "Train >>>> Loss: 5.90569\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/14\n",
            "Train >>>> Loss: 5.87916\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/15\n",
            "Train >>>> Loss: 5.87383\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/16\n",
            "Train >>>> Loss: 5.89865\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/17\n",
            "Train >>>> Loss: 5.90901\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/18\n",
            "Train >>>> Loss: 5.88743\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/19\n",
            "Train >>>> Loss: 5.89888\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/20\n",
            "Train >>>> Loss: 5.86422\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/21\n",
            "Train >>>> Loss: 5.87241\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/22\n",
            "Train >>>> Loss: 5.87261\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/23\n",
            "Train >>>> Loss:  5.915\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/24\n",
            "Train >>>> Loss: 5.91894\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/25\n",
            "Train >>>> Loss: 5.89258\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/26\n",
            "Train >>>> Loss: 5.87906\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/27\n",
            "Train >>>> Loss: 5.87457\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/28\n",
            "Train >>>> Loss: 5.92959\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/29\n",
            "Train >>>> Loss: 5.86306\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/30\n",
            "Train >>>> Loss: 5.89913\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/31\n",
            "Train >>>> Loss: 5.90265\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/32\n",
            "Train >>>> Loss: 5.84156\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/33\n",
            "Train >>>> Loss: 5.8846\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/34\n",
            "Train >>>> Loss: 5.88083\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 34/35\n",
            "Train >>>> Loss: 5.8205\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.88325\n",
            "[Average Testing Loss]: 5.88458\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/0\n",
            "Train >>>> Loss: 5.84929\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/1\n",
            "Train >>>> Loss: 5.86124\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/2\n",
            "Train >>>> Loss: 5.87041\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/3\n",
            "Train >>>> Loss: 5.88095\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/4\n",
            "Train >>>> Loss: 5.88185\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/5\n",
            "Train >>>> Loss: 5.87128\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/6\n",
            "Train >>>> Loss: 5.92432\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/7\n",
            "Train >>>> Loss: 5.89818\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/8\n",
            "Train >>>> Loss: 5.88349\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/9\n",
            "Train >>>> Loss: 5.90077\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/10\n",
            "Train >>>> Loss: 5.87119\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/11\n",
            "Train >>>> Loss: 5.86531\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/12\n",
            "Train >>>> Loss: 5.8722\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/13\n",
            "Train >>>> Loss: 5.86482\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/14\n",
            "Train >>>> Loss: 5.88448\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/15\n",
            "Train >>>> Loss: 5.86424\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/16\n",
            "Train >>>> Loss: 5.89523\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/17\n",
            "Train >>>> Loss: 5.86657\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/18\n",
            "Train >>>> Loss: 5.90024\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/19\n",
            "Train >>>> Loss: 5.86819\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/20\n",
            "Train >>>> Loss: 5.87122\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/21\n",
            "Train >>>> Loss: 5.88135\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/22\n",
            "Train >>>> Loss: 5.86583\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/23\n",
            "Train >>>> Loss: 5.88607\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/24\n",
            "Train >>>> Loss: 5.87856\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/25\n",
            "Train >>>> Loss: 5.89313\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/26\n",
            "Train >>>> Loss: 5.90388\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/27\n",
            "Train >>>> Loss: 5.89879\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/28\n",
            "Train >>>> Loss: 5.87699\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/29\n",
            "Train >>>> Loss: 5.90981\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/30\n",
            "Train >>>> Loss: 5.90228\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/31\n",
            "Train >>>> Loss: 5.88395\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/32\n",
            "Train >>>> Loss: 5.8842\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/33\n",
            "Train >>>> Loss: 5.84589\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/34\n",
            "Train >>>> Loss: 5.84871\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 35/35\n",
            "Train >>>> Loss: 5.88199\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.88019\n",
            "[Average Testing Loss]: 5.8563\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/0\n",
            "Train >>>> Loss: 5.88786\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/1\n",
            "Train >>>> Loss: 5.8757\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/2\n",
            "Train >>>> Loss: 5.8505\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/3\n",
            "Train >>>> Loss: 5.90129\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/4\n",
            "Train >>>> Loss: 5.91014\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/5\n",
            "Train >>>> Loss: 5.91274\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/6\n",
            "Train >>>> Loss: 5.91736\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/7\n",
            "Train >>>> Loss: 5.87924\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/8\n",
            "Train >>>> Loss: 5.84223\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/9\n",
            "Train >>>> Loss: 5.86054\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/10\n",
            "Train >>>> Loss: 5.92036\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/11\n",
            "Train >>>> Loss: 5.87876\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/12\n",
            "Train >>>> Loss: 5.91399\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/13\n",
            "Train >>>> Loss: 5.86253\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/14\n",
            "Train >>>> Loss: 5.89738\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/15\n",
            "Train >>>> Loss: 5.86379\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/16\n",
            "Train >>>> Loss: 5.90252\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/17\n",
            "Train >>>> Loss: 5.90691\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/18\n",
            "Train >>>> Loss: 5.89775\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/19\n",
            "Train >>>> Loss: 5.9019\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/20\n",
            "Train >>>> Loss: 5.88554\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/21\n",
            "Train >>>> Loss: 5.84057\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/22\n",
            "Train >>>> Loss: 5.87893\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/23\n",
            "Train >>>> Loss: 5.87601\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/24\n",
            "Train >>>> Loss: 5.83594\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/25\n",
            "Train >>>> Loss: 5.88593\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/26\n",
            "Train >>>> Loss: 5.85338\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/27\n",
            "Train >>>> Loss: 5.88951\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/28\n",
            "Train >>>> Loss: 5.86624\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/29\n",
            "Train >>>> Loss: 5.9001\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/30\n",
            "Train >>>> Loss: 5.88709\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/31\n",
            "Train >>>> Loss: 5.87522\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/32\n",
            "Train >>>> Loss: 5.85638\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/33\n",
            "Train >>>> Loss: 5.87992\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/34\n",
            "Train >>>> Loss: 5.90311\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 36/35\n",
            "Train >>>> Loss: 5.83928\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.88157\n",
            "[Average Testing Loss]: 5.85774\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/0\n",
            "Train >>>> Loss: 5.91754\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/1\n",
            "Train >>>> Loss: 5.91463\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/2\n",
            "Train >>>> Loss: 5.86591\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/3\n",
            "Train >>>> Loss: 5.89004\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/4\n",
            "Train >>>> Loss: 5.85683\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/5\n",
            "Train >>>> Loss: 5.89126\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/6\n",
            "Train >>>> Loss: 5.85288\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/7\n",
            "Train >>>> Loss: 5.88834\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/8\n",
            "Train >>>> Loss: 5.88475\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/9\n",
            "Train >>>> Loss: 5.87144\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/10\n",
            "Train >>>> Loss: 5.90518\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/11\n",
            "Train >>>> Loss: 5.89217\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/12\n",
            "Train >>>> Loss: 5.93712\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/13\n",
            "Train >>>> Loss: 5.89222\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/14\n",
            "Train >>>> Loss: 5.89183\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/15\n",
            "Train >>>> Loss: 5.89828\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/16\n",
            "Train >>>> Loss: 5.89218\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/17\n",
            "Train >>>> Loss: 5.89453\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/18\n",
            "Train >>>> Loss:  5.859\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/19\n",
            "Train >>>> Loss: 5.92026\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/20\n",
            "Train >>>> Loss: 5.84879\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/21\n",
            "Train >>>> Loss: 5.92808\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/22\n",
            "Train >>>> Loss: 5.89757\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/23\n",
            "Train >>>> Loss: 5.84263\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/24\n",
            "Train >>>> Loss: 5.91715\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/25\n",
            "Train >>>> Loss: 5.86836\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/26\n",
            "Train >>>> Loss: 5.89628\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/27\n",
            "Train >>>> Loss: 5.87427\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/28\n",
            "Train >>>> Loss: 5.87452\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/29\n",
            "Train >>>> Loss: 5.90808\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/30\n",
            "Train >>>> Loss: 5.90991\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/31\n",
            "Train >>>> Loss: 5.88747\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/32\n",
            "Train >>>> Loss: 5.90961\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/33\n",
            "Train >>>> Loss: 5.88558\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/34\n",
            "Train >>>> Loss: 5.85877\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 37/35\n",
            "Train >>>> Loss: 5.90206\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.8896\n",
            "[Average Testing Loss]: 5.86901\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/0\n",
            "Train >>>> Loss: 5.89001\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/1\n",
            "Train >>>> Loss: 5.84974\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/2\n",
            "Train >>>> Loss: 5.89385\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/3\n",
            "Train >>>> Loss: 5.85188\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/4\n",
            "Train >>>> Loss: 5.89106\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/5\n",
            "Train >>>> Loss: 5.9026\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/6\n",
            "Train >>>> Loss: 5.86623\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/7\n",
            "Train >>>> Loss: 5.87161\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/8\n",
            "Train >>>> Loss: 5.88581\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/9\n",
            "Train >>>> Loss: 5.90105\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/10\n",
            "Train >>>> Loss: 5.8648\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/11\n",
            "Train >>>> Loss: 5.87148\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/12\n",
            "Train >>>> Loss: 5.90023\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/13\n",
            "Train >>>> Loss: 5.90157\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/14\n",
            "Train >>>> Loss: 5.85773\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/15\n",
            "Train >>>> Loss: 5.87092\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/16\n",
            "Train >>>> Loss: 5.86933\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/17\n",
            "Train >>>> Loss: 5.86813\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/18\n",
            "Train >>>> Loss: 5.89362\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/19\n",
            "Train >>>> Loss: 5.89078\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/20\n",
            "Train >>>> Loss: 5.91035\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/21\n",
            "Train >>>> Loss: 5.89709\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/22\n",
            "Train >>>> Loss: 5.84065\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/23\n",
            "Train >>>> Loss: 5.85602\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/24\n",
            "Train >>>> Loss: 5.92759\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/25\n",
            "Train >>>> Loss: 5.84837\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/26\n",
            "Train >>>> Loss: 5.85903\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/27\n",
            "Train >>>> Loss: 5.89388\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/28\n",
            "Train >>>> Loss: 5.86801\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/29\n",
            "Train >>>> Loss: 5.88836\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/30\n",
            "Train >>>> Loss: 5.90508\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/31\n",
            "Train >>>> Loss: 5.87444\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/32\n",
            "Train >>>> Loss: 5.85344\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/33\n",
            "Train >>>> Loss: 5.88814\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/34\n",
            "Train >>>> Loss: 5.92924\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 38/35\n",
            "Train >>>> Loss: 5.87434\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.88074\n",
            "[Average Testing Loss]: 5.85459\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/0\n",
            "Train >>>> Loss: 5.88298\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/1\n",
            "Train >>>> Loss: 5.88947\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/2\n",
            "Train >>>> Loss: 5.91079\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/3\n",
            "Train >>>> Loss: 5.90833\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/4\n",
            "Train >>>> Loss: 5.89704\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/5\n",
            "Train >>>> Loss: 5.88007\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/6\n",
            "Train >>>> Loss: 5.91091\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/7\n",
            "Train >>>> Loss: 5.89796\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/8\n",
            "Train >>>> Loss: 5.89583\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/9\n",
            "Train >>>> Loss: 5.89469\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/10\n",
            "Train >>>> Loss: 5.88627\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/11\n",
            "Train >>>> Loss: 5.87955\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/12\n",
            "Train >>>> Loss: 5.91996\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/13\n",
            "Train >>>> Loss: 5.90104\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/14\n",
            "Train >>>> Loss: 5.84454\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/15\n",
            "Train >>>> Loss: 5.88807\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/16\n",
            "Train >>>> Loss: 5.89894\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/17\n",
            "Train >>>> Loss: 5.89365\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/18\n",
            "Train >>>> Loss: 5.89051\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/19\n",
            "Train >>>> Loss: 5.88187\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/20\n",
            "Train >>>> Loss: 5.88359\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/21\n",
            "Train >>>> Loss: 5.83603\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/22\n",
            "Train >>>> Loss: 5.92492\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/23\n",
            "Train >>>> Loss: 5.87461\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/24\n",
            "Train >>>> Loss: 5.87952\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/25\n",
            "Train >>>> Loss: 5.89289\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/26\n",
            "Train >>>> Loss: 5.88093\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/27\n",
            "Train >>>> Loss: 5.86788\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/28\n",
            "Train >>>> Loss: 5.88376\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/29\n",
            "Train >>>> Loss: 5.91697\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/30\n",
            "Train >>>> Loss: 5.87767\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/31\n",
            "Train >>>> Loss: 5.90582\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/32\n",
            "Train >>>> Loss: 5.88909\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/33\n",
            "Train >>>> Loss: 5.88839\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/34\n",
            "Train >>>> Loss: 5.88667\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 39/35\n",
            "Train >>>> Loss: 5.84584\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.88853\n",
            "[Average Testing Loss]: 5.8482\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/0\n",
            "Train >>>> Loss: 5.91066\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/1\n",
            "Train >>>> Loss: 5.88473\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/2\n",
            "Train >>>> Loss: 5.90709\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/3\n",
            "Train >>>> Loss: 5.83979\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/4\n",
            "Train >>>> Loss: 5.82015\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/5\n",
            "Train >>>> Loss: 5.83721\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/6\n",
            "Train >>>> Loss: 5.85684\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/7\n",
            "Train >>>> Loss: 5.87278\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/8\n",
            "Train >>>> Loss: 5.89063\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/9\n",
            "Train >>>> Loss: 5.87501\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/10\n",
            "Train >>>> Loss: 5.90156\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/11\n",
            "Train >>>> Loss: 5.90345\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/12\n",
            "Train >>>> Loss: 5.86329\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/13\n",
            "Train >>>> Loss: 5.85745\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/14\n",
            "Train >>>> Loss: 5.84788\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/15\n",
            "Train >>>> Loss: 5.83495\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/16\n",
            "Train >>>> Loss: 5.84991\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/17\n",
            "Train >>>> Loss: 5.90195\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/18\n",
            "Train >>>> Loss: 5.88984\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/19\n",
            "Train >>>> Loss: 5.87326\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/20\n",
            "Train >>>> Loss: 5.84976\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/21\n",
            "Train >>>> Loss: 5.87099\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/22\n",
            "Train >>>> Loss: 5.92775\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/23\n",
            "Train >>>> Loss: 5.87472\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/24\n",
            "Train >>>> Loss: 5.90959\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/25\n",
            "Train >>>> Loss: 5.87123\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/26\n",
            "Train >>>> Loss: 5.89995\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/27\n",
            "Train >>>> Loss: 5.85996\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/28\n",
            "Train >>>> Loss: 5.86189\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/29\n",
            "Train >>>> Loss: 5.86834\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/30\n",
            "Train >>>> Loss: 5.87847\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/31\n",
            "Train >>>> Loss: 5.87545\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/32\n",
            "Train >>>> Loss: 5.90129\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/33\n",
            "Train >>>> Loss: 5.89208\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/34\n",
            "Train >>>> Loss: 5.88392\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 40/35\n",
            "Train >>>> Loss: 5.89958\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.8762\n",
            "[Average Testing Loss]: 5.85936\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/0\n",
            "Train >>>> Loss: 5.87026\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/1\n",
            "Train >>>> Loss: 5.85807\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/2\n",
            "Train >>>> Loss: 5.88439\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/3\n",
            "Train >>>> Loss: 5.90229\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/4\n",
            "Train >>>> Loss: 5.91312\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/5\n",
            "Train >>>> Loss: 5.86128\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/6\n",
            "Train >>>> Loss: 5.90754\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/7\n",
            "Train >>>> Loss: 5.91006\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/8\n",
            "Train >>>> Loss: 5.91144\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/9\n",
            "Train >>>> Loss: 5.88868\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/10\n",
            "Train >>>> Loss: 5.86037\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/11\n",
            "Train >>>> Loss: 5.87868\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/12\n",
            "Train >>>> Loss: 5.89203\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/13\n",
            "Train >>>> Loss:  5.923\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/14\n",
            "Train >>>> Loss: 5.85709\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/15\n",
            "Train >>>> Loss: 5.85219\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/16\n",
            "Train >>>> Loss: 5.92971\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/17\n",
            "Train >>>> Loss: 5.85633\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/18\n",
            "Train >>>> Loss: 5.86214\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/19\n",
            "Train >>>> Loss: 5.89354\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/20\n",
            "Train >>>> Loss: 5.88065\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/21\n",
            "Train >>>> Loss:  5.855\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/22\n",
            "Train >>>> Loss: 5.85863\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/23\n",
            "Train >>>> Loss: 5.89975\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/24\n",
            "Train >>>> Loss: 5.8719\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/25\n",
            "Train >>>> Loss: 5.89484\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/26\n",
            "Train >>>> Loss: 5.85865\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/27\n",
            "Train >>>> Loss: 5.88334\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/28\n",
            "Train >>>> Loss: 5.86903\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/29\n",
            "Train >>>> Loss: 5.89183\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/30\n",
            "Train >>>> Loss: 5.86218\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/31\n",
            "Train >>>> Loss: 5.86767\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/32\n",
            "Train >>>> Loss: 5.85637\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/33\n",
            "Train >>>> Loss: 5.84441\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/34\n",
            "Train >>>> Loss: 5.86318\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 41/35\n",
            "Train >>>> Loss: 5.85693\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.87852\n",
            "[Average Testing Loss]: 5.8336\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/0\n",
            "Train >>>> Loss: 5.87385\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/1\n",
            "Train >>>> Loss: 5.91144\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/2\n",
            "Train >>>> Loss: 5.88849\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/3\n",
            "Train >>>> Loss: 5.84908\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/4\n",
            "Train >>>> Loss: 5.85819\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/5\n",
            "Train >>>> Loss: 5.88409\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/6\n",
            "Train >>>> Loss: 5.89808\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/7\n",
            "Train >>>> Loss: 5.8665\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/8\n",
            "Train >>>> Loss: 5.85498\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/9\n",
            "Train >>>> Loss: 5.90518\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/10\n",
            "Train >>>> Loss: 5.84596\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/11\n",
            "Train >>>> Loss: 5.87008\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/12\n",
            "Train >>>> Loss: 5.9066\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/13\n",
            "Train >>>> Loss: 5.88093\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/14\n",
            "Train >>>> Loss: 5.86023\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/15\n",
            "Train >>>> Loss: 5.88662\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/16\n",
            "Train >>>> Loss: 5.8613\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/17\n",
            "Train >>>> Loss: 5.87657\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/18\n",
            "Train >>>> Loss: 5.86691\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/19\n",
            "Train >>>> Loss: 5.81882\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/20\n",
            "Train >>>> Loss: 5.90633\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/21\n",
            "Train >>>> Loss: 5.91994\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/22\n",
            "Train >>>> Loss: 5.88479\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/23\n",
            "Train >>>> Loss: 5.84476\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/24\n",
            "Train >>>> Loss: 5.87501\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/25\n",
            "Train >>>> Loss: 5.85791\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/26\n",
            "Train >>>> Loss: 5.85436\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/27\n",
            "Train >>>> Loss: 5.85359\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/28\n",
            "Train >>>> Loss: 5.91145\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/29\n",
            "Train >>>> Loss: 5.84063\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/30\n",
            "Train >>>> Loss: 5.86741\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/31\n",
            "Train >>>> Loss: 5.82209\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/32\n",
            "Train >>>> Loss: 5.85556\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/33\n",
            "Train >>>> Loss: 5.86667\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/34\n",
            "Train >>>> Loss: 5.88011\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 42/35\n",
            "Train >>>> Loss: 5.87834\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.87175\n",
            "[Average Testing Loss]: 5.86705\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/0\n",
            "Train >>>> Loss: 5.8463\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/1\n",
            "Train >>>> Loss: 5.86605\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/2\n",
            "Train >>>> Loss: 5.91886\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/3\n",
            "Train >>>> Loss: 5.85554\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/4\n",
            "Train >>>> Loss: 5.9153\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/5\n",
            "Train >>>> Loss: 5.8854\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/6\n",
            "Train >>>> Loss: 5.87441\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/7\n",
            "Train >>>> Loss:  5.847\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/8\n",
            "Train >>>> Loss: 5.87444\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/9\n",
            "Train >>>> Loss: 5.86347\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/10\n",
            "Train >>>> Loss: 5.89187\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/11\n",
            "Train >>>> Loss: 5.87974\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/12\n",
            "Train >>>> Loss: 5.86248\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/13\n",
            "Train >>>> Loss: 5.91365\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/14\n",
            "Train >>>> Loss: 5.91436\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/15\n",
            "Train >>>> Loss: 5.88097\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/16\n",
            "Train >>>> Loss: 5.87481\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/17\n",
            "Train >>>> Loss: 5.86164\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/18\n",
            "Train >>>> Loss: 5.91545\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/19\n",
            "Train >>>> Loss: 5.88919\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/20\n",
            "Train >>>> Loss: 5.87066\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/21\n",
            "Train >>>> Loss: 5.87534\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/22\n",
            "Train >>>> Loss: 5.84945\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/23\n",
            "Train >>>> Loss: 5.87404\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/24\n",
            "Train >>>> Loss: 5.86761\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/25\n",
            "Train >>>> Loss: 5.89664\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/26\n",
            "Train >>>> Loss: 5.91076\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/27\n",
            "Train >>>> Loss: 5.83583\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/28\n",
            "Train >>>> Loss: 5.91457\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/29\n",
            "Train >>>> Loss: 5.88215\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/30\n",
            "Train >>>> Loss: 5.8997\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/31\n",
            "Train >>>> Loss: 5.8924\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/32\n",
            "Train >>>> Loss: 5.8698\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/33\n",
            "Train >>>> Loss: 5.87806\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/34\n",
            "Train >>>> Loss: 5.87081\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 43/35\n",
            "Train >>>> Loss: 5.89056\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.88081\n",
            "[Average Testing Loss]: 5.86562\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/0\n",
            "Train >>>> Loss: 5.89835\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/1\n",
            "Train >>>> Loss: 5.87331\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/2\n",
            "Train >>>> Loss: 5.87374\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/3\n",
            "Train >>>> Loss: 5.88845\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/4\n",
            "Train >>>> Loss: 5.88746\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/5\n",
            "Train >>>> Loss: 5.87664\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/6\n",
            "Train >>>> Loss: 5.8828\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/7\n",
            "Train >>>> Loss: 5.86399\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/8\n",
            "Train >>>> Loss: 5.92067\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/9\n",
            "Train >>>> Loss: 5.87781\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/10\n",
            "Train >>>> Loss: 5.87329\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/11\n",
            "Train >>>> Loss: 5.87438\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/12\n",
            "Train >>>> Loss: 5.91691\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/13\n",
            "Train >>>> Loss: 5.88873\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/14\n",
            "Train >>>> Loss: 5.83028\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/15\n",
            "Train >>>> Loss: 5.85134\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/16\n",
            "Train >>>> Loss: 5.82695\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/17\n",
            "Train >>>> Loss: 5.9011\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/18\n",
            "Train >>>> Loss: 5.92218\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/19\n",
            "Train >>>> Loss: 5.88514\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/20\n",
            "Train >>>> Loss: 5.83933\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/21\n",
            "Train >>>> Loss: 5.91277\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/22\n",
            "Train >>>> Loss: 5.88101\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/23\n",
            "Train >>>> Loss: 5.88225\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/24\n",
            "Train >>>> Loss: 5.87319\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/25\n",
            "Train >>>> Loss:  5.849\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/26\n",
            "Train >>>> Loss: 5.85812\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/27\n",
            "Train >>>> Loss: 5.88503\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/28\n",
            "Train >>>> Loss: 5.95263\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/29\n",
            "Train >>>> Loss: 5.89379\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/30\n",
            "Train >>>> Loss: 5.90713\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/31\n",
            "Train >>>> Loss: 5.90184\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/32\n",
            "Train >>>> Loss: 5.86547\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/33\n",
            "Train >>>> Loss: 5.9006\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/34\n",
            "Train >>>> Loss: 5.88812\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 44/35\n",
            "Train >>>> Loss: 5.85784\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.88227\n",
            "[Average Testing Loss]: 5.83008\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/0\n",
            "Train >>>> Loss: 5.86067\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/1\n",
            "Train >>>> Loss: 5.91414\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/2\n",
            "Train >>>> Loss: 5.8664\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/3\n",
            "Train >>>> Loss: 5.82426\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/4\n",
            "Train >>>> Loss: 5.8669\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/5\n",
            "Train >>>> Loss: 5.86264\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/6\n",
            "Train >>>> Loss: 5.86596\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/7\n",
            "Train >>>> Loss: 5.87514\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/8\n",
            "Train >>>> Loss: 5.87422\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/9\n",
            "Train >>>> Loss: 5.86645\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/10\n",
            "Train >>>> Loss: 5.88761\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/11\n",
            "Train >>>> Loss: 5.89387\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/12\n",
            "Train >>>> Loss: 5.88053\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/13\n",
            "Train >>>> Loss: 5.85219\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/14\n",
            "Train >>>> Loss: 5.89061\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/15\n",
            "Train >>>> Loss: 5.85891\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/16\n",
            "Train >>>> Loss: 5.89112\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/17\n",
            "Train >>>> Loss: 5.88351\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/18\n",
            "Train >>>> Loss: 5.91698\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/19\n",
            "Train >>>> Loss: 5.91429\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/20\n",
            "Train >>>> Loss: 5.83607\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/21\n",
            "Train >>>> Loss: 5.92175\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/22\n",
            "Train >>>> Loss: 5.89996\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/23\n",
            "Train >>>> Loss: 5.88097\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/24\n",
            "Train >>>> Loss: 5.86532\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/25\n",
            "Train >>>> Loss: 5.86018\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/26\n",
            "Train >>>> Loss: 5.84821\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/27\n",
            "Train >>>> Loss: 5.88291\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/28\n",
            "Train >>>> Loss: 5.88072\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/29\n",
            "Train >>>> Loss: 5.88028\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/30\n",
            "Train >>>> Loss: 5.84107\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/31\n",
            "Train >>>> Loss: 5.87593\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/32\n",
            "Train >>>> Loss: 5.84964\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/33\n",
            "Train >>>> Loss: 5.88145\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/34\n",
            "Train >>>> Loss: 5.88194\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 45/35\n",
            "Train >>>> Loss: 5.90703\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.87611\n",
            "[Average Testing Loss]: 5.86903\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/0\n",
            "Train >>>> Loss: 5.8671\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/1\n",
            "Train >>>> Loss: 5.88883\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/2\n",
            "Train >>>> Loss: 5.86804\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/3\n",
            "Train >>>> Loss: 5.88872\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/4\n",
            "Train >>>> Loss: 5.90778\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/5\n",
            "Train >>>> Loss: 5.9104\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/6\n",
            "Train >>>> Loss: 5.86686\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/7\n",
            "Train >>>> Loss: 5.86242\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/8\n",
            "Train >>>> Loss: 5.86383\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/9\n",
            "Train >>>> Loss: 5.86381\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/10\n",
            "Train >>>> Loss: 5.90063\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/11\n",
            "Train >>>> Loss: 5.83871\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/12\n",
            "Train >>>> Loss: 5.88861\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/13\n",
            "Train >>>> Loss: 5.85158\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/14\n",
            "Train >>>> Loss: 5.88747\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/15\n",
            "Train >>>> Loss: 5.86925\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/16\n",
            "Train >>>> Loss: 5.88558\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/17\n",
            "Train >>>> Loss: 5.87866\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/18\n",
            "Train >>>> Loss: 5.85534\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/19\n",
            "Train >>>> Loss: 5.88997\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/20\n",
            "Train >>>> Loss: 5.89324\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/21\n",
            "Train >>>> Loss: 5.85649\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/22\n",
            "Train >>>> Loss: 5.90699\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/23\n",
            "Train >>>> Loss: 5.86149\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/24\n",
            "Train >>>> Loss: 5.86474\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/25\n",
            "Train >>>> Loss: 5.89792\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/26\n",
            "Train >>>> Loss: 5.88363\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/27\n",
            "Train >>>> Loss: 5.85533\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/28\n",
            "Train >>>> Loss: 5.86192\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/29\n",
            "Train >>>> Loss: 5.85236\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/30\n",
            "Train >>>> Loss: 5.89434\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/31\n",
            "Train >>>> Loss: 5.87579\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/32\n",
            "Train >>>> Loss: 5.88298\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/33\n",
            "Train >>>> Loss: 5.86861\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/34\n",
            "Train >>>> Loss: 5.90012\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 46/35\n",
            "Train >>>> Loss: 5.89628\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.87738\n",
            "[Average Testing Loss]: 5.85731\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/0\n",
            "Train >>>> Loss: 5.88905\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/1\n",
            "Train >>>> Loss: 5.89768\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/2\n",
            "Train >>>> Loss: 5.90403\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/3\n",
            "Train >>>> Loss: 5.89788\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/4\n",
            "Train >>>> Loss: 5.86099\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/5\n",
            "Train >>>> Loss: 5.84935\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/6\n",
            "Train >>>> Loss: 5.86565\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/7\n",
            "Train >>>> Loss: 5.84751\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/8\n",
            "Train >>>> Loss: 5.87383\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/9\n",
            "Train >>>> Loss: 5.87938\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/10\n",
            "Train >>>> Loss: 5.91385\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/11\n",
            "Train >>>> Loss: 5.89975\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/12\n",
            "Train >>>> Loss: 5.8522\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/13\n",
            "Train >>>> Loss: 5.89368\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/14\n",
            "Train >>>> Loss: 5.88873\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/15\n",
            "Train >>>> Loss: 5.87923\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/16\n",
            "Train >>>> Loss: 5.90295\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/17\n",
            "Train >>>> Loss: 5.89027\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/18\n",
            "Train >>>> Loss: 5.85973\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/19\n",
            "Train >>>> Loss: 5.87312\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/20\n",
            "Train >>>> Loss: 5.88461\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/21\n",
            "Train >>>> Loss: 5.85514\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/22\n",
            "Train >>>> Loss: 5.89037\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/23\n",
            "Train >>>> Loss: 5.92877\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/24\n",
            "Train >>>> Loss: 5.86635\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/25\n",
            "Train >>>> Loss: 5.89156\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/26\n",
            "Train >>>> Loss: 5.88186\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/27\n",
            "Train >>>> Loss: 5.89657\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/28\n",
            "Train >>>> Loss: 5.88177\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/29\n",
            "Train >>>> Loss: 5.87212\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/30\n",
            "Train >>>> Loss: 5.87361\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/31\n",
            "Train >>>> Loss: 5.92832\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/32\n",
            "Train >>>> Loss: 5.89706\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/33\n",
            "Train >>>> Loss: 5.82688\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/34\n",
            "Train >>>> Loss: 5.86777\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 47/35\n",
            "Train >>>> Loss: 5.87764\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.88165\n",
            "[Average Testing Loss]: 5.8726\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/0\n",
            "Train >>>> Loss: 5.8905\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/1\n",
            "Train >>>> Loss: 5.87313\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/2\n",
            "Train >>>> Loss: 5.83623\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/3\n",
            "Train >>>> Loss: 5.8378\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/4\n",
            "Train >>>> Loss: 5.87962\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/5\n",
            "Train >>>> Loss: 5.87421\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/6\n",
            "Train >>>> Loss: 5.89191\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/7\n",
            "Train >>>> Loss: 5.83692\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/8\n",
            "Train >>>> Loss: 5.85386\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/9\n",
            "Train >>>> Loss: 5.90049\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/10\n",
            "Train >>>> Loss: 5.88101\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/11\n",
            "Train >>>> Loss: 5.90447\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/12\n",
            "Train >>>> Loss: 5.89896\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/13\n",
            "Train >>>> Loss: 5.82925\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/14\n",
            "Train >>>> Loss: 5.87453\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/15\n",
            "Train >>>> Loss: 5.87655\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/16\n",
            "Train >>>> Loss: 5.87441\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/17\n",
            "Train >>>> Loss: 5.91089\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/18\n",
            "Train >>>> Loss: 5.86485\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/19\n",
            "Train >>>> Loss: 5.85567\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/20\n",
            "Train >>>> Loss: 5.85775\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/21\n",
            "Train >>>> Loss: 5.92294\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/22\n",
            "Train >>>> Loss: 5.85726\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/23\n",
            "Train >>>> Loss: 5.91662\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/24\n",
            "Train >>>> Loss: 5.87789\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/25\n",
            "Train >>>> Loss: 5.87403\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/26\n",
            "Train >>>> Loss: 5.89042\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/27\n",
            "Train >>>> Loss: 5.85528\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/28\n",
            "Train >>>> Loss: 5.84875\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/29\n",
            "Train >>>> Loss: 5.89356\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/30\n",
            "Train >>>> Loss: 5.86868\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/31\n",
            "Train >>>> Loss: 5.88499\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/32\n",
            "Train >>>> Loss: 5.84625\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/33\n",
            "Train >>>> Loss: 5.88121\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/34\n",
            "Train >>>> Loss: 5.84961\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 48/35\n",
            "Train >>>> Loss: 5.91082\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.87448\n",
            "[Average Testing Loss]: 5.84825\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/0\n",
            "Train >>>> Loss: 5.88131\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/1\n",
            "Train >>>> Loss: 5.88667\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/2\n",
            "Train >>>> Loss: 5.8736\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/3\n",
            "Train >>>> Loss: 5.87253\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/4\n",
            "Train >>>> Loss: 5.88968\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/5\n",
            "Train >>>> Loss: 5.83175\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/6\n",
            "Train >>>> Loss:  5.883\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/7\n",
            "Train >>>> Loss: 5.89445\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/8\n",
            "Train >>>> Loss: 5.84338\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/9\n",
            "Train >>>> Loss: 5.89236\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/10\n",
            "Train >>>> Loss: 5.90202\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/11\n",
            "Train >>>> Loss: 5.86222\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/12\n",
            "Train >>>> Loss: 5.87745\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/13\n",
            "Train >>>> Loss: 5.89547\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/14\n",
            "Train >>>> Loss: 5.91126\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/15\n",
            "Train >>>> Loss: 5.86607\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/16\n",
            "Train >>>> Loss: 5.84503\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/17\n",
            "Train >>>> Loss: 5.84849\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/18\n",
            "Train >>>> Loss: 5.83597\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/19\n",
            "Train >>>> Loss: 5.89009\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/20\n",
            "Train >>>> Loss: 5.88878\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/21\n",
            "Train >>>> Loss: 5.87175\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/22\n",
            "Train >>>> Loss: 5.90586\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/23\n",
            "Train >>>> Loss: 5.88868\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/24\n",
            "Train >>>> Loss: 5.86357\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/25\n",
            "Train >>>> Loss: 5.90767\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/26\n",
            "Train >>>> Loss: 5.87951\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/27\n",
            "Train >>>> Loss: 5.88522\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/28\n",
            "Train >>>> Loss: 5.88085\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/29\n",
            "Train >>>> Loss: 5.85905\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/30\n",
            "Train >>>> Loss: 5.88326\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/31\n",
            "Train >>>> Loss: 5.88618\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/32\n",
            "Train >>>> Loss: 5.87659\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/33\n",
            "Train >>>> Loss: 5.9237\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/34\n",
            "Train >>>> Loss: 5.8982\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 49/35\n",
            "Train >>>> Loss: 5.87236\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.87928\n",
            "[Average Testing Loss]: 5.8943\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/0\n",
            "Train >>>> Loss: 5.87993\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/1\n",
            "Train >>>> Loss: 5.85039\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/2\n",
            "Train >>>> Loss: 5.85548\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/3\n",
            "Train >>>> Loss: 5.86777\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/4\n",
            "Train >>>> Loss: 5.85384\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/5\n",
            "Train >>>> Loss: 5.8765\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/6\n",
            "Train >>>> Loss: 5.87539\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/7\n",
            "Train >>>> Loss: 5.87522\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/8\n",
            "Train >>>> Loss: 5.84681\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/9\n",
            "Train >>>> Loss: 5.88876\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/10\n",
            "Train >>>> Loss: 5.8976\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/11\n",
            "Train >>>> Loss: 5.8882\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/12\n",
            "Train >>>> Loss: 5.86781\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/13\n",
            "Train >>>> Loss: 5.87165\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/14\n",
            "Train >>>> Loss: 5.85081\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/15\n",
            "Train >>>> Loss: 5.87898\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/16\n",
            "Train >>>> Loss: 5.8896\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/17\n",
            "Train >>>> Loss:  5.859\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/18\n",
            "Train >>>> Loss: 5.92374\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/19\n",
            "Train >>>> Loss: 5.87586\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/20\n",
            "Train >>>> Loss: 5.8604\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/21\n",
            "Train >>>> Loss: 5.88177\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/22\n",
            "Train >>>> Loss: 5.90623\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/23\n",
            "Train >>>> Loss: 5.89549\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/24\n",
            "Train >>>> Loss: 5.8771\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/25\n",
            "Train >>>> Loss: 5.83707\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/26\n",
            "Train >>>> Loss: 5.87726\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/27\n",
            "Train >>>> Loss: 5.87259\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/28\n",
            "Train >>>> Loss: 5.91322\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/29\n",
            "Train >>>> Loss: 5.91155\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/30\n",
            "Train >>>> Loss: 5.87098\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/31\n",
            "Train >>>> Loss: 5.89282\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/32\n",
            "Train >>>> Loss: 5.87768\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/33\n",
            "Train >>>> Loss: 5.92549\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/34\n",
            "Train >>>> Loss: 5.85474\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 50/35\n",
            "Train >>>> Loss: 5.87824\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.87794\n",
            "[Average Testing Loss]: 5.86452\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/0\n",
            "Train >>>> Loss: 5.85656\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/1\n",
            "Train >>>> Loss: 5.87492\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/2\n",
            "Train >>>> Loss: 5.89335\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/3\n",
            "Train >>>> Loss: 5.89207\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/4\n",
            "Train >>>> Loss: 5.85224\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/5\n",
            "Train >>>> Loss: 5.87826\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/6\n",
            "Train >>>> Loss: 5.88809\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/7\n",
            "Train >>>> Loss: 5.86567\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/8\n",
            "Train >>>> Loss: 5.87369\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/9\n",
            "Train >>>> Loss: 5.87322\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/10\n",
            "Train >>>> Loss: 5.86736\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/11\n",
            "Train >>>> Loss: 5.88995\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/12\n",
            "Train >>>> Loss: 5.87956\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/13\n",
            "Train >>>> Loss: 5.87197\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/14\n",
            "Train >>>> Loss: 5.85578\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/15\n",
            "Train >>>> Loss: 5.85403\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/16\n",
            "Train >>>> Loss: 5.92075\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/17\n",
            "Train >>>> Loss: 5.87962\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/18\n",
            "Train >>>> Loss: 5.8326\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/19\n",
            "Train >>>> Loss: 5.87769\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/20\n",
            "Train >>>> Loss: 5.85024\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/21\n",
            "Train >>>> Loss: 5.83831\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/22\n",
            "Train >>>> Loss: 5.91716\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/23\n",
            "Train >>>> Loss: 5.8878\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/24\n",
            "Train >>>> Loss: 5.87019\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/25\n",
            "Train >>>> Loss: 5.86952\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/26\n",
            "Train >>>> Loss: 5.90772\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/27\n",
            "Train >>>> Loss: 5.85965\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/28\n",
            "Train >>>> Loss: 5.84628\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/29\n",
            "Train >>>> Loss: 5.88759\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/30\n",
            "Train >>>> Loss: 5.86622\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/31\n",
            "Train >>>> Loss: 5.9109\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/32\n",
            "Train >>>> Loss: 5.90613\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/33\n",
            "Train >>>> Loss: 5.86266\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/34\n",
            "Train >>>> Loss: 5.87776\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 51/35\n",
            "Train >>>> Loss: 5.93174\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.87687\n",
            "[Average Testing Loss]:  5.863\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/0\n",
            "Train >>>> Loss: 5.84984\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/1\n",
            "Train >>>> Loss: 5.86958\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/2\n",
            "Train >>>> Loss: 5.87687\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/3\n",
            "Train >>>> Loss: 5.87918\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/4\n",
            "Train >>>> Loss: 5.85023\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/5\n",
            "Train >>>> Loss:  5.856\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/6\n",
            "Train >>>> Loss: 5.8256\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/7\n",
            "Train >>>> Loss: 5.90869\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/8\n",
            "Train >>>> Loss: 5.87747\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/9\n",
            "Train >>>> Loss: 5.86706\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/10\n",
            "Train >>>> Loss: 5.85606\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/11\n",
            "Train >>>> Loss: 5.84432\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/12\n",
            "Train >>>> Loss: 5.86225\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/13\n",
            "Train >>>> Loss: 5.88535\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/14\n",
            "Train >>>> Loss: 5.90741\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/15\n",
            "Train >>>> Loss: 5.89024\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/16\n",
            "Train >>>> Loss: 5.88187\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/17\n",
            "Train >>>> Loss: 5.86821\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/18\n",
            "Train >>>> Loss: 5.92356\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/19\n",
            "Train >>>> Loss: 5.84243\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/20\n",
            "Train >>>> Loss: 5.87994\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/21\n",
            "Train >>>> Loss: 5.86393\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/22\n",
            "Train >>>> Loss:  5.879\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/23\n",
            "Train >>>> Loss: 5.86146\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/24\n",
            "Train >>>> Loss: 5.85661\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/25\n",
            "Train >>>> Loss: 5.8793\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/26\n",
            "Train >>>> Loss: 5.8194\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/27\n",
            "Train >>>> Loss: 5.83614\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/28\n",
            "Train >>>> Loss: 5.88854\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/29\n",
            "Train >>>> Loss: 5.88082\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/30\n",
            "Train >>>> Loss: 5.88427\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/31\n",
            "Train >>>> Loss: 5.87642\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/32\n",
            "Train >>>> Loss: 5.8846\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/33\n",
            "Train >>>> Loss: 5.8645\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/34\n",
            "Train >>>> Loss: 5.85991\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 52/35\n",
            "Train >>>> Loss: 5.83716\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.86873\n",
            "[Average Testing Loss]: 5.8431\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/0\n",
            "Train >>>> Loss: 5.84564\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/1\n",
            "Train >>>> Loss: 5.87365\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/2\n",
            "Train >>>> Loss: 5.88462\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/3\n",
            "Train >>>> Loss: 5.85891\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/4\n",
            "Train >>>> Loss: 5.88481\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/5\n",
            "Train >>>> Loss: 5.88911\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/6\n",
            "Train >>>> Loss: 5.87046\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/7\n",
            "Train >>>> Loss: 5.85686\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/8\n",
            "Train >>>> Loss: 5.87577\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/9\n",
            "Train >>>> Loss: 5.89839\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/10\n",
            "Train >>>> Loss: 5.89102\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/11\n",
            "Train >>>> Loss: 5.86284\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/12\n",
            "Train >>>> Loss: 5.8574\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/13\n",
            "Train >>>> Loss: 5.87794\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/14\n",
            "Train >>>> Loss: 5.86143\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/15\n",
            "Train >>>> Loss: 5.8811\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/16\n",
            "Train >>>> Loss: 5.84058\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/17\n",
            "Train >>>> Loss: 5.87989\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/18\n",
            "Train >>>> Loss: 5.87464\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/19\n",
            "Train >>>> Loss: 5.83524\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/20\n",
            "Train >>>> Loss: 5.88386\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/21\n",
            "Train >>>> Loss: 5.91346\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/22\n",
            "Train >>>> Loss: 5.86157\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/23\n",
            "Train >>>> Loss: 5.83976\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/24\n",
            "Train >>>> Loss: 5.84471\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/25\n",
            "Train >>>> Loss: 5.87135\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/26\n",
            "Train >>>> Loss: 5.82822\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/27\n",
            "Train >>>> Loss: 5.93051\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/28\n",
            "Train >>>> Loss: 5.9399\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/29\n",
            "Train >>>> Loss: 5.86094\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/30\n",
            "Train >>>> Loss: 5.89052\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/31\n",
            "Train >>>> Loss: 5.8106\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/32\n",
            "Train >>>> Loss:  5.844\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/33\n",
            "Train >>>> Loss: 5.90144\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/34\n",
            "Train >>>> Loss: 5.89822\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 53/35\n",
            "Train >>>> Loss: 5.86739\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.87185\n",
            "[Average Testing Loss]: 5.83948\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/0\n",
            "Train >>>> Loss: 5.85424\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/1\n",
            "Train >>>> Loss: 5.86355\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/2\n",
            "Train >>>> Loss: 5.85314\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/3\n",
            "Train >>>> Loss: 5.83993\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/4\n",
            "Train >>>> Loss: 5.86514\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/5\n",
            "Train >>>> Loss: 5.89068\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/6\n",
            "Train >>>> Loss: 5.88142\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/7\n",
            "Train >>>> Loss: 5.84006\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/8\n",
            "Train >>>> Loss: 5.91446\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/9\n",
            "Train >>>> Loss: 5.85913\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/10\n",
            "Train >>>> Loss: 5.90222\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/11\n",
            "Train >>>> Loss: 5.84787\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/12\n",
            "Train >>>> Loss: 5.86747\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/13\n",
            "Train >>>> Loss: 5.86403\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/14\n",
            "Train >>>> Loss: 5.86405\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/15\n",
            "Train >>>> Loss: 5.86981\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/16\n",
            "Train >>>> Loss: 5.86247\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/17\n",
            "Train >>>> Loss: 5.86024\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/18\n",
            "Train >>>> Loss: 5.89839\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/19\n",
            "Train >>>> Loss: 5.87752\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/20\n",
            "Train >>>> Loss: 5.85981\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/21\n",
            "Train >>>> Loss: 5.84358\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/22\n",
            "Train >>>> Loss: 5.82815\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/23\n",
            "Train >>>> Loss: 5.91893\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/24\n",
            "Train >>>> Loss: 5.91363\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/25\n",
            "Train >>>> Loss: 5.87203\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/26\n",
            "Train >>>> Loss: 5.82684\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/27\n",
            "Train >>>> Loss: 5.89694\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/28\n",
            "Train >>>> Loss: 5.88079\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/29\n",
            "Train >>>> Loss: 5.88718\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/30\n",
            "Train >>>> Loss: 5.83567\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/31\n",
            "Train >>>> Loss: 5.88124\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/32\n",
            "Train >>>> Loss: 5.86285\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/33\n",
            "Train >>>> Loss: 5.87182\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/34\n",
            "Train >>>> Loss: 5.8735\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 54/35\n",
            "Train >>>> Loss: 5.8288\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.86827\n",
            "[Average Testing Loss]: 5.85832\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/0\n",
            "Train >>>> Loss: 5.87321\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/1\n",
            "Train >>>> Loss: 5.87675\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/2\n",
            "Train >>>> Loss: 5.89551\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/3\n",
            "Train >>>> Loss: 5.85147\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/4\n",
            "Train >>>> Loss: 5.88295\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/5\n",
            "Train >>>> Loss: 5.87127\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/6\n",
            "Train >>>> Loss: 5.87365\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/7\n",
            "Train >>>> Loss: 5.86568\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/8\n",
            "Train >>>> Loss: 5.87391\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/9\n",
            "Train >>>> Loss: 5.8971\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/10\n",
            "Train >>>> Loss: 5.88669\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/11\n",
            "Train >>>> Loss: 5.8556\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/12\n",
            "Train >>>> Loss: 5.90928\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/13\n",
            "Train >>>> Loss: 5.85821\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/14\n",
            "Train >>>> Loss: 5.87785\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/15\n",
            "Train >>>> Loss: 5.86054\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/16\n",
            "Train >>>> Loss: 5.85526\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/17\n",
            "Train >>>> Loss: 5.86125\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/18\n",
            "Train >>>> Loss: 5.86985\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/19\n",
            "Train >>>> Loss: 5.84757\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/20\n",
            "Train >>>> Loss: 5.86452\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/21\n",
            "Train >>>> Loss: 5.82238\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/22\n",
            "Train >>>> Loss: 5.91508\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/23\n",
            "Train >>>> Loss: 5.85664\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/24\n",
            "Train >>>> Loss: 5.90069\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/25\n",
            "Train >>>> Loss: 5.8959\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/26\n",
            "Train >>>> Loss: 5.88954\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/27\n",
            "Train >>>> Loss: 5.8869\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/28\n",
            "Train >>>> Loss: 5.89595\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/29\n",
            "Train >>>> Loss: 5.89657\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/30\n",
            "Train >>>> Loss: 5.86966\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/31\n",
            "Train >>>> Loss: 5.92539\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/32\n",
            "Train >>>> Loss: 5.87739\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/33\n",
            "Train >>>> Loss: 5.87828\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/34\n",
            "Train >>>> Loss: 5.86737\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 55/35\n",
            "Train >>>> Loss: 5.86038\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.87628\n",
            "[Average Testing Loss]: 5.86015\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/0\n",
            "Train >>>> Loss: 5.87477\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/1\n",
            "Train >>>> Loss: 5.86173\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/2\n",
            "Train >>>> Loss: 5.80951\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/3\n",
            "Train >>>> Loss: 5.85318\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/4\n",
            "Train >>>> Loss: 5.85889\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/5\n",
            "Train >>>> Loss: 5.90419\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/6\n",
            "Train >>>> Loss: 5.89257\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/7\n",
            "Train >>>> Loss: 5.83393\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/8\n",
            "Train >>>> Loss: 5.85618\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/9\n",
            "Train >>>> Loss: 5.89386\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/10\n",
            "Train >>>> Loss: 5.87848\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/11\n",
            "Train >>>> Loss: 5.89847\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/12\n",
            "Train >>>> Loss: 5.87735\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/13\n",
            "Train >>>> Loss: 5.88896\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/14\n",
            "Train >>>> Loss: 5.88341\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/15\n",
            "Train >>>> Loss: 5.84881\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/16\n",
            "Train >>>> Loss: 5.89221\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/17\n",
            "Train >>>> Loss: 5.84171\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/18\n",
            "Train >>>> Loss: 5.86587\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/19\n",
            "Train >>>> Loss: 5.90334\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/20\n",
            "Train >>>> Loss: 5.85755\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/21\n",
            "Train >>>> Loss: 5.87184\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/22\n",
            "Train >>>> Loss: 5.86949\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/23\n",
            "Train >>>> Loss: 5.87757\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/24\n",
            "Train >>>> Loss: 5.89616\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/25\n",
            "Train >>>> Loss: 5.85304\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/26\n",
            "Train >>>> Loss: 5.84037\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/27\n",
            "Train >>>> Loss: 5.85129\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/28\n",
            "Train >>>> Loss: 5.84187\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/29\n",
            "Train >>>> Loss: 5.83884\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/30\n",
            "Train >>>> Loss: 5.87756\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/31\n",
            "Train >>>> Loss: 5.84434\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/32\n",
            "Train >>>> Loss: 5.82495\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/33\n",
            "Train >>>> Loss: 5.87902\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/34\n",
            "Train >>>> Loss: 5.91166\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 56/35\n",
            "Train >>>> Loss: 5.86145\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.86707\n",
            "[Average Testing Loss]: 5.86327\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/0\n",
            "Train >>>> Loss: 5.91279\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/1\n",
            "Train >>>> Loss: 5.91328\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/2\n",
            "Train >>>> Loss: 5.90273\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/3\n",
            "Train >>>> Loss: 5.86267\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/4\n",
            "Train >>>> Loss: 5.88005\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/5\n",
            "Train >>>> Loss: 5.83226\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/6\n",
            "Train >>>> Loss: 5.88591\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/7\n",
            "Train >>>> Loss: 5.82669\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/8\n",
            "Train >>>> Loss: 5.85056\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/9\n",
            "Train >>>> Loss: 5.86674\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/10\n",
            "Train >>>> Loss: 5.90071\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/11\n",
            "Train >>>> Loss: 5.85488\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/12\n",
            "Train >>>> Loss: 5.85292\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/13\n",
            "Train >>>> Loss: 5.85141\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/14\n",
            "Train >>>> Loss: 5.88362\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/15\n",
            "Train >>>> Loss: 5.89779\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/16\n",
            "Train >>>> Loss: 5.83424\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/17\n",
            "Train >>>> Loss: 5.84642\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/18\n",
            "Train >>>> Loss: 5.91966\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/19\n",
            "Train >>>> Loss: 5.86271\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/20\n",
            "Train >>>> Loss: 5.88399\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/21\n",
            "Train >>>> Loss: 5.88263\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/22\n",
            "Train >>>> Loss: 5.86881\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/23\n",
            "Train >>>> Loss: 5.90813\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/24\n",
            "Train >>>> Loss: 5.88753\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/25\n",
            "Train >>>> Loss: 5.88653\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/26\n",
            "Train >>>> Loss: 5.88167\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/27\n",
            "Train >>>> Loss: 5.8456\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/28\n",
            "Train >>>> Loss: 5.89588\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/29\n",
            "Train >>>> Loss: 5.9025\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/30\n",
            "Train >>>> Loss: 5.86408\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/31\n",
            "Train >>>> Loss: 5.86928\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/32\n",
            "Train >>>> Loss: 5.85178\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/33\n",
            "Train >>>> Loss: 5.87546\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/34\n",
            "Train >>>> Loss: 5.85683\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 57/35\n",
            "Train >>>> Loss: 5.88122\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.87444\n",
            "[Average Testing Loss]: 5.87426\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/0\n",
            "Train >>>> Loss: 5.88638\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/1\n",
            "Train >>>> Loss: 5.89915\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/2\n",
            "Train >>>> Loss: 5.87625\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/3\n",
            "Train >>>> Loss: 5.85782\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/4\n",
            "Train >>>> Loss: 5.84584\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/5\n",
            "Train >>>> Loss: 5.87274\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/6\n",
            "Train >>>> Loss: 5.87073\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/7\n",
            "Train >>>> Loss: 5.87906\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/8\n",
            "Train >>>> Loss: 5.88258\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/9\n",
            "Train >>>> Loss: 5.90181\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/10\n",
            "Train >>>> Loss: 5.91534\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/11\n",
            "Train >>>> Loss: 5.85367\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/12\n",
            "Train >>>> Loss: 5.85294\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/13\n",
            "Train >>>> Loss: 5.83877\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/14\n",
            "Train >>>> Loss: 5.87564\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/15\n",
            "Train >>>> Loss: 5.81018\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/16\n",
            "Train >>>> Loss: 5.87286\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/17\n",
            "Train >>>> Loss: 5.92442\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/18\n",
            "Train >>>> Loss: 5.86287\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/19\n",
            "Train >>>> Loss: 5.86912\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/20\n",
            "Train >>>> Loss: 5.85743\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/21\n",
            "Train >>>> Loss: 5.86488\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/22\n",
            "Train >>>> Loss: 5.88362\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/23\n",
            "Train >>>> Loss: 5.85918\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/24\n",
            "Train >>>> Loss: 5.87553\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/25\n",
            "Train >>>> Loss: 5.87035\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/26\n",
            "Train >>>> Loss: 5.87835\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/27\n",
            "Train >>>> Loss: 5.89786\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/28\n",
            "Train >>>> Loss: 5.87574\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/29\n",
            "Train >>>> Loss: 5.84381\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/30\n",
            "Train >>>> Loss: 5.86898\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/31\n",
            "Train >>>> Loss: 5.85152\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/32\n",
            "Train >>>> Loss: 5.87577\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/33\n",
            "Train >>>> Loss: 5.87538\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/34\n",
            "Train >>>> Loss: 5.88119\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 58/35\n",
            "Train >>>> Loss: 5.87321\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.87169\n",
            "[Average Testing Loss]: 5.84668\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/0\n",
            "Train >>>> Loss: 5.89124\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/1\n",
            "Train >>>> Loss: 5.87671\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/2\n",
            "Train >>>> Loss: 5.89002\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/3\n",
            "Train >>>> Loss: 5.86964\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/4\n",
            "Train >>>> Loss: 5.89392\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/5\n",
            "Train >>>> Loss: 5.84632\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/6\n",
            "Train >>>> Loss: 5.87784\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/7\n",
            "Train >>>> Loss: 5.90164\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/8\n",
            "Train >>>> Loss: 5.86671\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/9\n",
            "Train >>>> Loss: 5.87143\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/10\n",
            "Train >>>> Loss: 5.86697\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/11\n",
            "Train >>>> Loss: 5.85568\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/12\n",
            "Train >>>> Loss: 5.88398\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/13\n",
            "Train >>>> Loss: 5.87587\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/14\n",
            "Train >>>> Loss: 5.86562\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/15\n",
            "Train >>>> Loss: 5.88842\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/16\n",
            "Train >>>> Loss: 5.8887\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/17\n",
            "Train >>>> Loss: 5.8647\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/18\n",
            "Train >>>> Loss: 5.88414\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/19\n",
            "Train >>>> Loss: 5.88173\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/20\n",
            "Train >>>> Loss: 5.89978\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/21\n",
            "Train >>>> Loss: 5.85345\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/22\n",
            "Train >>>> Loss: 5.84517\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/23\n",
            "Train >>>> Loss: 5.86069\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/24\n",
            "Train >>>> Loss: 5.90347\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/25\n",
            "Train >>>> Loss: 5.90872\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/26\n",
            "Train >>>> Loss: 5.84618\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/27\n",
            "Train >>>> Loss: 5.87758\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/28\n",
            "Train >>>> Loss: 5.87068\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/29\n",
            "Train >>>> Loss: 5.86667\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/30\n",
            "Train >>>> Loss: 5.86604\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/31\n",
            "Train >>>> Loss: 5.88176\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/32\n",
            "Train >>>> Loss: 5.90938\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/33\n",
            "Train >>>> Loss: 5.86072\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/34\n",
            "Train >>>> Loss: 5.88926\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 59/35\n",
            "Train >>>> Loss: 5.85778\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.87607\n",
            "[Average Testing Loss]: 5.86141\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/0\n",
            "Train >>>> Loss: 5.88012\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/1\n",
            "Train >>>> Loss: 5.8482\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/2\n",
            "Train >>>> Loss: 5.84019\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/3\n",
            "Train >>>> Loss: 5.84253\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/4\n",
            "Train >>>> Loss: 5.85992\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/5\n",
            "Train >>>> Loss: 5.91719\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/6\n",
            "Train >>>> Loss: 5.90856\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/7\n",
            "Train >>>> Loss: 5.86242\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/8\n",
            "Train >>>> Loss: 5.88585\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/9\n",
            "Train >>>> Loss: 5.84497\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/10\n",
            "Train >>>> Loss: 5.88316\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/11\n",
            "Train >>>> Loss: 5.85767\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/12\n",
            "Train >>>> Loss: 5.82678\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/13\n",
            "Train >>>> Loss: 5.84861\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/14\n",
            "Train >>>> Loss: 5.91577\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/15\n",
            "Train >>>> Loss: 5.83914\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/16\n",
            "Train >>>> Loss: 5.92669\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/17\n",
            "Train >>>> Loss: 5.9048\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/18\n",
            "Train >>>> Loss: 5.88438\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/19\n",
            "Train >>>> Loss: 5.8922\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/20\n",
            "Train >>>> Loss: 5.86732\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/21\n",
            "Train >>>> Loss: 5.87057\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/22\n",
            "Train >>>> Loss: 5.87574\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/23\n",
            "Train >>>> Loss: 5.84413\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/24\n",
            "Train >>>> Loss: 5.88565\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/25\n",
            "Train >>>> Loss: 5.85626\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/26\n",
            "Train >>>> Loss: 5.86515\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/27\n",
            "Train >>>> Loss: 5.87681\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/28\n",
            "Train >>>> Loss: 5.85679\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/29\n",
            "Train >>>> Loss: 5.8426\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/30\n",
            "Train >>>> Loss: 5.86818\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/31\n",
            "Train >>>> Loss: 5.91123\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/32\n",
            "Train >>>> Loss: 5.88608\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/33\n",
            "Train >>>> Loss: 5.8633\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/34\n",
            "Train >>>> Loss: 5.85628\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 60/35\n",
            "Train >>>> Loss: 5.85741\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.87091\n",
            "[Average Testing Loss]: 5.83193\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/0\n",
            "Train >>>> Loss: 5.90328\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/1\n",
            "Train >>>> Loss:  5.854\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/2\n",
            "Train >>>> Loss: 5.83964\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/3\n",
            "Train >>>> Loss: 5.86341\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/4\n",
            "Train >>>> Loss: 5.84234\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/5\n",
            "Train >>>> Loss: 5.88547\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/6\n",
            "Train >>>> Loss: 5.84957\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/7\n",
            "Train >>>> Loss: 5.92004\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/8\n",
            "Train >>>> Loss: 5.89469\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/9\n",
            "Train >>>> Loss: 5.85435\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/10\n",
            "Train >>>> Loss: 5.87889\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/11\n",
            "Train >>>> Loss: 5.86939\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/12\n",
            "Train >>>> Loss: 5.88621\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/13\n",
            "Train >>>> Loss: 5.87478\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/14\n",
            "Train >>>> Loss: 5.83958\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/15\n",
            "Train >>>> Loss: 5.87995\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/16\n",
            "Train >>>> Loss: 5.88438\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/17\n",
            "Train >>>> Loss: 5.88533\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/18\n",
            "Train >>>> Loss: 5.87308\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/19\n",
            "Train >>>> Loss: 5.89052\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/20\n",
            "Train >>>> Loss: 5.86774\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/21\n",
            "Train >>>> Loss: 5.87811\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/22\n",
            "Train >>>> Loss: 5.86896\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/23\n",
            "Train >>>> Loss: 5.86523\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/24\n",
            "Train >>>> Loss: 5.8501\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/25\n",
            "Train >>>> Loss: 5.8698\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/26\n",
            "Train >>>> Loss: 5.85377\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/27\n",
            "Train >>>> Loss: 5.86551\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/28\n",
            "Train >>>> Loss: 5.88453\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/29\n",
            "Train >>>> Loss: 5.88399\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/30\n",
            "Train >>>> Loss: 5.8705\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/31\n",
            "Train >>>> Loss: 5.88433\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/32\n",
            "Train >>>> Loss: 5.87968\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/33\n",
            "Train >>>> Loss: 5.84521\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/34\n",
            "Train >>>> Loss: 5.89294\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 61/35\n",
            "Train >>>> Loss: 5.86724\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.87213\n",
            "[Average Testing Loss]: 5.88627\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/0\n",
            "Train >>>> Loss: 5.9089\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/1\n",
            "Train >>>> Loss: 5.8621\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/2\n",
            "Train >>>> Loss: 5.84445\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/3\n",
            "Train >>>> Loss: 5.89813\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/4\n",
            "Train >>>> Loss: 5.8548\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/5\n",
            "Train >>>> Loss: 5.84856\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/6\n",
            "Train >>>> Loss: 5.88597\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/7\n",
            "Train >>>> Loss: 5.88106\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/8\n",
            "Train >>>> Loss: 5.84578\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/9\n",
            "Train >>>> Loss: 5.82945\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/10\n",
            "Train >>>> Loss: 5.83194\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/11\n",
            "Train >>>> Loss: 5.85787\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/12\n",
            "Train >>>> Loss: 5.83513\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/13\n",
            "Train >>>> Loss: 5.88773\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/14\n",
            "Train >>>> Loss: 5.87636\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/15\n",
            "Train >>>> Loss: 5.86713\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/16\n",
            "Train >>>> Loss: 5.83045\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/17\n",
            "Train >>>> Loss: 5.88874\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/18\n",
            "Train >>>> Loss: 5.90822\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/19\n",
            "Train >>>> Loss: 5.89883\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/20\n",
            "Train >>>> Loss: 5.87225\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/21\n",
            "Train >>>> Loss: 5.91129\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/22\n",
            "Train >>>> Loss: 5.86837\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/23\n",
            "Train >>>> Loss: 5.87556\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/24\n",
            "Train >>>> Loss: 5.86603\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/25\n",
            "Train >>>> Loss: 5.86635\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/26\n",
            "Train >>>> Loss: 5.86236\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/27\n",
            "Train >>>> Loss: 5.88457\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/28\n",
            "Train >>>> Loss: 5.81406\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/29\n",
            "Train >>>> Loss: 5.88605\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/30\n",
            "Train >>>> Loss: 5.8794\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/31\n",
            "Train >>>> Loss: 5.88823\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/32\n",
            "Train >>>> Loss: 5.86354\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/33\n",
            "Train >>>> Loss: 5.87664\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/34\n",
            "Train >>>> Loss: 5.91764\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 62/35\n",
            "Train >>>> Loss: 5.84748\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.87004\n",
            "[Average Testing Loss]: 5.85604\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/0\n",
            "Train >>>> Loss: 5.89624\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/1\n",
            "Train >>>> Loss: 5.89687\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/2\n",
            "Train >>>> Loss: 5.85541\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/3\n",
            "Train >>>> Loss: 5.8582\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/4\n",
            "Train >>>> Loss: 5.87946\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/5\n",
            "Train >>>> Loss: 5.84198\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/6\n",
            "Train >>>> Loss: 5.85241\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/7\n",
            "Train >>>> Loss: 5.86946\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/8\n",
            "Train >>>> Loss: 5.84226\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/9\n",
            "Train >>>> Loss: 5.79986\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/10\n",
            "Train >>>> Loss: 5.86605\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/11\n",
            "Train >>>> Loss: 5.84612\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/12\n",
            "Train >>>> Loss: 5.85613\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/13\n",
            "Train >>>> Loss: 5.89064\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/14\n",
            "Train >>>> Loss: 5.85191\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/15\n",
            "Train >>>> Loss: 5.85665\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/16\n",
            "Train >>>> Loss: 5.86602\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/17\n",
            "Train >>>> Loss: 5.88649\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/18\n",
            "Train >>>> Loss: 5.87137\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/19\n",
            "Train >>>> Loss: 5.89081\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/20\n",
            "Train >>>> Loss: 5.87538\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/21\n",
            "Train >>>> Loss: 5.85114\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/22\n",
            "Train >>>> Loss: 5.88621\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/23\n",
            "Train >>>> Loss: 5.84751\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/24\n",
            "Train >>>> Loss: 5.89825\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/25\n",
            "Train >>>> Loss: 5.86171\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/26\n",
            "Train >>>> Loss: 5.91561\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/27\n",
            "Train >>>> Loss: 5.85174\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/28\n",
            "Train >>>> Loss: 5.83889\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/29\n",
            "Train >>>> Loss: 5.83665\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/30\n",
            "Train >>>> Loss: 5.85436\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/31\n",
            "Train >>>> Loss: 5.87659\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/32\n",
            "Train >>>> Loss: 5.87177\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/33\n",
            "Train >>>> Loss: 5.90238\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/34\n",
            "Train >>>> Loss: 5.86689\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 63/35\n",
            "Train >>>> Loss: 5.89568\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.86681\n",
            "[Average Testing Loss]: 5.86929\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/0\n",
            "Train >>>> Loss: 5.88216\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/1\n",
            "Train >>>> Loss: 5.87266\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/2\n",
            "Train >>>> Loss: 5.88668\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/3\n",
            "Train >>>> Loss: 5.85886\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/4\n",
            "Train >>>> Loss: 5.8637\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/5\n",
            "Train >>>> Loss: 5.89406\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/6\n",
            "Train >>>> Loss: 5.87646\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/7\n",
            "Train >>>> Loss: 5.86692\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/8\n",
            "Train >>>> Loss: 5.85897\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/9\n",
            "Train >>>> Loss: 5.83933\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/10\n",
            "Train >>>> Loss: 5.83811\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/11\n",
            "Train >>>> Loss: 5.85907\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/12\n",
            "Train >>>> Loss: 5.8854\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/13\n",
            "Train >>>> Loss: 5.87098\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/14\n",
            "Train >>>> Loss: 5.88513\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/15\n",
            "Train >>>> Loss: 5.87912\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/16\n",
            "Train >>>> Loss: 5.88739\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/17\n",
            "Train >>>> Loss: 5.86962\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/18\n",
            "Train >>>> Loss: 5.87087\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/19\n",
            "Train >>>> Loss: 5.86529\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/20\n",
            "Train >>>> Loss: 5.83092\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/21\n",
            "Train >>>> Loss: 5.85717\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/22\n",
            "Train >>>> Loss: 5.85568\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/23\n",
            "Train >>>> Loss: 5.78436\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/24\n",
            "Train >>>> Loss: 5.89524\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/25\n",
            "Train >>>> Loss: 5.83041\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/26\n",
            "Train >>>> Loss: 5.82652\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/27\n",
            "Train >>>> Loss: 5.89551\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/28\n",
            "Train >>>> Loss: 5.87798\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/29\n",
            "Train >>>> Loss: 5.91003\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/30\n",
            "Train >>>> Loss: 5.85631\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/31\n",
            "Train >>>> Loss:  5.845\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/32\n",
            "Train >>>> Loss: 5.86361\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/33\n",
            "Train >>>> Loss: 5.85257\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/34\n",
            "Train >>>> Loss: 5.85823\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 64/35\n",
            "Train >>>> Loss: 5.89022\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.86501\n",
            "[Average Testing Loss]: 5.84938\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/0\n",
            "Train >>>> Loss: 5.86748\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/1\n",
            "Train >>>> Loss: 5.8404\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/2\n",
            "Train >>>> Loss: 5.90391\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/3\n",
            "Train >>>> Loss: 5.8025\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/4\n",
            "Train >>>> Loss: 5.85683\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/5\n",
            "Train >>>> Loss: 5.89347\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/6\n",
            "Train >>>> Loss: 5.83491\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/7\n",
            "Train >>>> Loss: 5.90926\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/8\n",
            "Train >>>> Loss: 5.81174\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/9\n",
            "Train >>>> Loss: 5.87167\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/10\n",
            "Train >>>> Loss: 5.88087\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/11\n",
            "Train >>>> Loss: 5.88162\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/12\n",
            "Train >>>> Loss: 5.87433\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/13\n",
            "Train >>>> Loss: 5.86927\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/14\n",
            "Train >>>> Loss: 5.91741\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/15\n",
            "Train >>>> Loss: 5.8997\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/16\n",
            "Train >>>> Loss: 5.88884\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/17\n",
            "Train >>>> Loss: 5.84506\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/18\n",
            "Train >>>> Loss: 5.83805\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/19\n",
            "Train >>>> Loss:  5.889\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/20\n",
            "Train >>>> Loss: 5.82112\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/21\n",
            "Train >>>> Loss: 5.88555\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/22\n",
            "Train >>>> Loss: 5.8762\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/23\n",
            "Train >>>> Loss: 5.85994\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/24\n",
            "Train >>>> Loss: 5.88945\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/25\n",
            "Train >>>> Loss: 5.83474\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/26\n",
            "Train >>>> Loss: 5.88128\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/27\n",
            "Train >>>> Loss: 5.86742\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/28\n",
            "Train >>>> Loss: 5.88425\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/29\n",
            "Train >>>> Loss: 5.89587\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/30\n",
            "Train >>>> Loss: 5.88763\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/31\n",
            "Train >>>> Loss: 5.89446\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/32\n",
            "Train >>>> Loss: 5.91234\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/33\n",
            "Train >>>> Loss: 5.86835\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/34\n",
            "Train >>>> Loss: 5.85892\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 65/35\n",
            "Train >>>> Loss: 5.87134\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.87126\n",
            "[Average Testing Loss]: 5.82743\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/0\n",
            "Train >>>> Loss: 5.8946\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/1\n",
            "Train >>>> Loss: 5.88666\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/2\n",
            "Train >>>> Loss: 5.84072\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/3\n",
            "Train >>>> Loss: 5.85385\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/4\n",
            "Train >>>> Loss: 5.84627\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/5\n",
            "Train >>>> Loss: 5.85468\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/6\n",
            "Train >>>> Loss: 5.88197\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/7\n",
            "Train >>>> Loss: 5.8399\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/8\n",
            "Train >>>> Loss: 5.84387\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/9\n",
            "Train >>>> Loss: 5.85177\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/10\n",
            "Train >>>> Loss: 5.89662\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/11\n",
            "Train >>>> Loss: 5.86298\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/12\n",
            "Train >>>> Loss: 5.84195\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/13\n",
            "Train >>>> Loss: 5.86392\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/14\n",
            "Train >>>> Loss: 5.83239\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/15\n",
            "Train >>>> Loss: 5.8876\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/16\n",
            "Train >>>> Loss: 5.90335\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/17\n",
            "Train >>>> Loss: 5.89916\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/18\n",
            "Train >>>> Loss: 5.84767\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/19\n",
            "Train >>>> Loss: 5.90439\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/20\n",
            "Train >>>> Loss: 5.89105\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/21\n",
            "Train >>>> Loss: 5.87285\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/22\n",
            "Train >>>> Loss: 5.86902\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/23\n",
            "Train >>>> Loss: 5.84396\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/24\n",
            "Train >>>> Loss: 5.87972\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/25\n",
            "Train >>>> Loss: 5.82568\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/26\n",
            "Train >>>> Loss: 5.83963\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/27\n",
            "Train >>>> Loss: 5.90164\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/28\n",
            "Train >>>> Loss: 5.85218\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/29\n",
            "Train >>>> Loss: 5.84702\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/30\n",
            "Train >>>> Loss: 5.8534\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/31\n",
            "Train >>>> Loss: 5.91426\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/32\n",
            "Train >>>> Loss: 5.83474\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/33\n",
            "Train >>>> Loss: 5.85574\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/34\n",
            "Train >>>> Loss: 5.87105\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 66/35\n",
            "Train >>>> Loss:  5.881\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.86576\n",
            "[Average Testing Loss]: 5.83644\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/0\n",
            "Train >>>> Loss: 5.93889\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/1\n",
            "Train >>>> Loss: 5.8891\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/2\n",
            "Train >>>> Loss: 5.87045\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/3\n",
            "Train >>>> Loss: 5.85807\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/4\n",
            "Train >>>> Loss: 5.8641\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/5\n",
            "Train >>>> Loss:  5.889\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/6\n",
            "Train >>>> Loss: 5.88722\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/7\n",
            "Train >>>> Loss: 5.87201\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/8\n",
            "Train >>>> Loss: 5.89088\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/9\n",
            "Train >>>> Loss: 5.87257\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/10\n",
            "Train >>>> Loss: 5.83752\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/11\n",
            "Train >>>> Loss: 5.86482\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/12\n",
            "Train >>>> Loss: 5.84193\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/13\n",
            "Train >>>> Loss: 5.89686\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/14\n",
            "Train >>>> Loss: 5.85807\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/15\n",
            "Train >>>> Loss: 5.8581\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/16\n",
            "Train >>>> Loss: 5.8382\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/17\n",
            "Train >>>> Loss: 5.89946\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/18\n",
            "Train >>>> Loss: 5.86628\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/19\n",
            "Train >>>> Loss: 5.87986\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/20\n",
            "Train >>>> Loss: 5.84057\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/21\n",
            "Train >>>> Loss: 5.88854\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/22\n",
            "Train >>>> Loss: 5.86725\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/23\n",
            "Train >>>> Loss: 5.82077\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/24\n",
            "Train >>>> Loss: 5.86122\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/25\n",
            "Train >>>> Loss: 5.83588\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/26\n",
            "Train >>>> Loss: 5.88317\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/27\n",
            "Train >>>> Loss: 5.92452\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/28\n",
            "Train >>>> Loss: 5.87772\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/29\n",
            "Train >>>> Loss: 5.84444\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/30\n",
            "Train >>>> Loss:   5.83\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/31\n",
            "Train >>>> Loss: 5.90195\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/32\n",
            "Train >>>> Loss: 5.86508\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/33\n",
            "Train >>>> Loss: 5.88786\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/34\n",
            "Train >>>> Loss: 5.88391\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 67/35\n",
            "Train >>>> Loss: 5.88812\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.87151\n",
            "[Average Testing Loss]: 5.85387\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/0\n",
            "Train >>>> Loss: 5.8974\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/1\n",
            "Train >>>> Loss: 5.8639\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/2\n",
            "Train >>>> Loss: 5.8896\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/3\n",
            "Train >>>> Loss: 5.84892\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/4\n",
            "Train >>>> Loss: 5.81175\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/5\n",
            "Train >>>> Loss:  5.848\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/6\n",
            "Train >>>> Loss: 5.82226\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/7\n",
            "Train >>>> Loss: 5.87667\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/8\n",
            "Train >>>> Loss: 5.85877\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/9\n",
            "Train >>>> Loss: 5.86655\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/10\n",
            "Train >>>> Loss: 5.83659\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/11\n",
            "Train >>>> Loss: 5.85183\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/12\n",
            "Train >>>> Loss:  5.848\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/13\n",
            "Train >>>> Loss: 5.88367\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/14\n",
            "Train >>>> Loss: 5.85923\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/15\n",
            "Train >>>> Loss: 5.84727\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/16\n",
            "Train >>>> Loss: 5.86471\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/17\n",
            "Train >>>> Loss: 5.88918\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/18\n",
            "Train >>>> Loss: 5.87022\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/19\n",
            "Train >>>> Loss: 5.85125\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/20\n",
            "Train >>>> Loss: 5.85559\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/21\n",
            "Train >>>> Loss: 5.86531\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/22\n",
            "Train >>>> Loss: 5.89667\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/23\n",
            "Train >>>> Loss: 5.80917\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/24\n",
            "Train >>>> Loss: 5.84741\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/25\n",
            "Train >>>> Loss: 5.84208\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/26\n",
            "Train >>>> Loss: 5.89292\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/27\n",
            "Train >>>> Loss: 5.91271\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/28\n",
            "Train >>>> Loss: 5.82042\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/29\n",
            "Train >>>> Loss: 5.87253\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/30\n",
            "Train >>>> Loss: 5.88765\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/31\n",
            "Train >>>> Loss: 5.82489\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/32\n",
            "Train >>>> Loss: 5.88287\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/33\n",
            "Train >>>> Loss: 5.84877\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/34\n",
            "Train >>>> Loss: 5.88186\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 68/35\n",
            "Train >>>> Loss: 5.86252\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.86081\n",
            "[Average Testing Loss]: 5.84269\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/0\n",
            "Train >>>> Loss: 5.90432\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/1\n",
            "Train >>>> Loss: 5.83032\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/2\n",
            "Train >>>> Loss: 5.8584\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/3\n",
            "Train >>>> Loss: 5.89983\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/4\n",
            "Train >>>> Loss: 5.88425\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/5\n",
            "Train >>>> Loss: 5.83754\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/6\n",
            "Train >>>> Loss: 5.90108\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/7\n",
            "Train >>>> Loss: 5.85431\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/8\n",
            "Train >>>> Loss: 5.85031\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/9\n",
            "Train >>>> Loss: 5.84743\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/10\n",
            "Train >>>> Loss: 5.8928\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/11\n",
            "Train >>>> Loss: 5.85151\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/12\n",
            "Train >>>> Loss: 5.86289\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/13\n",
            "Train >>>> Loss: 5.89439\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/14\n",
            "Train >>>> Loss: 5.83924\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/15\n",
            "Train >>>> Loss: 5.90322\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/16\n",
            "Train >>>> Loss: 5.84755\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/17\n",
            "Train >>>> Loss: 5.86583\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/18\n",
            "Train >>>> Loss: 5.82332\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/19\n",
            "Train >>>> Loss: 5.88121\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/20\n",
            "Train >>>> Loss: 5.79983\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/21\n",
            "Train >>>> Loss: 5.88385\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/22\n",
            "Train >>>> Loss: 5.87388\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/23\n",
            "Train >>>> Loss: 5.90053\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/24\n",
            "Train >>>> Loss: 5.87152\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/25\n",
            "Train >>>> Loss: 5.87032\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/26\n",
            "Train >>>> Loss: 5.86953\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/27\n",
            "Train >>>> Loss: 5.86659\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/28\n",
            "Train >>>> Loss: 5.86833\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/29\n",
            "Train >>>> Loss: 5.88157\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/30\n",
            "Train >>>> Loss: 5.87348\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/31\n",
            "Train >>>> Loss: 5.85152\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/32\n",
            "Train >>>> Loss: 5.86956\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/33\n",
            "Train >>>> Loss: 5.86372\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/34\n",
            "Train >>>> Loss: 5.87063\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 69/35\n",
            "Train >>>> Loss: 5.87434\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.86719\n",
            "[Average Testing Loss]: 5.85598\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/0\n",
            "Train >>>> Loss: 5.87726\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/1\n",
            "Train >>>> Loss: 5.8854\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/2\n",
            "Train >>>> Loss: 5.90176\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/3\n",
            "Train >>>> Loss: 5.89506\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/4\n",
            "Train >>>> Loss: 5.87509\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/5\n",
            "Train >>>> Loss: 5.86058\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/6\n",
            "Train >>>> Loss: 5.88477\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/7\n",
            "Train >>>> Loss: 5.85276\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/8\n",
            "Train >>>> Loss: 5.84337\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/9\n",
            "Train >>>> Loss: 5.86079\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/10\n",
            "Train >>>> Loss: 5.89549\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/11\n",
            "Train >>>> Loss: 5.88248\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/12\n",
            "Train >>>> Loss: 5.85413\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/13\n",
            "Train >>>> Loss: 5.86635\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/14\n",
            "Train >>>> Loss: 5.88631\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/15\n",
            "Train >>>> Loss: 5.85711\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/16\n",
            "Train >>>> Loss: 5.86518\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/17\n",
            "Train >>>> Loss: 5.84602\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/18\n",
            "Train >>>> Loss: 5.91321\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/19\n",
            "Train >>>> Loss: 5.89203\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/20\n",
            "Train >>>> Loss: 5.8994\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/21\n",
            "Train >>>> Loss: 5.87148\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/22\n",
            "Train >>>> Loss: 5.86287\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/23\n",
            "Train >>>> Loss:   5.87\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/24\n",
            "Train >>>> Loss: 5.87652\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/25\n",
            "Train >>>> Loss: 5.86401\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/26\n",
            "Train >>>> Loss: 5.88798\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/27\n",
            "Train >>>> Loss: 5.86384\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/28\n",
            "Train >>>> Loss: 5.8852\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/29\n",
            "Train >>>> Loss: 5.8549\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/30\n",
            "Train >>>> Loss: 5.92384\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/31\n",
            "Train >>>> Loss: 5.83346\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/32\n",
            "Train >>>> Loss: 5.90592\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/33\n",
            "Train >>>> Loss: 5.88204\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/34\n",
            "Train >>>> Loss: 5.87689\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 70/35\n",
            "Train >>>> Loss: 5.87853\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.87589\n",
            "[Average Testing Loss]: 5.86055\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/0\n",
            "Train >>>> Loss: 5.87578\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/1\n",
            "Train >>>> Loss: 5.8917\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/2\n",
            "Train >>>> Loss: 5.86995\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/3\n",
            "Train >>>> Loss: 5.86773\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/4\n",
            "Train >>>> Loss: 5.88934\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/5\n",
            "Train >>>> Loss: 5.86899\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/6\n",
            "Train >>>> Loss: 5.86962\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/7\n",
            "Train >>>> Loss: 5.86031\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/8\n",
            "Train >>>> Loss: 5.83955\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/9\n",
            "Train >>>> Loss: 5.8322\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/10\n",
            "Train >>>> Loss: 5.89897\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/11\n",
            "Train >>>> Loss: 5.90337\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/12\n",
            "Train >>>> Loss: 5.86653\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/13\n",
            "Train >>>> Loss: 5.83658\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/14\n",
            "Train >>>> Loss: 5.81642\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/15\n",
            "Train >>>> Loss: 5.8466\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/16\n",
            "Train >>>> Loss: 5.87805\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/17\n",
            "Train >>>> Loss: 5.8666\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/18\n",
            "Train >>>> Loss: 5.87028\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/19\n",
            "Train >>>> Loss: 5.88678\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/20\n",
            "Train >>>> Loss: 5.87495\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/21\n",
            "Train >>>> Loss: 5.83583\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/22\n",
            "Train >>>> Loss: 5.87854\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/23\n",
            "Train >>>> Loss: 5.88392\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/24\n",
            "Train >>>> Loss: 5.86421\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/25\n",
            "Train >>>> Loss: 5.8717\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/26\n",
            "Train >>>> Loss: 5.85271\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/27\n",
            "Train >>>> Loss: 5.8792\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/28\n",
            "Train >>>> Loss: 5.85997\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/29\n",
            "Train >>>> Loss: 5.8269\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/30\n",
            "Train >>>> Loss: 5.87866\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/31\n",
            "Train >>>> Loss: 5.86913\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/32\n",
            "Train >>>> Loss: 5.88188\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/33\n",
            "Train >>>> Loss: 5.86261\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/34\n",
            "Train >>>> Loss: 5.8733\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 71/35\n",
            "Train >>>> Loss: 5.81212\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.86503\n",
            "[Average Testing Loss]: 5.86611\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/0\n",
            "Train >>>> Loss: 5.89125\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/1\n",
            "Train >>>> Loss: 5.84879\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/2\n",
            "Train >>>> Loss: 5.83446\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/3\n",
            "Train >>>> Loss: 5.87292\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/4\n",
            "Train >>>> Loss: 5.86729\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/5\n",
            "Train >>>> Loss: 5.84166\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/6\n",
            "Train >>>> Loss: 5.83251\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/7\n",
            "Train >>>> Loss: 5.89894\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/8\n",
            "Train >>>> Loss: 5.89541\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/9\n",
            "Train >>>> Loss: 5.89779\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/10\n",
            "Train >>>> Loss: 5.87832\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/11\n",
            "Train >>>> Loss: 5.83776\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/12\n",
            "Train >>>> Loss: 5.85528\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/13\n",
            "Train >>>> Loss: 5.88968\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/14\n",
            "Train >>>> Loss: 5.87202\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/15\n",
            "Train >>>> Loss: 5.87128\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/16\n",
            "Train >>>> Loss: 5.87851\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/17\n",
            "Train >>>> Loss: 5.87152\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/18\n",
            "Train >>>> Loss: 5.85614\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/19\n",
            "Train >>>> Loss: 5.8475\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/20\n",
            "Train >>>> Loss: 5.85462\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/21\n",
            "Train >>>> Loss: 5.85297\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/22\n",
            "Train >>>> Loss: 5.85235\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/23\n",
            "Train >>>> Loss: 5.82452\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/24\n",
            "Train >>>> Loss: 5.89799\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/25\n",
            "Train >>>> Loss: 5.88537\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/26\n",
            "Train >>>> Loss: 5.83339\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/27\n",
            "Train >>>> Loss: 5.85287\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/28\n",
            "Train >>>> Loss: 5.86755\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/29\n",
            "Train >>>> Loss: 5.85824\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/30\n",
            "Train >>>> Loss: 5.85737\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/31\n",
            "Train >>>> Loss: 5.8627\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/32\n",
            "Train >>>> Loss: 5.85417\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/33\n",
            "Train >>>> Loss: 5.81457\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/34\n",
            "Train >>>> Loss: 5.87505\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 72/35\n",
            "Train >>>> Loss: 5.87294\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.86266\n",
            "[Average Testing Loss]:  5.843\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/0\n",
            "Train >>>> Loss: 5.87287\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/1\n",
            "Train >>>> Loss: 5.86063\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/2\n",
            "Train >>>> Loss: 5.88045\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/3\n",
            "Train >>>> Loss: 5.86399\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/4\n",
            "Train >>>> Loss: 5.84451\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/5\n",
            "Train >>>> Loss: 5.84044\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/6\n",
            "Train >>>> Loss: 5.83086\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/7\n",
            "Train >>>> Loss: 5.84602\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/8\n",
            "Train >>>> Loss: 5.82461\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/9\n",
            "Train >>>> Loss: 5.84125\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/10\n",
            "Train >>>> Loss: 5.85471\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/11\n",
            "Train >>>> Loss: 5.87597\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/12\n",
            "Train >>>> Loss: 5.85545\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/13\n",
            "Train >>>> Loss: 5.82835\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/14\n",
            "Train >>>> Loss: 5.83014\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/15\n",
            "Train >>>> Loss: 5.8439\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/16\n",
            "Train >>>> Loss: 5.82289\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/17\n",
            "Train >>>> Loss: 5.86972\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/18\n",
            "Train >>>> Loss: 5.87853\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/19\n",
            "Train >>>> Loss: 5.88491\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/20\n",
            "Train >>>> Loss: 5.83488\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/21\n",
            "Train >>>> Loss: 5.79998\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/22\n",
            "Train >>>> Loss: 5.79082\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/23\n",
            "Train >>>> Loss: 5.76569\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/24\n",
            "Train >>>> Loss: 5.81785\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/25\n",
            "Train >>>> Loss: 5.85795\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/26\n",
            "Train >>>> Loss: 5.83236\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/27\n",
            "Train >>>> Loss: 5.84715\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/28\n",
            "Train >>>> Loss: 5.84047\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/29\n",
            "Train >>>> Loss: 5.83971\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/30\n",
            "Train >>>> Loss: 5.84318\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/31\n",
            "Train >>>> Loss: 5.85766\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/32\n",
            "Train >>>> Loss: 5.8463\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/33\n",
            "Train >>>> Loss: 5.81352\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/34\n",
            "Train >>>> Loss: 5.83459\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 73/35\n",
            "Train >>>> Loss: 5.82114\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.84149\n",
            "[Average Testing Loss]: 5.80247\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/0\n",
            "Train >>>> Loss: 5.81872\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/1\n",
            "Train >>>> Loss: 5.80731\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/2\n",
            "Train >>>> Loss: 5.81785\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/3\n",
            "Train >>>> Loss: 5.79004\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/4\n",
            "Train >>>> Loss: 5.78827\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/5\n",
            "Train >>>> Loss: 5.77683\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/6\n",
            "Train >>>> Loss: 5.81818\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/7\n",
            "Train >>>> Loss: 5.87356\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/8\n",
            "Train >>>> Loss:  5.846\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/9\n",
            "Train >>>> Loss:  5.842\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/10\n",
            "Train >>>> Loss: 5.90811\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/11\n",
            "Train >>>> Loss: 5.80307\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/12\n",
            "Train >>>> Loss: 5.82719\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/13\n",
            "Train >>>> Loss: 5.81639\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/14\n",
            "Train >>>> Loss: 5.8564\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/15\n",
            "Train >>>> Loss: 5.8501\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/16\n",
            "Train >>>> Loss: 5.80986\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/17\n",
            "Train >>>> Loss: 5.84791\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/18\n",
            "Train >>>> Loss: 5.87984\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/19\n",
            "Train >>>> Loss: 5.84355\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/20\n",
            "Train >>>> Loss: 5.81317\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/21\n",
            "Train >>>> Loss: 5.81365\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/22\n",
            "Train >>>> Loss: 5.84614\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/23\n",
            "Train >>>> Loss: 5.82986\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/24\n",
            "Train >>>> Loss: 5.85045\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/25\n",
            "Train >>>> Loss: 5.85357\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/26\n",
            "Train >>>> Loss: 5.81445\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/27\n",
            "Train >>>> Loss: 5.82981\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/28\n",
            "Train >>>> Loss: 5.77369\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/29\n",
            "Train >>>> Loss: 5.79022\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/30\n",
            "Train >>>> Loss: 5.84994\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/31\n",
            "Train >>>> Loss: 5.8315\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/32\n",
            "Train >>>> Loss: 5.86518\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/33\n",
            "Train >>>> Loss: 5.77044\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/34\n",
            "Train >>>> Loss: 5.81323\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 74/35\n",
            "Train >>>> Loss: 5.82324\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.82749\n",
            "[Average Testing Loss]: 5.8142\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/0\n",
            "Train >>>> Loss: 5.82942\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/1\n",
            "Train >>>> Loss: 5.83864\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/2\n",
            "Train >>>> Loss: 5.82913\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/3\n",
            "Train >>>> Loss: 5.83165\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/4\n",
            "Train >>>> Loss: 5.84072\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/5\n",
            "Train >>>> Loss: 5.7672\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/6\n",
            "Train >>>> Loss: 5.84469\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/7\n",
            "Train >>>> Loss: 5.84976\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/8\n",
            "Train >>>> Loss: 5.82812\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/9\n",
            "Train >>>> Loss: 5.82475\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/10\n",
            "Train >>>> Loss: 5.8238\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/11\n",
            "Train >>>> Loss: 5.82579\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/12\n",
            "Train >>>> Loss: 5.83658\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/13\n",
            "Train >>>> Loss: 5.77916\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/14\n",
            "Train >>>> Loss: 5.82353\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/15\n",
            "Train >>>> Loss: 5.84835\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/16\n",
            "Train >>>> Loss: 5.8115\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/17\n",
            "Train >>>> Loss: 5.8184\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/18\n",
            "Train >>>> Loss: 5.84637\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/19\n",
            "Train >>>> Loss: 5.80498\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/20\n",
            "Train >>>> Loss: 5.80478\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/21\n",
            "Train >>>> Loss: 5.81748\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/22\n",
            "Train >>>> Loss: 5.82724\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/23\n",
            "Train >>>> Loss: 5.83242\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/24\n",
            "Train >>>> Loss: 5.81376\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/25\n",
            "Train >>>> Loss: 5.80064\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/26\n",
            "Train >>>> Loss: 5.83703\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/27\n",
            "Train >>>> Loss: 5.80931\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/28\n",
            "Train >>>> Loss: 5.84584\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/29\n",
            "Train >>>> Loss: 5.80079\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/30\n",
            "Train >>>> Loss: 5.82371\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/31\n",
            "Train >>>> Loss: 5.84647\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/32\n",
            "Train >>>> Loss: 5.83696\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/33\n",
            "Train >>>> Loss:  5.829\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/34\n",
            "Train >>>> Loss: 5.81628\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 75/35\n",
            "Train >>>> Loss: 5.8038\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.82356\n",
            "[Average Testing Loss]: 5.84976\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/0\n",
            "Train >>>> Loss: 5.82962\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/1\n",
            "Train >>>> Loss: 5.82148\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/2\n",
            "Train >>>> Loss: 5.86463\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/3\n",
            "Train >>>> Loss: 5.89726\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/4\n",
            "Train >>>> Loss: 5.82028\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/5\n",
            "Train >>>> Loss: 5.79822\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/6\n",
            "Train >>>> Loss: 5.83783\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/7\n",
            "Train >>>> Loss: 5.8022\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/8\n",
            "Train >>>> Loss: 5.84847\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/9\n",
            "Train >>>> Loss: 5.84338\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/10\n",
            "Train >>>> Loss: 5.81723\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/11\n",
            "Train >>>> Loss: 5.86729\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/12\n",
            "Train >>>> Loss: 5.84773\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/13\n",
            "Train >>>> Loss: 5.87268\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/14\n",
            "Train >>>> Loss: 5.80356\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/15\n",
            "Train >>>> Loss: 5.85155\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/16\n",
            "Train >>>> Loss: 5.80196\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/17\n",
            "Train >>>> Loss: 5.80461\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/18\n",
            "Train >>>> Loss: 5.80185\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/19\n",
            "Train >>>> Loss: 5.87126\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/20\n",
            "Train >>>> Loss: 5.9041\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/21\n",
            "Train >>>> Loss: 5.85345\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/22\n",
            "Train >>>> Loss: 5.86327\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/23\n",
            "Train >>>> Loss: 5.8015\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/24\n",
            "Train >>>> Loss: 5.86132\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/25\n",
            "Train >>>> Loss: 5.84314\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/26\n",
            "Train >>>> Loss: 5.88385\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/27\n",
            "Train >>>> Loss: 5.79353\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/28\n",
            "Train >>>> Loss: 5.76535\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/29\n",
            "Train >>>> Loss:  5.815\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/30\n",
            "Train >>>> Loss: 5.8214\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/31\n",
            "Train >>>> Loss: 5.86946\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/32\n",
            "Train >>>> Loss: 5.80558\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/33\n",
            "Train >>>> Loss: 5.83352\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/34\n",
            "Train >>>> Loss: 5.81639\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 76/35\n",
            "Train >>>> Loss: 5.85143\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.8357\n",
            "[Average Testing Loss]: 5.82288\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/0\n",
            "Train >>>> Loss: 5.8394\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/1\n",
            "Train >>>> Loss: 5.81917\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/2\n",
            "Train >>>> Loss: 5.83918\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/3\n",
            "Train >>>> Loss: 5.84975\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/4\n",
            "Train >>>> Loss: 5.84439\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/5\n",
            "Train >>>> Loss: 5.82165\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/6\n",
            "Train >>>> Loss: 5.83022\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/7\n",
            "Train >>>> Loss: 5.80325\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/8\n",
            "Train >>>> Loss: 5.83947\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/9\n",
            "Train >>>> Loss: 5.84324\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/10\n",
            "Train >>>> Loss: 5.81689\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/11\n",
            "Train >>>> Loss: 5.79184\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/12\n",
            "Train >>>> Loss: 5.79785\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/13\n",
            "Train >>>> Loss: 5.8319\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/14\n",
            "Train >>>> Loss: 5.84378\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/15\n",
            "Train >>>> Loss: 5.82811\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/16\n",
            "Train >>>> Loss: 5.82362\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/17\n",
            "Train >>>> Loss: 5.78517\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/18\n",
            "Train >>>> Loss: 5.85363\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/19\n",
            "Train >>>> Loss: 5.82047\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/20\n",
            "Train >>>> Loss: 5.80334\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/21\n",
            "Train >>>> Loss: 5.81536\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/22\n",
            "Train >>>> Loss: 5.80351\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/23\n",
            "Train >>>> Loss: 5.8186\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/24\n",
            "Train >>>> Loss: 5.8581\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/25\n",
            "Train >>>> Loss: 5.81357\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/26\n",
            "Train >>>> Loss: 5.7834\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/27\n",
            "Train >>>> Loss: 5.79567\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/28\n",
            "Train >>>> Loss: 5.83102\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/29\n",
            "Train >>>> Loss: 5.78191\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/30\n",
            "Train >>>> Loss: 5.81502\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/31\n",
            "Train >>>> Loss: 5.80056\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/32\n",
            "Train >>>> Loss: 5.82354\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/33\n",
            "Train >>>> Loss: 5.85761\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/34\n",
            "Train >>>> Loss: 5.83531\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 77/35\n",
            "Train >>>> Loss: 5.86519\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.82291\n",
            "[Average Testing Loss]: 5.82551\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/0\n",
            "Train >>>> Loss: 5.8265\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/1\n",
            "Train >>>> Loss: 5.83244\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/2\n",
            "Train >>>> Loss: 5.82842\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/3\n",
            "Train >>>> Loss: 5.81301\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/4\n",
            "Train >>>> Loss: 5.80418\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/5\n",
            "Train >>>> Loss: 5.80572\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/6\n",
            "Train >>>> Loss: 5.87688\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/7\n",
            "Train >>>> Loss: 5.82705\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/8\n",
            "Train >>>> Loss: 5.82132\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/9\n",
            "Train >>>> Loss: 5.83955\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/10\n",
            "Train >>>> Loss: 5.81386\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/11\n",
            "Train >>>> Loss: 5.80985\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/12\n",
            "Train >>>> Loss: 5.80279\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/13\n",
            "Train >>>> Loss: 5.83021\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/14\n",
            "Train >>>> Loss: 5.86187\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/15\n",
            "Train >>>> Loss: 5.82516\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/16\n",
            "Train >>>> Loss: 5.8595\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/17\n",
            "Train >>>> Loss: 5.83304\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/18\n",
            "Train >>>> Loss: 5.83329\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/19\n",
            "Train >>>> Loss: 5.88757\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/20\n",
            "Train >>>> Loss: 5.81731\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/21\n",
            "Train >>>> Loss: 5.84448\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/22\n",
            "Train >>>> Loss: 5.86131\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/23\n",
            "Train >>>> Loss: 5.85354\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/24\n",
            "Train >>>> Loss: 5.88076\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/25\n",
            "Train >>>> Loss: 5.81532\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/26\n",
            "Train >>>> Loss: 5.81018\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/27\n",
            "Train >>>> Loss: 5.78038\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/28\n",
            "Train >>>> Loss: 5.82082\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/29\n",
            "Train >>>> Loss: 5.85092\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/30\n",
            "Train >>>> Loss: 5.81135\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/31\n",
            "Train >>>> Loss: 5.80979\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/32\n",
            "Train >>>> Loss: 5.79657\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/33\n",
            "Train >>>> Loss: 5.83535\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/34\n",
            "Train >>>> Loss: 5.81832\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 78/35\n",
            "Train >>>> Loss: 5.82582\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.82957\n",
            "[Average Testing Loss]: 5.79993\n",
            "checkpoint is saved !\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/0\n",
            "Train >>>> Loss: 5.83882\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/1\n",
            "Train >>>> Loss: 5.82625\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/2\n",
            "Train >>>> Loss: 5.79835\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/3\n",
            "Train >>>> Loss: 5.87135\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/4\n",
            "Train >>>> Loss: 5.82063\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/5\n",
            "Train >>>> Loss: 5.84877\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/6\n",
            "Train >>>> Loss: 5.83529\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/7\n",
            "Train >>>> Loss: 5.83006\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/8\n",
            "Train >>>> Loss: 5.8637\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/9\n",
            "Train >>>> Loss: 5.84541\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/10\n",
            "Train >>>> Loss: 5.82404\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/11\n",
            "Train >>>> Loss: 5.83138\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/12\n",
            "Train >>>> Loss: 5.81667\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/13\n",
            "Train >>>> Loss: 5.8409\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/14\n",
            "Train >>>> Loss: 5.77524\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/15\n",
            "Train >>>> Loss: 5.84263\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/16\n",
            "Train >>>> Loss: 5.81065\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/17\n",
            "Train >>>> Loss: 5.75167\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/18\n",
            "Train >>>> Loss: 5.84787\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/19\n",
            "Train >>>> Loss: 5.76994\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/20\n",
            "Train >>>> Loss:  5.829\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/21\n",
            "Train >>>> Loss: 5.80368\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/22\n",
            "Train >>>> Loss: 5.84509\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/23\n",
            "Train >>>> Loss: 5.78203\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/24\n",
            "Train >>>> Loss: 5.86313\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/25\n",
            "Train >>>> Loss: 5.78881\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/26\n",
            "Train >>>> Loss: 5.82187\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/27\n",
            "Train >>>> Loss: 5.83529\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/28\n",
            "Train >>>> Loss: 5.82779\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/29\n",
            "Train >>>> Loss: 5.78133\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/30\n",
            "Train >>>> Loss: 5.83203\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/31\n",
            "Train >>>> Loss: 5.83548\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/32\n",
            "Train >>>> Loss: 5.86557\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/33\n",
            "Train >>>> Loss: 5.84286\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/34\n",
            "Train >>>> Loss: 5.75554\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 79/35\n",
            "Train >>>> Loss: 5.84903\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.82356\n",
            "[Average Testing Loss]: 5.80818\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/0\n",
            "Train >>>> Loss: 5.8339\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/1\n",
            "Train >>>> Loss: 5.83689\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/2\n",
            "Train >>>> Loss: 5.85466\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/3\n",
            "Train >>>> Loss: 5.85244\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/4\n",
            "Train >>>> Loss: 5.81828\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/5\n",
            "Train >>>> Loss: 5.80355\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/6\n",
            "Train >>>> Loss: 5.83647\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/7\n",
            "Train >>>> Loss: 5.80672\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/8\n",
            "Train >>>> Loss: 5.84913\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/9\n",
            "Train >>>> Loss: 5.82001\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/10\n",
            "Train >>>> Loss: 5.85532\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/11\n",
            "Train >>>> Loss: 5.84794\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/12\n",
            "Train >>>> Loss: 5.84545\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/13\n",
            "Train >>>> Loss: 5.8532\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/14\n",
            "Train >>>> Loss: 5.81956\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/15\n",
            "Train >>>> Loss: 5.85053\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/16\n",
            "Train >>>> Loss: 5.88286\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/17\n",
            "Train >>>> Loss: 5.89281\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/18\n",
            "Train >>>> Loss: 5.84434\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/19\n",
            "Train >>>> Loss: 5.84163\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/20\n",
            "Train >>>> Loss: 5.85582\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/21\n",
            "Train >>>> Loss: 5.87692\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/22\n",
            "Train >>>> Loss:  5.861\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/23\n",
            "Train >>>> Loss: 5.83072\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/24\n",
            "Train >>>> Loss: 5.85249\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/25\n",
            "Train >>>> Loss: 5.81748\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/26\n",
            "Train >>>> Loss: 5.84715\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/27\n",
            "Train >>>> Loss:  5.811\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/28\n",
            "Train >>>> Loss: 5.87245\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/29\n",
            "Train >>>> Loss: 5.83844\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/30\n",
            "Train >>>> Loss: 5.8186\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/31\n",
            "Train >>>> Loss: 5.77877\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/32\n",
            "Train >>>> Loss: 5.86194\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/33\n",
            "Train >>>> Loss: 5.79553\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/34\n",
            "Train >>>> Loss: 5.8485\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 80/35\n",
            "Train >>>> Loss: 5.82716\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.83999\n",
            "[Average Testing Loss]: 5.8297\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/0\n",
            "Train >>>> Loss: 5.83491\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/1\n",
            "Train >>>> Loss: 5.82329\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/2\n",
            "Train >>>> Loss: 5.82076\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/3\n",
            "Train >>>> Loss: 5.80298\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/4\n",
            "Train >>>> Loss: 5.81724\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/5\n",
            "Train >>>> Loss: 5.77514\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/6\n",
            "Train >>>> Loss: 5.83467\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/7\n",
            "Train >>>> Loss: 5.83629\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/8\n",
            "Train >>>> Loss: 5.86927\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/9\n",
            "Train >>>> Loss: 5.81258\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/10\n",
            "Train >>>> Loss: 5.7851\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/11\n",
            "Train >>>> Loss: 5.79497\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/12\n",
            "Train >>>> Loss: 5.79351\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/13\n",
            "Train >>>> Loss: 5.81884\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/14\n",
            "Train >>>> Loss: 5.78942\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/15\n",
            "Train >>>> Loss: 5.79295\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/16\n",
            "Train >>>> Loss: 5.8062\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/17\n",
            "Train >>>> Loss: 5.87958\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/18\n",
            "Train >>>> Loss: 5.85453\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/19\n",
            "Train >>>> Loss: 5.85382\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/20\n",
            "Train >>>> Loss: 5.87722\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/21\n",
            "Train >>>> Loss: 5.85268\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/22\n",
            "Train >>>> Loss: 5.83854\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/23\n",
            "Train >>>> Loss: 5.84899\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/24\n",
            "Train >>>> Loss: 5.85338\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/25\n",
            "Train >>>> Loss: 5.8182\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/26\n",
            "Train >>>> Loss: 5.87114\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/27\n",
            "Train >>>> Loss: 5.83003\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/28\n",
            "Train >>>> Loss: 5.84157\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/29\n",
            "Train >>>> Loss: 5.82179\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/30\n",
            "Train >>>> Loss: 5.86691\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/31\n",
            "Train >>>> Loss: 5.83065\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/32\n",
            "Train >>>> Loss: 5.8251\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/33\n",
            "Train >>>> Loss: 5.81113\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/34\n",
            "Train >>>> Loss: 5.86028\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 81/35\n",
            "Train >>>> Loss: 5.87385\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.83104\n",
            "[Average Testing Loss]: 5.81496\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/0\n",
            "Train >>>> Loss: 5.85205\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/1\n",
            "Train >>>> Loss: 5.83535\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/2\n",
            "Train >>>> Loss: 5.81525\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/3\n",
            "Train >>>> Loss: 5.84404\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/4\n",
            "Train >>>> Loss: 5.83241\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/5\n",
            "Train >>>> Loss: 5.79624\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/6\n",
            "Train >>>> Loss: 5.84797\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/7\n",
            "Train >>>> Loss: 5.85165\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/8\n",
            "Train >>>> Loss: 5.87883\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/9\n",
            "Train >>>> Loss: 5.83908\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/10\n",
            "Train >>>> Loss: 5.78481\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/11\n",
            "Train >>>> Loss: 5.8345\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/12\n",
            "Train >>>> Loss: 5.77207\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/13\n",
            "Train >>>> Loss: 5.84524\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/14\n",
            "Train >>>> Loss: 5.81904\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/15\n",
            "Train >>>> Loss: 5.82283\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/16\n",
            "Train >>>> Loss: 5.85672\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/17\n",
            "Train >>>> Loss: 5.83643\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/18\n",
            "Train >>>> Loss: 5.8096\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/19\n",
            "Train >>>> Loss: 5.82657\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/20\n",
            "Train >>>> Loss: 5.82617\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/21\n",
            "Train >>>> Loss: 5.80802\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/22\n",
            "Train >>>> Loss: 5.86423\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/23\n",
            "Train >>>> Loss: 5.78779\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/24\n",
            "Train >>>> Loss: 5.82872\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/25\n",
            "Train >>>> Loss: 5.79306\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/26\n",
            "Train >>>> Loss: 5.81923\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/27\n",
            "Train >>>> Loss: 5.85199\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/28\n",
            "Train >>>> Loss: 5.86701\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/29\n",
            "Train >>>> Loss: 5.78785\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/30\n",
            "Train >>>> Loss: 5.81495\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/31\n",
            "Train >>>> Loss: 5.85754\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/32\n",
            "Train >>>> Loss: 5.89285\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/33\n",
            "Train >>>> Loss: 5.86795\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/34\n",
            "Train >>>> Loss: 5.87446\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 82/35\n",
            "Train >>>> Loss: 5.87877\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.83392\n",
            "[Average Testing Loss]: 5.85383\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/0\n",
            "Train >>>> Loss: 5.87679\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/1\n",
            "Train >>>> Loss: 5.89917\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/2\n",
            "Train >>>> Loss: 5.90588\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/3\n",
            "Train >>>> Loss: 5.86177\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/4\n",
            "Train >>>> Loss: 5.87207\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/5\n",
            "Train >>>> Loss: 5.87866\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/6\n",
            "Train >>>> Loss: 5.91554\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/7\n",
            "Train >>>> Loss: 5.85158\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/8\n",
            "Train >>>> Loss: 5.88395\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/9\n",
            "Train >>>> Loss: 5.88129\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/10\n",
            "Train >>>> Loss: 5.8363\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/11\n",
            "Train >>>> Loss: 5.85349\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/12\n",
            "Train >>>> Loss: 5.88584\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/13\n",
            "Train >>>> Loss: 5.88409\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/14\n",
            "Train >>>> Loss: 5.85657\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/15\n",
            "Train >>>> Loss: 5.83948\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/16\n",
            "Train >>>> Loss: 5.88039\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/17\n",
            "Train >>>> Loss: 5.87441\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/18\n",
            "Train >>>> Loss: 5.90014\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/19\n",
            "Train >>>> Loss: 5.86202\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/20\n",
            "Train >>>> Loss: 5.84574\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/21\n",
            "Train >>>> Loss: 5.89071\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/22\n",
            "Train >>>> Loss: 5.84717\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/23\n",
            "Train >>>> Loss: 5.8697\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/24\n",
            "Train >>>> Loss: 5.88537\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/25\n",
            "Train >>>> Loss: 5.87897\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/26\n",
            "Train >>>> Loss: 5.85056\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/27\n",
            "Train >>>> Loss: 5.84336\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/28\n",
            "Train >>>> Loss: 5.84225\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/29\n",
            "Train >>>> Loss: 5.87351\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/30\n",
            "Train >>>> Loss: 5.85755\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/31\n",
            "Train >>>> Loss: 5.83591\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/32\n",
            "Train >>>> Loss: 5.86969\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/33\n",
            "Train >>>> Loss: 5.86765\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/34\n",
            "Train >>>> Loss: 5.84745\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 83/35\n",
            "Train >>>> Loss: 5.83326\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.86773\n",
            "[Average Testing Loss]: 5.84782\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/0\n",
            "Train >>>> Loss: 5.8989\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/1\n",
            "Train >>>> Loss: 5.84885\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/2\n",
            "Train >>>> Loss: 5.83964\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/3\n",
            "Train >>>> Loss: 5.86711\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/4\n",
            "Train >>>> Loss: 5.88867\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/5\n",
            "Train >>>> Loss: 5.86784\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/6\n",
            "Train >>>> Loss: 5.88906\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/7\n",
            "Train >>>> Loss: 5.85211\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/8\n",
            "Train >>>> Loss: 5.8709\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/9\n",
            "Train >>>> Loss: 5.88785\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/10\n",
            "Train >>>> Loss: 5.86339\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/11\n",
            "Train >>>> Loss: 5.84358\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/12\n",
            "Train >>>> Loss: 5.88101\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/13\n",
            "Train >>>> Loss: 5.86888\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/14\n",
            "Train >>>> Loss: 5.83981\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/15\n",
            "Train >>>> Loss: 5.81447\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/16\n",
            "Train >>>> Loss: 5.85778\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/17\n",
            "Train >>>> Loss: 5.89392\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/18\n",
            "Train >>>> Loss: 5.86831\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/19\n",
            "Train >>>> Loss: 5.87189\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/20\n",
            "Train >>>> Loss: 5.89982\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/21\n",
            "Train >>>> Loss: 5.87346\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/22\n",
            "Train >>>> Loss: 5.85036\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/23\n",
            "Train >>>> Loss: 5.87697\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/24\n",
            "Train >>>> Loss: 5.85095\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/25\n",
            "Train >>>> Loss: 5.88841\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/26\n",
            "Train >>>> Loss: 5.82989\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/27\n",
            "Train >>>> Loss: 5.82489\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/28\n",
            "Train >>>> Loss: 5.82644\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/29\n",
            "Train >>>> Loss: 5.86341\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/30\n",
            "Train >>>> Loss: 5.81136\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/31\n",
            "Train >>>> Loss: 5.87505\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/32\n",
            "Train >>>> Loss: 5.86175\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/33\n",
            "Train >>>> Loss: 5.88905\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/34\n",
            "Train >>>> Loss: 5.86734\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 84/35\n",
            "Train >>>> Loss: 5.81237\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.86154\n",
            "[Average Testing Loss]: 5.86605\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/0\n",
            "Train >>>> Loss: 5.90054\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/1\n",
            "Train >>>> Loss: 5.87055\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/2\n",
            "Train >>>> Loss: 5.85378\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/3\n",
            "Train >>>> Loss: 5.90574\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/4\n",
            "Train >>>> Loss: 5.88205\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/5\n",
            "Train >>>> Loss: 5.82586\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/6\n",
            "Train >>>> Loss: 5.85044\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/7\n",
            "Train >>>> Loss: 5.83873\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/8\n",
            "Train >>>> Loss: 5.83455\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/9\n",
            "Train >>>> Loss: 5.90285\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/10\n",
            "Train >>>> Loss: 5.9044\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/11\n",
            "Train >>>> Loss: 5.8292\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/12\n",
            "Train >>>> Loss: 5.90338\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/13\n",
            "Train >>>> Loss: 5.88142\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/14\n",
            "Train >>>> Loss: 5.85558\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/15\n",
            "Train >>>> Loss: 5.84645\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/16\n",
            "Train >>>> Loss: 5.80306\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/17\n",
            "Train >>>> Loss: 5.89594\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/18\n",
            "Train >>>> Loss: 5.8496\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/19\n",
            "Train >>>> Loss: 5.85965\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/20\n",
            "Train >>>> Loss: 5.86782\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/21\n",
            "Train >>>> Loss: 5.85526\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/22\n",
            "Train >>>> Loss: 5.90712\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/23\n",
            "Train >>>> Loss: 5.87564\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/24\n",
            "Train >>>> Loss: 5.86129\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/25\n",
            "Train >>>> Loss: 5.86251\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/26\n",
            "Train >>>> Loss: 5.83924\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/27\n",
            "Train >>>> Loss: 5.88936\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/28\n",
            "Train >>>> Loss: 5.8561\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/29\n",
            "Train >>>> Loss: 5.84121\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/30\n",
            "Train >>>> Loss: 5.90941\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/31\n",
            "Train >>>> Loss: 5.88383\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/32\n",
            "Train >>>> Loss: 5.85077\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/33\n",
            "Train >>>> Loss: 5.80879\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/34\n",
            "Train >>>> Loss: 5.84658\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 85/35\n",
            "Train >>>> Loss: 5.83135\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.86333\n",
            "[Average Testing Loss]: 5.87626\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/0\n",
            "Train >>>> Loss: 5.8314\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/1\n",
            "Train >>>> Loss: 5.86215\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/2\n",
            "Train >>>> Loss: 5.87231\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/3\n",
            "Train >>>> Loss: 5.86415\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/4\n",
            "Train >>>> Loss: 5.87003\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/5\n",
            "Train >>>> Loss: 5.84355\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/6\n",
            "Train >>>> Loss: 5.84452\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/7\n",
            "Train >>>> Loss: 5.85446\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/8\n",
            "Train >>>> Loss: 5.84198\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/9\n",
            "Train >>>> Loss: 5.85746\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/10\n",
            "Train >>>> Loss: 5.88505\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/11\n",
            "Train >>>> Loss: 5.86042\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/12\n",
            "Train >>>> Loss: 5.8521\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/13\n",
            "Train >>>> Loss: 5.82172\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/14\n",
            "Train >>>> Loss: 5.8391\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/15\n",
            "Train >>>> Loss: 5.84486\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/16\n",
            "Train >>>> Loss: 5.87741\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/17\n",
            "Train >>>> Loss: 5.85749\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/18\n",
            "Train >>>> Loss: 5.83116\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/19\n",
            "Train >>>> Loss: 5.8346\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/20\n",
            "Train >>>> Loss: 5.85512\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/21\n",
            "Train >>>> Loss: 5.87363\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/22\n",
            "Train >>>> Loss: 5.84489\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/23\n",
            "Train >>>> Loss: 5.89038\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/24\n",
            "Train >>>> Loss: 5.87689\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/25\n",
            "Train >>>> Loss: 5.87535\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/26\n",
            "Train >>>> Loss: 5.86824\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/27\n",
            "Train >>>> Loss: 5.89943\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/28\n",
            "Train >>>> Loss: 5.82584\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/29\n",
            "Train >>>> Loss: 5.84819\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/30\n",
            "Train >>>> Loss: 5.88177\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/31\n",
            "Train >>>> Loss: 5.86281\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/32\n",
            "Train >>>> Loss: 5.91279\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/33\n",
            "Train >>>> Loss: 5.84436\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/34\n",
            "Train >>>> Loss: 5.89868\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 86/35\n",
            "Train >>>> Loss: 5.85557\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]:   5.86\n",
            "[Average Testing Loss]: 5.83404\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/0\n",
            "Train >>>> Loss: 5.84609\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/1\n",
            "Train >>>> Loss: 5.84395\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/2\n",
            "Train >>>> Loss: 5.8776\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/3\n",
            "Train >>>> Loss: 5.84535\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/4\n",
            "Train >>>> Loss: 5.84794\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/5\n",
            "Train >>>> Loss: 5.84333\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/6\n",
            "Train >>>> Loss: 5.87157\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/7\n",
            "Train >>>> Loss: 5.80039\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/8\n",
            "Train >>>> Loss: 5.83031\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/9\n",
            "Train >>>> Loss: 5.86916\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/10\n",
            "Train >>>> Loss: 5.87273\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/11\n",
            "Train >>>> Loss: 5.86229\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/12\n",
            "Train >>>> Loss: 5.86684\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/13\n",
            "Train >>>> Loss: 5.89244\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/14\n",
            "Train >>>> Loss: 5.85551\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/15\n",
            "Train >>>> Loss: 5.8564\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/16\n",
            "Train >>>> Loss: 5.81789\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/17\n",
            "Train >>>> Loss: 5.8271\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/18\n",
            "Train >>>> Loss: 5.8782\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/19\n",
            "Train >>>> Loss: 5.81985\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/20\n",
            "Train >>>> Loss: 5.86821\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/21\n",
            "Train >>>> Loss: 5.88823\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/22\n",
            "Train >>>> Loss: 5.8152\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/23\n",
            "Train >>>> Loss: 5.86376\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/24\n",
            "Train >>>> Loss: 5.84365\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/25\n",
            "Train >>>> Loss: 5.82716\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/26\n",
            "Train >>>> Loss: 5.89094\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/27\n",
            "Train >>>> Loss: 5.82774\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/28\n",
            "Train >>>> Loss: 5.85674\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/29\n",
            "Train >>>> Loss: 5.85012\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/30\n",
            "Train >>>> Loss: 5.87234\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/31\n",
            "Train >>>> Loss: 5.88078\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/32\n",
            "Train >>>> Loss: 5.89877\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/33\n",
            "Train >>>> Loss: 5.85384\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/34\n",
            "Train >>>> Loss: 5.89612\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 87/35\n",
            "Train >>>> Loss: 5.86887\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.85632\n",
            "[Average Testing Loss]: 5.86408\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/0\n",
            "Train >>>> Loss: 5.83708\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/1\n",
            "Train >>>> Loss: 5.85534\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/2\n",
            "Train >>>> Loss: 5.85235\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/3\n",
            "Train >>>> Loss: 5.81614\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/4\n",
            "Train >>>> Loss: 5.87509\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/5\n",
            "Train >>>> Loss: 5.85551\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/6\n",
            "Train >>>> Loss: 5.87513\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/7\n",
            "Train >>>> Loss: 5.88441\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/8\n",
            "Train >>>> Loss: 5.85792\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/9\n",
            "Train >>>> Loss: 5.9075\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/10\n",
            "Train >>>> Loss: 5.85472\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/11\n",
            "Train >>>> Loss: 5.84084\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/12\n",
            "Train >>>> Loss: 5.86538\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/13\n",
            "Train >>>> Loss: 5.87151\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/14\n",
            "Train >>>> Loss: 5.90939\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/15\n",
            "Train >>>> Loss: 5.84558\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/16\n",
            "Train >>>> Loss: 5.84339\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/17\n",
            "Train >>>> Loss: 5.87184\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/18\n",
            "Train >>>> Loss: 5.85458\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/19\n",
            "Train >>>> Loss: 5.87875\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/20\n",
            "Train >>>> Loss: 5.8269\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/21\n",
            "Train >>>> Loss: 5.83747\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/22\n",
            "Train >>>> Loss: 5.84198\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/23\n",
            "Train >>>> Loss: 5.88278\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/24\n",
            "Train >>>> Loss: 5.89184\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/25\n",
            "Train >>>> Loss: 5.88388\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/26\n",
            "Train >>>> Loss: 5.8834\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/27\n",
            "Train >>>> Loss: 5.83244\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/28\n",
            "Train >>>> Loss: 5.88209\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/29\n",
            "Train >>>> Loss: 5.87937\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/30\n",
            "Train >>>> Loss: 5.8575\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/31\n",
            "Train >>>> Loss: 5.85627\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/32\n",
            "Train >>>> Loss: 5.87469\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/33\n",
            "Train >>>> Loss: 5.85976\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/34\n",
            "Train >>>> Loss: 5.8533\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 88/35\n",
            "Train >>>> Loss: 5.82898\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.86181\n",
            "[Average Testing Loss]: 5.86999\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/0\n",
            "Train >>>> Loss: 5.83083\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/1\n",
            "Train >>>> Loss: 5.85055\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/2\n",
            "Train >>>> Loss: 5.90805\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/3\n",
            "Train >>>> Loss: 5.85763\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/4\n",
            "Train >>>> Loss: 5.86716\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/5\n",
            "Train >>>> Loss: 5.83446\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/6\n",
            "Train >>>> Loss: 5.88518\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/7\n",
            "Train >>>> Loss: 5.8768\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/8\n",
            "Train >>>> Loss: 5.83055\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/9\n",
            "Train >>>> Loss: 5.85603\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/10\n",
            "Train >>>> Loss: 5.87195\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/11\n",
            "Train >>>> Loss: 5.86425\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/12\n",
            "Train >>>> Loss: 5.85446\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/13\n",
            "Train >>>> Loss: 5.84688\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/14\n",
            "Train >>>> Loss: 5.8755\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/15\n",
            "Train >>>> Loss: 5.83055\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/16\n",
            "Train >>>> Loss: 5.8818\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/17\n",
            "Train >>>> Loss: 5.85226\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/18\n",
            "Train >>>> Loss: 5.90469\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/19\n",
            "Train >>>> Loss: 5.84421\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/20\n",
            "Train >>>> Loss: 5.8487\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/21\n",
            "Train >>>> Loss: 5.85297\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/22\n",
            "Train >>>> Loss: 5.84963\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/23\n",
            "Train >>>> Loss: 5.85788\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/24\n",
            "Train >>>> Loss: 5.88661\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/25\n",
            "Train >>>> Loss: 5.86815\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/26\n",
            "Train >>>> Loss: 5.88481\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/27\n",
            "Train >>>> Loss: 5.86562\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/28\n",
            "Train >>>> Loss: 5.87116\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/29\n",
            "Train >>>> Loss: 5.86702\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/30\n",
            "Train >>>> Loss: 5.81359\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/31\n",
            "Train >>>> Loss: 5.80689\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/32\n",
            "Train >>>> Loss: 5.88383\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/33\n",
            "Train >>>> Loss: 5.87883\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/34\n",
            "Train >>>> Loss: 5.89417\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 89/35\n",
            "Train >>>> Loss: 5.82683\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.86057\n",
            "[Average Testing Loss]: 5.87579\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/0\n",
            "Train >>>> Loss: 5.83444\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/1\n",
            "Train >>>> Loss: 5.8297\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/2\n",
            "Train >>>> Loss: 5.85891\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/3\n",
            "Train >>>> Loss: 5.87564\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/4\n",
            "Train >>>> Loss: 5.8641\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/5\n",
            "Train >>>> Loss: 5.86654\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/6\n",
            "Train >>>> Loss: 5.80914\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/7\n",
            "Train >>>> Loss: 5.8337\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/8\n",
            "Train >>>> Loss: 5.84413\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/9\n",
            "Train >>>> Loss: 5.83726\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/10\n",
            "Train >>>> Loss: 5.8511\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/11\n",
            "Train >>>> Loss: 5.89251\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/12\n",
            "Train >>>> Loss: 5.82534\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/13\n",
            "Train >>>> Loss: 5.88711\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/14\n",
            "Train >>>> Loss: 5.85044\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/15\n",
            "Train >>>> Loss: 5.84638\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/16\n",
            "Train >>>> Loss: 5.88725\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/17\n",
            "Train >>>> Loss: 5.85138\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/18\n",
            "Train >>>> Loss: 5.84675\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/19\n",
            "Train >>>> Loss: 5.84248\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/20\n",
            "Train >>>> Loss: 5.8534\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/21\n",
            "Train >>>> Loss: 5.87372\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/22\n",
            "Train >>>> Loss: 5.82774\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/23\n",
            "Train >>>> Loss: 5.82662\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/24\n",
            "Train >>>> Loss: 5.87633\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/25\n",
            "Train >>>> Loss: 5.86287\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/26\n",
            "Train >>>> Loss: 5.86667\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/27\n",
            "Train >>>> Loss: 5.88097\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/28\n",
            "Train >>>> Loss:  5.864\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/29\n",
            "Train >>>> Loss: 5.79126\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/30\n",
            "Train >>>> Loss: 5.87468\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/31\n",
            "Train >>>> Loss: 5.83674\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/32\n",
            "Train >>>> Loss: 5.86759\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/33\n",
            "Train >>>> Loss: 5.81784\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/34\n",
            "Train >>>> Loss: 5.87748\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 90/35\n",
            "Train >>>> Loss: 5.82247\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.85152\n",
            "[Average Testing Loss]: 5.95762\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/0\n",
            "Train >>>> Loss: 5.87225\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/1\n",
            "Train >>>> Loss: 5.86026\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/2\n",
            "Train >>>> Loss: 5.82779\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/3\n",
            "Train >>>> Loss: 5.8286\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/4\n",
            "Train >>>> Loss:  5.822\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/5\n",
            "Train >>>> Loss: 5.88368\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/6\n",
            "Train >>>> Loss: 5.87121\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/7\n",
            "Train >>>> Loss: 5.82489\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/8\n",
            "Train >>>> Loss: 5.81288\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/9\n",
            "Train >>>> Loss: 5.84735\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/10\n",
            "Train >>>> Loss: 5.81552\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/11\n",
            "Train >>>> Loss: 5.87033\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/12\n",
            "Train >>>> Loss: 5.86845\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/13\n",
            "Train >>>> Loss: 5.81941\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/14\n",
            "Train >>>> Loss: 5.85685\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/15\n",
            "Train >>>> Loss: 5.85873\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/16\n",
            "Train >>>> Loss: 5.86917\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/17\n",
            "Train >>>> Loss: 5.86968\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/18\n",
            "Train >>>> Loss: 5.83037\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/19\n",
            "Train >>>> Loss: 5.82963\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/20\n",
            "Train >>>> Loss: 5.84138\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/21\n",
            "Train >>>> Loss: 5.84069\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/22\n",
            "Train >>>> Loss: 5.82511\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/23\n",
            "Train >>>> Loss: 5.81705\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/24\n",
            "Train >>>> Loss: 5.83375\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/25\n",
            "Train >>>> Loss: 5.85357\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/26\n",
            "Train >>>> Loss: 5.8673\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/27\n",
            "Train >>>> Loss: 5.82944\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/28\n",
            "Train >>>> Loss: 5.83691\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/29\n",
            "Train >>>> Loss: 5.80976\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/30\n",
            "Train >>>> Loss: 5.80905\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/31\n",
            "Train >>>> Loss: 5.80096\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/32\n",
            "Train >>>> Loss: 5.83792\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/33\n",
            "Train >>>> Loss: 5.88059\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/34\n",
            "Train >>>> Loss: 5.79995\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 91/35\n",
            "Train >>>> Loss: 5.87955\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.84172\n",
            "[Average Testing Loss]: 5.87977\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/0\n",
            "Train >>>> Loss: 5.83685\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/1\n",
            "Train >>>> Loss: 5.82575\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/2\n",
            "Train >>>> Loss: 5.80078\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/3\n",
            "Train >>>> Loss: 5.88963\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/4\n",
            "Train >>>> Loss: 5.85702\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/5\n",
            "Train >>>> Loss: 5.83194\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/6\n",
            "Train >>>> Loss: 5.84945\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/7\n",
            "Train >>>> Loss: 5.90297\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/8\n",
            "Train >>>> Loss: 5.85389\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/9\n",
            "Train >>>> Loss: 5.88271\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/10\n",
            "Train >>>> Loss: 5.87769\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/11\n",
            "Train >>>> Loss: 5.87902\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/12\n",
            "Train >>>> Loss:   5.87\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/13\n",
            "Train >>>> Loss: 5.8207\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/14\n",
            "Train >>>> Loss: 5.86333\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/15\n",
            "Train >>>> Loss: 5.94006\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/16\n",
            "Train >>>> Loss: 5.84309\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/17\n",
            "Train >>>> Loss: 5.85252\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/18\n",
            "Train >>>> Loss: 5.84463\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/19\n",
            "Train >>>> Loss: 5.86485\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/20\n",
            "Train >>>> Loss: 5.84094\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/21\n",
            "Train >>>> Loss: 5.87823\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/22\n",
            "Train >>>> Loss: 5.84872\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/23\n",
            "Train >>>> Loss: 5.82051\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/24\n",
            "Train >>>> Loss: 5.84034\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/25\n",
            "Train >>>> Loss: 5.81468\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/26\n",
            "Train >>>> Loss: 5.86443\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/27\n",
            "Train >>>> Loss:  5.846\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/28\n",
            "Train >>>> Loss: 5.86113\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/29\n",
            "Train >>>> Loss: 5.84106\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/30\n",
            "Train >>>> Loss: 5.87488\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/31\n",
            "Train >>>> Loss: 5.85805\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/32\n",
            "Train >>>> Loss: 5.87174\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/33\n",
            "Train >>>> Loss: 5.89312\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/34\n",
            "Train >>>> Loss: 5.87344\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 92/35\n",
            "Train >>>> Loss: 5.84794\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.85728\n",
            "[Average Testing Loss]: 5.86864\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/0\n",
            "Train >>>> Loss: 5.84078\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/1\n",
            "Train >>>> Loss: 5.83866\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/2\n",
            "Train >>>> Loss: 5.84889\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/3\n",
            "Train >>>> Loss: 5.89993\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/4\n",
            "Train >>>> Loss: 5.86527\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/5\n",
            "Train >>>> Loss: 5.85794\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/6\n",
            "Train >>>> Loss: 5.82493\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/7\n",
            "Train >>>> Loss: 5.83409\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/8\n",
            "Train >>>> Loss: 5.8492\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/9\n",
            "Train >>>> Loss: 5.81097\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/10\n",
            "Train >>>> Loss: 5.86027\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/11\n",
            "Train >>>> Loss: 5.83816\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/12\n",
            "Train >>>> Loss: 5.82817\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/13\n",
            "Train >>>> Loss: 5.84652\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/14\n",
            "Train >>>> Loss: 5.82994\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/15\n",
            "Train >>>> Loss: 5.87405\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/16\n",
            "Train >>>> Loss: 5.91287\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/17\n",
            "Train >>>> Loss: 5.83255\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/18\n",
            "Train >>>> Loss: 5.84561\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/19\n",
            "Train >>>> Loss: 5.82644\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/20\n",
            "Train >>>> Loss: 5.85214\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/21\n",
            "Train >>>> Loss: 5.83753\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/22\n",
            "Train >>>> Loss: 5.84392\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/23\n",
            "Train >>>> Loss: 5.81141\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/24\n",
            "Train >>>> Loss: 5.84235\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/25\n",
            "Train >>>> Loss: 5.85187\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/26\n",
            "Train >>>> Loss: 5.8134\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/27\n",
            "Train >>>> Loss: 5.8381\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/28\n",
            "Train >>>> Loss: 5.87411\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/29\n",
            "Train >>>> Loss: 5.82668\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/30\n",
            "Train >>>> Loss: 5.81204\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/31\n",
            "Train >>>> Loss: 5.87323\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/32\n",
            "Train >>>> Loss: 5.87809\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/33\n",
            "Train >>>> Loss: 5.87697\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/34\n",
            "Train >>>> Loss: 5.87167\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 93/35\n",
            "Train >>>> Loss: 5.84253\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.84754\n",
            "[Average Testing Loss]: 5.89231\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/0\n",
            "Train >>>> Loss: 5.83088\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/1\n",
            "Train >>>> Loss: 5.88848\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/2\n",
            "Train >>>> Loss: 5.8565\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/3\n",
            "Train >>>> Loss: 5.87911\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/4\n",
            "Train >>>> Loss: 5.80551\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/5\n",
            "Train >>>> Loss: 5.87507\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/6\n",
            "Train >>>> Loss: 5.84022\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/7\n",
            "Train >>>> Loss: 5.80933\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/8\n",
            "Train >>>> Loss: 5.86509\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/9\n",
            "Train >>>> Loss: 5.85806\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/10\n",
            "Train >>>> Loss: 5.84857\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/11\n",
            "Train >>>> Loss: 5.83487\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/12\n",
            "Train >>>> Loss: 5.82777\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/13\n",
            "Train >>>> Loss: 5.85028\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/14\n",
            "Train >>>> Loss: 5.83784\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/15\n",
            "Train >>>> Loss: 5.85847\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/16\n",
            "Train >>>> Loss: 5.90528\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/17\n",
            "Train >>>> Loss: 5.82762\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/18\n",
            "Train >>>> Loss: 5.85281\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/19\n",
            "Train >>>> Loss: 5.80695\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/20\n",
            "Train >>>> Loss: 5.8487\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/21\n",
            "Train >>>> Loss: 5.85754\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/22\n",
            "Train >>>> Loss: 5.83077\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/23\n",
            "Train >>>> Loss: 5.83535\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/24\n",
            "Train >>>> Loss: 5.85765\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/25\n",
            "Train >>>> Loss: 5.84145\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/26\n",
            "Train >>>> Loss: 5.87707\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/27\n",
            "Train >>>> Loss: 5.82566\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/28\n",
            "Train >>>> Loss: 5.8643\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/29\n",
            "Train >>>> Loss: 5.89548\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/30\n",
            "Train >>>> Loss: 5.83596\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/31\n",
            "Train >>>> Loss: 5.82518\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/32\n",
            "Train >>>> Loss: 5.87037\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/33\n",
            "Train >>>> Loss: 5.87021\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/34\n",
            "Train >>>> Loss: 5.83237\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 94/35\n",
            "Train >>>> Loss: 5.82689\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.84871\n",
            "[Average Testing Loss]: 5.9191\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/0\n",
            "Train >>>> Loss: 5.88237\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/1\n",
            "Train >>>> Loss: 5.87334\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/2\n",
            "Train >>>> Loss: 5.93297\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/3\n",
            "Train >>>> Loss: 5.84435\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/4\n",
            "Train >>>> Loss: 5.91151\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/5\n",
            "Train >>>> Loss: 5.86231\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/6\n",
            "Train >>>> Loss: 5.85493\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/7\n",
            "Train >>>> Loss: 5.82649\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/8\n",
            "Train >>>> Loss: 5.81267\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/9\n",
            "Train >>>> Loss: 5.84009\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/10\n",
            "Train >>>> Loss: 5.81139\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/11\n",
            "Train >>>> Loss: 5.83418\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/12\n",
            "Train >>>> Loss: 5.84353\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/13\n",
            "Train >>>> Loss: 5.82112\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/14\n",
            "Train >>>> Loss: 5.85487\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/15\n",
            "Train >>>> Loss: 5.87382\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/16\n",
            "Train >>>> Loss: 5.82563\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/17\n",
            "Train >>>> Loss: 5.83904\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/18\n",
            "Train >>>> Loss: 5.82793\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/19\n",
            "Train >>>> Loss: 5.82923\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/20\n",
            "Train >>>> Loss: 5.84524\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/21\n",
            "Train >>>> Loss: 5.82822\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/22\n",
            "Train >>>> Loss: 5.7921\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/23\n",
            "Train >>>> Loss: 5.85573\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/24\n",
            "Train >>>> Loss: 5.88071\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/25\n",
            "Train >>>> Loss: 5.86542\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/26\n",
            "Train >>>> Loss: 5.84006\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/27\n",
            "Train >>>> Loss: 5.85288\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/28\n",
            "Train >>>> Loss: 5.81466\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/29\n",
            "Train >>>> Loss: 5.89488\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/30\n",
            "Train >>>> Loss: 5.85392\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/31\n",
            "Train >>>> Loss: 5.83353\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/32\n",
            "Train >>>> Loss: 5.8454\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/33\n",
            "Train >>>> Loss: 5.82444\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/34\n",
            "Train >>>> Loss: 5.80486\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 95/35\n",
            "Train >>>> Loss: 5.82564\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.8461\n",
            "[Average Testing Loss]: 5.97285\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/0\n",
            "Train >>>> Loss: 5.84148\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/1\n",
            "Train >>>> Loss: 5.81046\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/2\n",
            "Train >>>> Loss: 5.78703\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/3\n",
            "Train >>>> Loss: 5.85905\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/4\n",
            "Train >>>> Loss: 5.78712\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/5\n",
            "Train >>>> Loss: 5.88667\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/6\n",
            "Train >>>> Loss: 5.87071\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/7\n",
            "Train >>>> Loss: 5.84684\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/8\n",
            "Train >>>> Loss: 5.84924\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/9\n",
            "Train >>>> Loss: 5.89798\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/10\n",
            "Train >>>> Loss: 5.84833\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/11\n",
            "Train >>>> Loss: 5.8371\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/12\n",
            "Train >>>> Loss: 5.84794\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/13\n",
            "Train >>>> Loss: 5.8721\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/14\n",
            "Train >>>> Loss: 5.86105\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/15\n",
            "Train >>>> Loss: 5.8416\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/16\n",
            "Train >>>> Loss: 5.84332\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/17\n",
            "Train >>>> Loss: 5.86476\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/18\n",
            "Train >>>> Loss: 5.86903\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/19\n",
            "Train >>>> Loss: 5.84808\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/20\n",
            "Train >>>> Loss: 5.87999\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/21\n",
            "Train >>>> Loss: 5.86622\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/22\n",
            "Train >>>> Loss: 5.84212\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/23\n",
            "Train >>>> Loss: 5.82402\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/24\n",
            "Train >>>> Loss: 5.84363\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/25\n",
            "Train >>>> Loss: 5.7961\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/26\n",
            "Train >>>> Loss: 5.87965\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/27\n",
            "Train >>>> Loss: 5.85801\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/28\n",
            "Train >>>> Loss: 5.84448\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/29\n",
            "Train >>>> Loss: 5.80997\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/30\n",
            "Train >>>> Loss: 5.81824\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/31\n",
            "Train >>>> Loss: 5.84573\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/32\n",
            "Train >>>> Loss: 5.82558\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/33\n",
            "Train >>>> Loss: 5.85387\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/34\n",
            "Train >>>> Loss: 5.84619\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 96/35\n",
            "Train >>>> Loss: 5.82691\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.84529\n",
            "[Average Testing Loss]: 6.02196\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/0\n",
            "Train >>>> Loss: 5.85913\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/1\n",
            "Train >>>> Loss: 5.87595\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/2\n",
            "Train >>>> Loss: 5.82746\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/3\n",
            "Train >>>> Loss: 5.83839\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/4\n",
            "Train >>>> Loss: 5.85718\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/5\n",
            "Train >>>> Loss: 5.82632\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/6\n",
            "Train >>>> Loss: 5.83656\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/7\n",
            "Train >>>> Loss: 5.80192\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/8\n",
            "Train >>>> Loss: 5.82652\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/9\n",
            "Train >>>> Loss: 5.84831\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/10\n",
            "Train >>>> Loss: 5.81781\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/11\n",
            "Train >>>> Loss: 5.80836\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/12\n",
            "Train >>>> Loss: 5.85282\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/13\n",
            "Train >>>> Loss: 5.80501\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/14\n",
            "Train >>>> Loss: 5.84656\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/15\n",
            "Train >>>> Loss: 5.89905\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/16\n",
            "Train >>>> Loss: 5.8881\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/17\n",
            "Train >>>> Loss: 5.86691\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/18\n",
            "Train >>>> Loss:  5.841\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/19\n",
            "Train >>>> Loss: 5.85599\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/20\n",
            "Train >>>> Loss: 5.83855\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/21\n",
            "Train >>>> Loss: 5.9058\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/22\n",
            "Train >>>> Loss: 5.82028\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/23\n",
            "Train >>>> Loss: 5.86277\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/24\n",
            "Train >>>> Loss: 5.88813\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/25\n",
            "Train >>>> Loss: 5.83502\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/26\n",
            "Train >>>> Loss: 5.81142\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/27\n",
            "Train >>>> Loss: 5.84081\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/28\n",
            "Train >>>> Loss: 5.84162\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/29\n",
            "Train >>>> Loss: 5.84948\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/30\n",
            "Train >>>> Loss: 5.82833\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/31\n",
            "Train >>>> Loss: 5.88474\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/32\n",
            "Train >>>> Loss: 5.81887\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/33\n",
            "Train >>>> Loss: 5.81027\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/34\n",
            "Train >>>> Loss: 5.81738\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 97/35\n",
            "Train >>>> Loss: 5.84095\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.84372\n",
            "[Average Testing Loss]: 6.03145\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/0\n",
            "Train >>>> Loss: 5.83193\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/1\n",
            "Train >>>> Loss: 5.86427\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/2\n",
            "Train >>>> Loss: 5.86134\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/3\n",
            "Train >>>> Loss: 5.81191\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/4\n",
            "Train >>>> Loss: 5.80524\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/5\n",
            "Train >>>> Loss: 5.81442\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/6\n",
            "Train >>>> Loss: 5.90851\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/7\n",
            "Train >>>> Loss: 5.86971\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/8\n",
            "Train >>>> Loss: 5.86869\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/9\n",
            "Train >>>> Loss: 5.8666\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/10\n",
            "Train >>>> Loss: 5.81184\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/11\n",
            "Train >>>> Loss: 5.88708\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/12\n",
            "Train >>>> Loss: 5.84291\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/13\n",
            "Train >>>> Loss: 5.84644\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/14\n",
            "Train >>>> Loss: 5.84994\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/15\n",
            "Train >>>> Loss: 5.82764\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/16\n",
            "Train >>>> Loss: 5.86013\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/17\n",
            "Train >>>> Loss: 5.87055\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/18\n",
            "Train >>>> Loss: 5.84867\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/19\n",
            "Train >>>> Loss: 5.8212\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/20\n",
            "Train >>>> Loss: 5.89589\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/21\n",
            "Train >>>> Loss: 5.8552\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/22\n",
            "Train >>>> Loss: 5.8235\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/23\n",
            "Train >>>> Loss: 5.86023\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/24\n",
            "Train >>>> Loss: 5.82875\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/25\n",
            "Train >>>> Loss: 5.84604\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/26\n",
            "Train >>>> Loss: 5.83817\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/27\n",
            "Train >>>> Loss: 5.84237\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/28\n",
            "Train >>>> Loss: 5.8237\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/29\n",
            "Train >>>> Loss: 5.82942\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/30\n",
            "Train >>>> Loss: 5.84283\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/31\n",
            "Train >>>> Loss: 5.86736\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/32\n",
            "Train >>>> Loss: 5.87957\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/33\n",
            "Train >>>> Loss: 5.91806\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/34\n",
            "Train >>>> Loss: 5.90445\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 98/35\n",
            "Train >>>> Loss: 5.87993\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.8529\n",
            "[Average Testing Loss]: 5.91116\n",
            "\n",
            "**************************************************\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/0\n",
            "Train >>>> Loss: 5.86651\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/1\n",
            "Train >>>> Loss: 5.92383\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/2\n",
            "Train >>>> Loss: 5.87642\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/3\n",
            "Train >>>> Loss: 5.90632\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/4\n",
            "Train >>>> Loss: 5.84519\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/5\n",
            "Train >>>> Loss: 5.86687\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/6\n",
            "Train >>>> Loss: 5.8882\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/7\n",
            "Train >>>> Loss: 5.90175\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/8\n",
            "Train >>>> Loss: 5.9032\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/9\n",
            "Train >>>> Loss: 5.86016\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/10\n",
            "Train >>>> Loss: 5.92098\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/11\n",
            "Train >>>> Loss: 5.84798\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/12\n",
            "Train >>>> Loss: 5.89375\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/13\n",
            "Train >>>> Loss: 5.86441\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/14\n",
            "Train >>>> Loss: 5.85873\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/15\n",
            "Train >>>> Loss: 5.89247\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/16\n",
            "Train >>>> Loss: 5.88961\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/17\n",
            "Train >>>> Loss: 5.87105\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/18\n",
            "Train >>>> Loss: 5.91237\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/19\n",
            "Train >>>> Loss: 5.90149\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/20\n",
            "Train >>>> Loss: 5.86077\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/21\n",
            "Train >>>> Loss: 5.8848\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/22\n",
            "Train >>>> Loss: 5.87674\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/23\n",
            "Train >>>> Loss: 5.95351\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/24\n",
            "Train >>>> Loss: 5.90936\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/25\n",
            "Train >>>> Loss: 5.89348\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/26\n",
            "Train >>>> Loss: 5.90599\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/27\n",
            "Train >>>> Loss:  5.906\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/28\n",
            "Train >>>> Loss: 5.88456\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/29\n",
            "Train >>>> Loss: 5.87259\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/30\n",
            "Train >>>> Loss: 5.84028\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/31\n",
            "Train >>>> Loss: 5.87913\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/32\n",
            "Train >>>> Loss: 5.92929\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/33\n",
            "Train >>>> Loss: 5.88401\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/34\n",
            "Train >>>> Loss: 5.88173\n",
            "\n",
            "====================================================\n",
            "Epoch/Batch: 99/35\n",
            "Train >>>> Loss: 5.85917\n",
            "\n",
            "**************************************************\n",
            "\n",
            "*** Test *** \n",
            "[Average Train Loss]: 5.88646\n",
            "[Average Testing Loss]: 5.87736\n",
            "\n",
            "**************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTqvzYhl05eK",
        "colab_type": "code",
        "outputId": "b6526adf-0b64-4ba0-ba27-b798f1c7fc16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(total_valid_loss, label='train loss')\n",
        "plt.plot(total_train_loss, label='validation loss')\n",
        "plt.legend()\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "img_path = os.path.join('/gdrive/My Drive/my_data/library/checkpoints/', \"encoded_midi_6-1.png\")\n",
        "plt.savefig(img_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdd3hUVfrA8e+ZyaT3CkmAhJ4EAoHQ\nexER7KJgx7LYWetPdtfd1d3Vta9iQ0SxgBUriljpTTqEXgKkkJDee87vjzupJCEBJgHm/TxPnpC5\nd+49Kcw755z3vEdprRFCCGG/TG3dACGEEG1LAoEQQtg5CQRCCGHnJBAIIYSdk0AghBB2zqGtG9BS\n/v7+OiwsrK2bIYQQ55XNmzena60DGjp23gWCsLAwNm3a1NbNEEKI84pS6mhjx2w6NKSU8lZKLVJK\n7VVK7VFKDal3fLRSKkcptc368Q9btkcIIcTJbN0jeBVYqrWeopRyBFwbOGeV1vpSG7dDCCFEI2wW\nCJRSXsBIYDqA1roUKLXV/YQQQpweW/YIwoE0YL5Sqg+wGfiz1rqg3nlDlFLbgWTgUa31rvoXUkrN\nAGYAdOzY0YZNFkI0pKysjMTERIqLi9u6KeIUnJ2dCQ0NxWKxNPs5yla1hpRSscB6YJjWeoNS6lUg\nV2v991rneAKVWut8pdQk4FWtdbemrhsbG6tlsliI1hUfH4+Hhwd+fn4opdq6OaIRWmsyMjLIy8sj\nPDy8zjGl1GatdWxDz7PlZHEikKi13mD9ehHQr/YJWutcrXW+9d9LAItSyt+GbRJCnIbi4mIJAucB\npRR+fn4t7rnZLBBorVOABKVUD+tD44Ddtc9RSrVT1r8spdRAa3sybNUmIcTpkyBwfjid35Ots4Ye\nABZaM4YOA7cppe4G0FrPAaYA9yilyoEiYJq21VhV6m6I+xIG3wtufja5hRBCnI9suo5Aa71Nax2r\ntY7WWl+ptc7SWs+xBgG01q9rraO01n201oO11mtt1piMg7DqRchLttkthBC2kZ2dzZtvvnlaz500\naRLZ2dnNPv/JJ5/kxRdfPK17na/sp9aQs6fxuTi3bdshhGixpgJBeXl5k89dsmQJ3t7etmjWBcOO\nAoGX8bk4p23bIYRosVmzZnHo0CH69u3LY489xvLlyxkxYgSXX345kZGRAFx55ZX079+fqKgo5s6d\nW/3csLAw0tPTOXLkCBEREfzpT38iKiqKCRMmUFRU1OR9t23bxuDBg4mOjuaqq64iKysLgNmzZxMZ\nGUl0dDTTpk0DYMWKFfTt25e+ffsSExNDXl6ejX4aZ995V2votDlZewQl0iMQ4kw8tXgXu5PP7v+j\nyGBP/nlZVKPHn332WeLi4ti2bRsAy5cvZ8uWLcTFxVWnSb733nv4+vpSVFTEgAEDuOaaa/Dzqzsf\neODAAT755BPeeecdrrvuOr788ktuuummRu97yy238NprrzFq1Cj+8Y9/8NRTT/HKK6/w7LPPEh8f\nj5OTU/Ww04svvsgbb7zBsGHDyM/Px9nZ+Ux/LK3GjnoE1q6h9AiEuCAMHDiwTq787Nmz6dOnD4MH\nDyYhIYEDBw6c9Jzw8HD69u0LQP/+/Tly5Eij18/JySE7O5tRo0YBcOutt7Jy5UoAoqOjufHGG1mw\nYAEODsb76WHDhvHwww8ze/ZssrOzqx8/H5w/LT1TMkcgxFnR1Dv31uTm5lb97+XLl/Prr7+ybt06\nXF1dGT16dIO59E5OTtX/NpvNpxwaaswPP/zAypUrWbx4MU8//TQ7d+5k1qxZTJ48mSVLljBs2DB+\n+uknevbseVrXb2320yMwW8DiCsXNzx4QQpwbPDw8mhxzz8nJwcfHB1dXV/bu3cv69evP+J5eXl74\n+PiwatUqAD766CNGjRpFZWUlCQkJjBkzhueee46cnBzy8/M5dOgQvXv35vHHH2fAgAHs3bv3jNvQ\nWuynRwDGPIHMEQhx3vHz82PYsGH06tWLSy65hMmTJ9c5PnHiRObMmUNERAQ9evRg8ODBZ+W+H3zw\nAXfffTeFhYV07tyZ+fPnU1FRwU033UROTg5aa2bOnIm3tzd///vfWbZsGSaTiaioKC655JKz0obW\nYLNaQ7ZyRrWGXh8IgT3hug/PbqOEuMDt2bOHiIiItm6GaKaGfl9tVWvo3OPsKXMEQghRj30FAidP\nyRoSQoh67CsQOHvJHIEQQtRjZ4FAhoaEEKI+OwsEXjI0JIQQ9dhXIHDyhIoSKJPt9oQQoop9BYKq\nwnMyTyDEBc/d3R2A5ORkpkyZ0uA5o0eP5lTp6K+88gqFhYXVX7e0rHVjzqVy1/YZCGSeQAi7ERwc\nzKJFi077+fUDwYVY1tq+AkFVBVKZJxDivDJr1izeeOON6q+r3k3n5+czbtw4+vXrR+/evfn2229P\neu6RI0fo1asXAEVFRUybNo2IiAiuuuqqOrWG7rnnHmJjY4mKiuKf//wnYBSyS05OZsyYMYwZMwao\nKWsN8PLLL9OrVy969erFK6+8Un2/863ctX2VmKgeGpJAIMRp+3EWpOw8u9ds1xsuebbRw1OnTuXB\nBx/kvvvuA+Dzzz/np59+wtnZma+//hpPT0/S09MZPHgwl19+eaP79r711lu4urqyZ88eduzYQb9+\n/aqPPf300/j6+lJRUcG4cePYsWMHM2fO5OWXX2bZsmX4+/vXudbmzZuZP38+GzZsQGvNoEGDGDVq\nFD4+PudduWv76hE4S49AiPNRTEwMJ06cIDk5me3bt+Pj40OHDh3QWvPXv/6V6Ohoxo8fT1JSEqmp\nqY1eZ+XKldUvyNHR0URHR1cf+/zzz+nXrx8xMTHs2rWL3bt3N9mm1atXc9VVV+Hm5oa7uztXX311\ndYG6863ctX32CGSOQIjT18Q7d1u69tprWbRoESkpKUydOhWAhQsXkpaWxubNm7FYLISFhTVYfvpU\n4uPjefHFF9m4cSM+Pj5Mnz79tK5T5Xwrd23THoFSylsptUgptVcptUcpNaSR8wYopcqVUg1P7Z8t\nMkcgxHlr6tSpfPrppyxatIhrr70WMN5NBwYGYrFYWLZsGUePHm3yGiNHjuTjjz8GIC4ujh07dgCQ\nm5uLm5sbXl5epKam8uOPP1Y/p7ES2CNGjOCbb76hsLCQgoICvv76a0aMGNHi7+tcKHdt6x7Bq8BS\nrfUUpZQj4Fr/BKWUGXgO+NnGbQFHd1AmSR8V4jwUFRVFXl4eISEhtG/fHoAbb7yRyy67jN69exMb\nG3vKd8b33HMPt912GxEREURERNC/f38A+vTpQ0xMDD179qRDhw4MGzas+jkzZsxg4sSJBAcHs2zZ\nsurH+/Xrx/Tp0xk4cCAAd955JzExMU0OAzWmrctd26wMtVLKC9gGdNZN3EQp9SBQBgwAvtdaN5nn\ndUZlqAGe7QjR02DS86d/DSHsjJShPr+cS2Wow4E0YL5SaqtSap5Syq32CUqpEOAq4K2mLqSUmqGU\n2qSU2pSWlnZmrZIyE0IIUYctA4ED0A94S2sdAxQAs+qd8wrwuNa6sqkLaa3naq1jtdaxAQEBZ9Yq\nJ6lAKoQQtdlyjiARSNRab7B+vYiTA0Es8Kk159cfmKSUKtdaf2OzVjnLngRCnA6tdaP5+eLccTrD\n/TbrEWitU4AEpVQP60PjgN31zgnXWodprcMwAsW9Ng0CYB0akh6BEC3h7OxMRkbGab3IiNajtSYj\nI6PFi8xsnTX0ALDQmjF0GLhNKXU3gNZ6jo3v3TDZpUyIFgsNDSUxMZEznqMTNufs7ExoaGiLnmPT\nQKC13oYx/FNbgwFAaz3dlm1ZtvcETy3exXddXPGUEhNCtIjFYiE8PLytmyFsxG5KTJRWVHIko5Bi\ns7sxNFTZ5Py0EELYDbsJBK6OZgAjEKChNL9tGySEEOcIuwkELhZrIDBZlzLIPIEQQgD2FAisPYKC\nqkAgawmEEAKwp0Bg7REUKmP7OkkhFUIIg90EAldHI0Eqv6runQwNCSEEYEeBoKpHkIsMDQkhRG32\nEwiscwS52rriTnoEQggB2FEgsJgVZpMiR7sYD0ggEEIIwI4CgVIKV4uZvHIHMDtJIBBCCCu7CQQA\nzo5missqjAqkMkcghBCAnQUCV0czhaUVsjmNEELUYleBwMVipqi0wlqBVHoEQggB9hYIHM0UlUmP\nQAgharOvQGCpGhqSOQIhhKhiV4HA1dE6NCS7lAkhRDW7CgTOFuvQkOxSJoQQ1ewqENT0CLyhvAjK\nS9u6SUII0ebsKhAYcwTlxhwByDyBEEJgb4HA0YHiskpjaAhkeEgIIbBxIFBKeSulFiml9iql9iil\nhtQ7foVSaodSaptSapNSargt2+NiMVNaUUmFo4fxgAQCIYTAwcbXfxVYqrWeopRyhKrNAKr9Bnyn\ntdZKqWjgc6CnrRpTtW9xiYO70RAZGhJCCNsFAqWUFzASmA6gtS4F6szOaq1r7yDvBmhbtQeMWkMA\nxcrVGghkA3shhLDl0FA4kAbMV0ptVUrNU0q51T9JKXWVUmov8ANwe0MXUkrNsA4dbUpLSzvtBrla\nN6cpMlk7JqUSCIQQwpaBwAHoB7yltY4BCoBZ9U/SWn+tte4JXAn8u6ELaa3naq1jtdaxAQEBp92g\n6g3sse5JUJJ32tcSQogLhS0DQSKQqLXeYP16EUZgaJDWeiXQWSnlb6sGVQcCJYFACCGq2CwQaK1T\ngASlVA/rQ+OA3bXPUUp1VUop67/7AU5Ahq3aVLVvcWGFBZRZhoaEEALbZw09ACy0ZgwdBm5TSt0N\noLWeA1wD3KKUKgOKgKlaa5tNGFdlDRWVVYKTu/QIhBACGwcCrfU2ILbew3NqHX8OeM6WbaitukdQ\nVW9IsoaEEMLeVhZb00dLK8DRXdYRCCEEdhYIXB2NDlBhabkxNCRzBEIIYV+BoGpoyJgj8JA5AiGE\nwM4CgbPF+HaLSsutQ0PSIxBCCLsKBEopYwP7qsliGRoSQgj7CgRgpJAWllZI+qgQQljZXSCo2a7S\nOkdgu2ULQghxXrC7QFC9XaWjO6ChtKCtmySEEG3K7gKBi2NVj8DdeEDmCYQQds7+AoGlao6gat9i\nCQRCCPtmf4GgztAQsrpYCGH37C4QuDrWmiwGGRoSQtg9uwsEzhZrj6BqjkBSSIUQds7uAkF1j8DR\n2iOQOQIhhJ2zu0BgTBaX1xoakh6BEMK+2V8gcHSguKySSoub8YAMDQkh7Jz9BQJrBdJi5QTKJEND\nQgi7Z3eBoM52lY5SiloIIewuEFRvV1lqTSGV9FEhhJ2zv0BQtV1lmVQgFUIIsHEgUEp5K6UWKaX2\nKqX2KKWG1Dt+o1Jqh1Jqp1JqrVKqjy3bA/V6BI4SCIQQwsHG138VWKq1nqKUcgRc6x2PB0ZprbOU\nUpcAc4FBtmxQzRyBDA0JIQTYMBAopbyAkcB0AK11KVBa+xyt9dpaX64HQm3VnirOVYGganVx3nFb\n31IIIc5pthwaCgfSgPlKqa1KqXlKKbcmzr8D+LGhA0qpGUqpTUqpTWlpaWfUqLo9Ak9JHxVC2D1b\nBgIHoB/wltY6BigAZjV0olJqDEYgeLyh41rruVrrWK11bEBAwBk16qQ5AllZLISwc7YMBIlAotZ6\ng/XrRRiBoQ6lVDQwD7hCa51hw/YANVlDRbWzhmS7SiGEHbNZINBapwAJSqke1ofGAbtrn6OU6gh8\nBdystd5vq7bUVtUjKKqqN6QroayoNW4thBDnJFtnDT0ALLRmDB0GblNK3Q2gtZ4D/APwA95USgGU\na61jbdmgmkBQCZ61SlE71k9oEkII+2DTQKC13gbUf2GfU+v4ncCdtmxDfQ5mE45mE4Vl5TXbVZbm\nA0Gt2QwhhDhn2N3KYjDmCYplcxohhADsNRBUb2BftTmNBAIhhP2yy0BQs0uZtUcgq4uFEHbMLgNB\nzb7F0iMQQgi7DATVPQIJBEII0bxAoJT6s1LKUxneVUptUUpNsHXjbMXF0VyzshgkEAgh7FpzewS3\na61zgQmAD3Az8KzNWmVjLhazsR+BoxugZI5ACGHXmhsIlPXzJOAjrfWuWo+dd6p7BEoZw0NSeE4I\nYceaGwg2K6V+xggEPymlPIBK2zXLtlyrAgHI5jRCCLvX3JXFdwB9gcNa60KllC9wm+2aZVsuFgdj\naAism9NIIBBC2K/m9giGAPu01tlKqZuAJ4Ac2zXLtlwcTRSWlqO1ln2LhRB2r7mB4C2g0Lqn8CPA\nIeBDm7XKxlwdHajUUFpRKXMEQgi719xAUK611sAVwOta6zcAD9s1y7acLbW2q3R0l6whIYRda24g\nyFNK/QUjbfQHpZQJsNiuWbZ18naVMjQkhLBfzQ0EU4ESjPUEKRibzL9gs1bZWJ3tKmWOQAhh55oV\nCKwv/gsBL6XUpUCx1vq8nSOo3q6yamhItqsUQtix5paYuA74A7gWuA7YoJSaYsuG2VLdHoEH6Aoo\nL27jVgkhRNto7jqCvwEDtNYnAJRSAcCvGBvSn3d8XB0ByCosrVV4Lh8sLm3YKiGEaBvNnSMwVQUB\nq4wWPPec4+9hBIK0vJJagSC3DVskhBBtp7k9gqVKqZ+AT6xfTwWW2KZJtufn5gRAen4JeMnmNEII\n+9bcyeLHgLlAtPVjrtb68VM9TynlrZRapJTaq5Tao5QaUu94T6XUOqVUiVLq0dP5Bk6Ho4MJb1eL\nEQhk32IhhJ1rbo8ArfWXwJctvP6rwFKt9RSllCPgWu94JjATuLKF1z1j/u5OpOeVyuY0Qgi712Qg\nUErlAQ3lVSpAa609m3iuFzASmI5xcilQWvsc67zDCaXU5JY1+8z5uzsaPQLPzsYDOYmt3QQhhDgn\nNDk0pLX20Fp7NvDh0VQQsAoH0oD5SqmtSql5Sim302mkUmqGUmqTUmpTWlra6VziJP7uTkYgcA8y\n1hKkHzgr1xVCiPONLTN/HIB+wFta6xigAJh1OhfSWs/VWsdqrWMDAgLOSuOMQFBqbE7j1wUyDp6V\n6wohxPnGloEgEUjUWm+wfr0IIzCcEwI8nMgvKTf2JfDrJoFACGG3bBYIrGUpEpRSPawPjQN22+p+\nLeXvXmstgV9XyD4GZbK6WAhhf5qdNXSaHgAWWjOGDgO3KaXuBtBaz1FKtQM2AZ5ApVLqQSBSa23z\n1V3+7jVrCTr4dQU0ZMVDYIStby2EEOcUmwYCrfU2ILbew3NqHa+qZNrqagJBKfh3NR5MPyCBQAhh\nd87bMhFnyt+j1upi3y7GgzJPIISwQ3YbCPzcjDmC9LwScPY00kgzDrVxq4QQovXZbSBwtpjxcHYw\negRgzRyStQRCCPtjt4EAIKBqLQHIWgIhhN2y60Dg7+FkpI8C+HeDwgwozGzbRgkhRCuz60AQUFVm\nAoy1BCDzBEIIu2PXgcDf3ZG0kwKBDA8JIeyLnQcCJ/KKrWUmfMJAmWXCWAhhd+w7EFjXEmQUlILZ\nYgQD6REIIeyMfQeCqtXFebWGh2SOQAhhZ+w8EFgXleXXyhzKOASVlW3YKiGEaF12HghqlZkAYy1B\neRHkJrVhq4QQonXZdSAI8KhVeA4kc0gIYZfsOhA4W8y4OznULCoLjDQyh3Z83rYNE0LYpfWHMxj5\n/DI2HM5o1fvadSCAWpvYA7j5w7A/w/aP4cAvbdswIYRd+WHHcW559w+OZRayIzGnVe8tgaD26mKA\n0bMgoCcs/jMUt+4vQwhhn95fE8/9n2yhd6gXjmYT6QUlp37SWWT3gSDAo1bhOQAHJ7jiTcg7Dj8/\n0XYNE0LYhd3JuTy5eDfjI4JYeOcg/NwdSc8rPfUTzyK7DwQn9QgAQvvD0Adgy4ew65u2aZgQwi5s\nPGIUunzy8iicLWb83Z3IkB5B6/J3dyK7sIzS8nprB0b/FUJi4YvpsPY10LpN2ieEuLBtS8gmwMOJ\nYC9noN68ZSuRQOBhLCo7KQJbnOHWxRB5uTFE9N0DUN663TUhxIVv67Es+nbwRikFgJ+7Exn5F9DQ\nkFLKWym1SCm1Vym1Ryk1pN5xpZSarZQ6qJTaoZTqZ8v2NKSmzEQDP3hHV5jyPox8DLZ+BAuvgaLs\n1m2gEOKClVVQypGMQmI6elc/5m8NBLoVRyFs3SN4FViqte4J9AH21Dt+CdDN+jEDeMvG7TlJVSA4\nnlPU8AkmE4x9Aq6cA0fXwnsTISexFVsohLhQbUs03lj27VA7EDhSWlFJblF5q7XDZoFAKeUFjATe\nBdBal2qt67+dvgL4UBvWA95Kqfa2alNDItp74OnswLfbk5s+se/1cNOXRvmJd8ZB6u7WaaAQ4oK1\n7Vg2SkF0aN0eAdCqKaS27BGEA2nAfKXUVqXUPKWUW71zQoCEWl8nWh+rQyk1Qym1SSm1KS0t7aw2\n0tXRgesHdmRpXApJ2Y30Cqp0Hg23L4XKMvj1ybPaDiGE/dmWkE33QA/cnRyqH/OrKoaZd2EEAgeg\nH/CW1joGKABmnc6FtNZztdaxWuvYgICAs9lGAG4ZGgbAh2uPnPrkoCjofS3Er4DSwrPeFiGEfdBa\nsy0hu878ANT0CDIKWm/C2JaBIBFI1FpvsH69CCMw1JYEdKj1daj1sVYV4u3CxKh2fPLHMQpKmjEu\n120ClBfDkVW2b5wQ4oIUn15ATlFZnfkBaKAqciuwWSDQWqcACUqpHtaHxgH1B9a/A26xZg8NBnK0\n1sdt1aam3D48nNzicr7c0oyJ4LDhYHGD/Utt3zAhxAVpW4J1orhej8DH1YJSF87QEMADwEKl1A6g\nL/CMUupupdTd1uNLgMPAQeAd4F4bt6dR/Tp606eDN/PXHKGy8hRpWw5O0GUM7P9ZFpoJIU7LtoRs\n3BzNdAv0qPO4g9mEr6sj6RfI0BBa623Wsf1orfWVWussrfUcrfUc63Gttb5Pa91Fa91ba73Jlu1p\nilKKO4aHE59ewLzVh0+dw9t9IuQmQuqu1mmgEOKCsvVYNtGh3phN6qRjRr2hC6dHcF65pFc7xkcE\n8sySvTz42bam5wu6TTA+y/CQEKKFissq2HM896RhoSpGvaELpEdwvrGYTcy9OZZHJ3Rn8fZkrnxj\nDWsOpjc8VOQRBMExsP+n1m+oEOK8tis5h/JKfdJEcZUGi2HakASCekwmxf1ju/HRHYPIKizjxnkb\nGPPSct5afujkHkL3iZC4EQpadzchIcT5bad145k+oQ0HAj93x1atNySBoBHDuvqz+vExvDK1L0Ge\nzjy3dC9//yau7kndJgAaDspuZkKI5tuZlIu/uxNBnk4NHvd3dyK/pJzisopWaY8EgiY4W8xcGRPC\n53cN4fZh4Xy7PZnErFqLyNr3BfcgGR4SQrTIruQceoV4Vlccrc/furo4rZUmjCUQNNOdI8JRwLxV\n8TUPmkzQeYyxyriystHnCiEuPHFJOfy6O7XFzysuq+DAiXx6h3g1ek5rry6WQNBMwd4uXNE3hM82\nJpBZ+5fTeRQUZsAJKUInhD159bcDzPpqR4uft+d4LhWVmqjgUweC1kohlUDQAneP6kxRWQUf1K5J\nFD7S+By/ok3aJIRoGwmZhaTnl5LfnLI0tcQlGRPFvUMbDwRVhedaa8tKCQQt0C3Ig/ERQXyw7giF\npdZfvlco+HaBwxIIhLAXWmsSMo35wmMZLSs+GZeUi4+rpXpryobU1BuSoaFz0j2jO5NdWMbnG2tV\nz+48Co6ugYqytmuYEKLVZBWWUVBqZPQcyyxo0XN3JuXQK8Sr0YliMBJV3J0cZLL4XNW/ky/dg9xZ\nvr/Wvgjho6A0H5K3tl3DhBCtpqo3AHC0BT2CkvIK9qfm0auJieIq/u6OMll8LusV4sXu5NyaB8JG\nGJ9leEgIu5BQK438aGbzA8G+lDzKK3WTGUNV/NydZLL4XBbZ3pMTeSU13TY3P2jXWyaMhbATCZnG\nboadA9xaNEcQl2S8gezVRMZQFaNHIIHgnFWV9rX7eK1eQfgoSPgDyk6x3aUQ4ryXkFWIj6uFqGAv\njrWgR7AzKQdPZwc6+Lqc8lyj3pAMDZ2zItt7AtQdHgofBRUlkLChkWcJIS4UCZmFdPB1pZOvK0nZ\nRZRVNG9BqbGiuOmJ4ip+7k5kFZZS3sxrnwkJBKfBy9VCiLcLu5Jzah7sNBRMDjJPIIQdSMgspIOP\nKx39XKmo1CRnn3okoLS8kr3H85o1PwAQ4O6I1pBZaPtegQSC0xQV7Fl3aMjJHToMhk3vwqHf265h\nQgibqqjUJGUXEerrQidfV6B5mUMHTuRRWlHZrIwhMHoEAOl5EgjOWZHBnsSnF9QsLAO44nXwDIEF\n18Ca2TXbWBbnQEl+2zRUCHFWpeYWU1ah6ejrSic/N6B5mUOrD6QDNLoHQX019YZsP2HsYPM7XKAi\n23uiNew5nkf/Tj7Gg77hcMcv8M098Mvfjd5BYSaU5Bqb3Y96DAbfa+x5DMYCtJI8cPVtu29ECNEi\nVWsIOvi4EujhhJODiWMZTS8q01rz2cYEBoT50MHaiziVqgqkrbFBjQSC0xQVUpM5VB0IwBgiuu5D\nWP8mHFljlKDwCoVj6+DXJ2HLRxBzIyRugvhVUFYA/W+DMX8z0lCFEOe0hCxjPqCDrysmk6KDr+sp\nh4Y2xGdyOL2A+8Z0bfZ9/D2MN4zJ2cWn39hmsmkgUEodAfKACqBcax1b77gP8B7QBSgGbtdax9W/\nzrko2MsZLxdL3cyhKkrBkPuMjyrDZsKBX2Hp4/Dbv8C7E0RfCyjY/D7ELYJRs2DAneDg2FrfhhCi\nhY5lFqIUBHsbtYI6+bqeMoX00z+O4eHswKTe7Zt9H09nCz3bebBiX1qLAsjpaI0ewRitdXojx/4K\nbNNaX6WU6gm8AYxrhTadMaWUMWFcO3PoVLqNh87roSAdPGv9QQycAT/9xfj4Yy6MfxIirzACihDi\nnJKYWUg7T2ecHMwAdPRzZd3hDLTWDaaFZheWsiQuhWkDOuDiaG7RvSZEteP13w+Qnl9SPWdgC209\nWRwJ/A6gtd4LhCmlgtq2Sc0X2d6TvSl5LcvzNVvqBgGAwJ5w01dw4yKwuMAXt8K88bB1ARRlG+do\nDZnxsOd7KMpq/v0KMyE/re5jZcVwaBkcWV0zod2c6+xcJBvwCLuXkFVYZ5y/k68rhaUVjS7++mpL\nEqXllUwb0LHF95oQGUSlhovL19sAACAASURBVN/3nDjt9jaHrXsEGvhZKaWBt7XWc+sd3w5cDaxS\nSg0EOgGhQJ1tf5RSM4AZAB07tvyHaSuRwZ6UlFdyOL2A7kEeZ3YxpaDbRdBlLGxbCKtegm/vg+8f\ngtCBkHkI8o4b57r4wKjHIfYOyE81ehHbP4XACBg9y1jTUJwLq/9nzFWUF4N3Rwjpb0xOH1kD5da8\n58AoGHIv9JoClkbK4pYVwcJrIWmTsXr6kuekt1JLYWk55ZUaT2dLWzdFtIKEzCKGdfWv/roqc+hY\nZgEBHnXftWut+XTjMfp08CYy2LPF94oK9iTE24Wfd6dw3YAOZ9bwJtg6EAzXWicppQKBX5RSe7XW\nK2sdfxZ4VSm1DdgJbMWYT6jDGkDmAsTGxjbzLaztVZeaSM4980BQxWSGfrdAzM2QvAXivjJqGHUa\nBp2GgE84rJ0NS2fBmlch/wSgodvFkLQZ5l8CHYdC+n4oTIfoqUYdpMRNkLjZyFjqdwt0HWcMUa17\nwwg4i/9sTGp7d4LgGBj2ZyObSWv45l4jCHQdD3+8De4BMPIx41j8CtjxBfiEQUgMBPWGokyj95Kb\nZKTTto+mxLUdTpYLMzfhsUU7OJxWwJKZw5u1YlScv0rKK0jNK65TIqKjX81agv6d6mYAbjmWxf7U\nfJ69uvdp3U8pxYSoIBZuOEZBSTluTrb5P2TT/5la6yTr5xNKqa+BgcDKWsdzgdsAlPE/KB44bMs2\nnU2dA9xwdDDx064U2nk5097LmVAfV8ymui8Gqw+k88eRTB4a3635LxRKGe/gQ/qffKzLWDj4G6x/\nA3pPMeYYvDtCaaEx8bz+LaN3cNG/IKRf0/fpewMcXm68oGcfg6yjRqDZ8iGM+zvkHoddXxnzFkP/\nDN/cDb//x7hXwgZjHwYnT6OnQeMxuhAPtgRdypBbnzn9dNmKclj5PKTugsteBbead2WkH4QjK6Hf\ndGMv6VZSWalZczCd7MIydiXnNnuxkDg/JWUVobWROlol1McFpRpeVDZ/zRE8nBy4rE/wad9zQmQ7\n5q85wuo9SVzct9NpX6cpNgsESik3wKS1zrP+ewLwr3rneAOFWutS4E5gpTU4nBcsZhMDwnz4MS6F\nH+NSABjVPYD3bxtQ/YJfVlHJ41/uICm7CD83R24dGnbmN1bKmHjuNr7u446uxjDPkHtbdq0uY4yP\nKilx8OP/GcNSAH1ugGEPGude8YYxb7H6ZXBvB5e8AP1vNYafkrcZeze7BRg9F68QyD5G/M61xK37\nkckpn1L68vc4jvk/8O0MaXspTtlDjnYjzbsPSe7R9OwRQSd/95PbmXscvrzDCDwmB3hnDFz/mRHw\ntn0MSx6FskLIOAQXP93yn+lpOpyeT3ahsSHRN1uTzplA8NzSvbg7Odg828Te1E4dreLkYCbYy+Wk\nzKHErEJ+jEvhjuHhZ/ROfkCYDz4uZgZ+Px6y7zCGf88yW/YIgoCvrS+IDsDHWuulSqm7AbTWc4AI\n4APrHMIu4A4btscm5k8fSEJWISk5xfy+9wTvro5n+b40xvQMBOD7HckkZRfRyc+Vp5fsYXBnP3q0\nO0vDSLbSrhdM/wHivjQ22xn3j5o5AbMFrvvAKKPRZawxuQ3GkFPnUcZHbR7tWHLQlxfKurE8JIfL\n0t5m9C9/rz6cpX3xooAg9QG9gJTfgqD/5dBtAngGQ/ZR48V97WwoLYCr3gb/bvDJDfDuRcZeEPt/\nND77dIJ1rxsL+wbceerv8+haI+hlxUNuMvSYBNHXNTz/UZQNu7817tF5dPXDm44YE/c9gjz4bnsy\nf5kUcVKPsLVprVm4/ihawx3Dw3G2tCxTRTSu6sW+Y71FYR18XThab1HZ+2uOoIDpZ/jmz8Fs4rZO\nGfgcOUG5d7hNXrRtFgi01oeBPg08PqfWv9cB3W3Vhtbg6GCiS4A7XQLcGRDmyy+7U3nx532M6h4A\nwFvLD9EjyIOP7hzIpFdXMfOTrXx7/7Cz+p+zsLScjzccY1xEEOH+bmd0re+2J/P1lkReuq4vvr2n\nGENP9VlcoOfkZl9zy9EsugS48Y87JnDFG+3pULwXV4uJ1dl+jInuzLX92tO+6CBJO5ZTfnAZgVsX\nYNr4Tt2LBEbBre8ZGVYAM5bBJ9PgwE/GYrwRjxhzFvlpsOQx8GgP7kGQuBFyEiH2dvDrYjy3orwm\nVRfA4grOXrD7G9g4Dy55FoJ6Gc/LijfmaeK+qplg73sTTHwGnL04sWcNbzt/yCCVy9piV45/sYTQ\nTt2NzDCP9uDXtdVXjidmFZFbbJQ+Wb4vjYm92rXq/S9kiZmFODqYCKw3KRzm58YPO45zPKeI9l4u\n5BaX8enGBCZHtyfY+9Qlp0/lMqfNlGkzmywDGHLGVzvZhTl710YcHUw8OL4bD3++nR/jUnB0MLE/\nNZ//Te1DoIczL0zpw23vb+TZH/fy5OVRZ+Wev+5O5Z/f7SIpu4hVB9L54PaBp3xOZaXmX9/vpqyi\nkqevqpnEKq+o5Nkle0jOKeameRv4+E+D8HY9s8VtWms2H8tiQmQQXq4W5t0ay1VvlhLo6sTcO3vV\nyr5oj3/3QQx6pj939g3m8YhM4124T5jx4eJT9526Z7BRziMvxXiXXmXKezB/Inx6Q81jymy8wI+e\nBf2nw1cz4MDPMOR+GPqAETC0hu0fw69PwTtj634Tju7QZ5oxgb/vByMb6/By8O7AzGPrKDS54eQz\ngJ5Z+wnauw321EojtLjCZbOtiweb/EHBpveMbLFx/zDuV3WovJT9Kz4mvFM4jp0GGdldRVmw7RPY\n8akxzDbiESMpAIhLMta2mE2K77Yn2T4QJG+FpX+Fofe36A1CWzt4Io8HPtnGzLFduaQZC70SMgtZ\nfTCdUG8XTPV6fTcP6cT3O45z47wNfH7XEL7ZmkR+STl3Du985g3Vmk4nlrGWKH4+WMiQs/PSUYfS\nzc0jP0fExsbqTZs2tXUzGlVRqZn4ykoqtcbD2UJ6fgnLHx2Ng9mYwHxq8S7mrznCoxO6c//Ybg1e\nY39qHm8uO8gjE3o0WpekpLyChz7bxpKdKXQPcicq2Iuvtybxy0Mj6dZEBpPWmqcW7+b9tUcA+Pyu\nIQwMN96xLt6ezAOfbOXO4eF8uP4o3QLd+fjOwXi5nn5a5MET+Yx/eQXPXxNdnf6WU1SGm6O5+mdS\n210fbWLz0WzW/2Vsg8ebJS8Vtn9iDBGFxIIywY+PwZ7FYHaCynKY9AIMaGAksjjXCBoVZeDdAbw6\nQHBfKizuPPrFdiLaezCjczYsnklFcS7/SR9FyJgZ3Dk+mse+2M7SuGQ2PhSDc3GaMa+x+n9wbK2R\n6nvxM5Bx0OjFpB+ALuOgx0TjvosfNFaXu/pBYYbRyxn5GDr9ACnv30r7gt3GeQ7O0L4PHN9h9FDa\n9zWGzkrzoMdkaB/Njl1x5KQewcfbl5+z2nH3tKtwDR/UdM8keatxzV5Xg1Otv5/ETXDgFyNgOjUw\nd7NzkZF1Vl5izN1MXVDzPZ3DdifncvO7G8goKGV8RCDzbh3Q6LnZhaW89vtBPlp3FJMJnro8iqkN\nrAn4Iz6TW97bQLi/O7lFZYT6uPDZXWfh/XvqbnhrCEcG/we/0XfjcZppykqpzfWrO1SRHsFZZjYp\nHpnQnbsXbAHgX1dE1XlBe2JyJDmFZbz4837KKzV/Hlc3k+hoRgE3zttAWl4JWxOy+eKuIQR6npzf\n/+/vd7NkZwqPXdyDGSM7k1dczpKdx3lvTTz/vTq60fa9+tsB3l97hFuGdGLJzhRe/Hkfn80YDMC8\n1fF09nfjr5MiGNbNn7s+3Mwt8//gq3uGNmvcu7yikqOZhXT2d6v+nrYcNcbQ+9Wqx+Tl0vgf8pT+\nHfhpVyorD6QxtufJawvnrjzEtoRsXpkag6NDI4HCIwiGP1j3sakL0Lu/I3/FbLL6z8QvehINDaJV\nOnowM2E0w7r6c33fmv/s81Yc4uutSSyNMzP1L+PwumcNv+1KYf5Hm/miaygAV8aE8MXmRH47ppkc\n3dt4h95ljFFSZO1sIziVWScUnb2Nrx2cjd5OfqrRExh8H3z/ICx7Go6upfzIWlwqHHhMP4B2dOf5\n/tmYEjYYPYwBdxpBoTDTGOZa/ybs+4GOJl9SLX504hi9zCvgi89AmdAdB/NrRSxr6c0/pl+BcnAy\nhtJ+e8pYvIg2/j38Ieg8xsjQ2v2t0d4jq+DGL8DR+lOrKDfauPpl6DjESCL48g74/Ga4/hMj1fhc\nUVlp/KzDhoNPJ3YkZnPzu3/g6mhmZPcANsRnUlGpT/obLymv4MO1R3nt9wPkl5Rzbf8OPHRRd9p5\nNbzeZmC4L3NvjuXODzZRWlHJU2ep18/e7wFF2LBrwUZrVSQQ2MDFUe2IDvUiObuI62LrLgIxmxQv\nXNsHk0nxyq8HKCytYMbIzvi7O3E8p4gb522gvKKSl67twz++jeOmdzfw2Ywh+LjVDNF8tSWRBeuP\ncdeoztVZIb5ujlzdL5SvtiTy6IQe+Lk7obVmwYZj7EzMxmwyUVBSznfbk7mmXyhPXhZFZ383nly8\nmzUHM3BxNLE9IZt/XxGFyaQY0yOQZ67uzaNfbGfF/hMNviiD0cP4aVcKP+1KZdm+E2QXlvH8lOjq\n73vT0Uy8XS10CWje3MXoHgH4uTmyaHPiSfect+owzyzZC0Cw116euDSyeb8Qq3VOQ7nhqBmOAl/9\nRJCnE2/d1J9+HWuC1N6UPL7fcZzvdxzHzcmBy/sEs+d4Li/9vJ++HbzZlpDNZ5uOMWNkFzYfzcLR\nbKreaGRwZz8CPZx4e+UhCkrLCfNzI6K9Bx4T/m28WO5ZbKwF6TYB3AKN9NtdXxnv+K5+B8JHGI24\n8i20V0fUyudYV9Gb33s+yajekdz/8VYu7TyA0ZcE1v3GXH2NYa/hD6PRjPvvSsb0DOSFKdFc/PwP\njPQ4zt96niBryzdclD+bi4DKp+9B+XUxhtbKCoxhnW4XGy/sPz9hXNfiZtS/8u4A3z0AH0+FGz6H\n49vgh0fhxC5jTcqkl8DBkbxrP6do3mQ8F0wjIXwa3WLHGYshvUJa9Hs66w4vg2/vBQdn8mLv4471\nffFwcefT2/qwf88ONu/PY8/xuqm/K/an8fdv4jiWWcio7gH8dVJEs5I8RnYPYO4t/Vm5P52xPQNP\neX6z7FkMoQPAw3ZDfBIIbEApxbu3DqCwtLzBSWGzSfH8NdFYzCbmrjzMO6sOE9PBm8yCUrILy/jk\nT4PpHepFe29nps/fyK3z/2DWxJ706+RDfHoBf/16J4PCfXlsQo86171jeBif/HGMhRuO8cDYrjyz\nZA/vrIrH390RpRQVlZpr+oXy3DW9MZkU1w/qyNyVh3nx53208zSK6F3TP7T6elf0Dea5pXtZuP5Y\no4HgrRWHeH7pPnxcLYztEciOpBzmLD/ENf1CMZsUm49m0b+jT7PXT1jMJq7oG8KC9UfJKiitDoCf\nbTzGf37Yw6Te7fB1c2Te6niGdvVrtF2Vlfqkcdwfd6bgbDHx/JQ+JGQW8vrvB/lqS2KdQLDqgFGO\nIzrUi0c+34a7k5nnl+7D08XCe9MHcM+CzXyw9ii3Dwtn09EseoV4Vv+OzSbF7cPDefGnffzfoh0A\nBHo48ctDo/DqOQl6TqrbyE5DjA9rexMyCtiZlMPyfWks3zcAh+LX6N87ilenxVChNT6uFj7flMDo\nHsYLjNaa134/yNAufsSG+YKDI6k5xWQUlNIr2BOlFOP7dmXOChMxIy5lZmYMV4dX4J25lY4VR7nB\ntxBTUCSM/isEWHM2wkcYpUcSN0Gf643eFRhDal/PgDcGQc4xY8jsuo8g4jLySsr5ZO0h3lp+CAof\n5lXntxkU/xnEf2Q8d8LTRqBpK3t/MOZqelyCx/qX+F774OvojeXNI4Si2eLkQOpXg2HoddDzUood\nvbl/4RYCPJ347NpABiW+D78kWysJdzB6Sx0aH0oa3SOw+nd0xrKOQsoOY02QDUkgsBFjqXnjRaJM\nJsUzV/XixkEd+W3PCX7bm0pGfinv3hpL71DjncnQLv68eUM/7vt4CzfM24CjgwkXixlPZwuv3RBz\n0hh610APRnUP4MN1R0nPL+HDdUe5dUgnnrw8qsEXYicHMw+M68ZfvtoJwD2ju+DqWPMnYTGbmDag\nA68vO0hiViGhPnXnKzYfzeKln/czuXd7Zl8fg9mk+H5HMvd/vJWfd6UwuLMfh9IK6gSX5pjSP5T3\n1sTzxrKDdA10Z39qPvPXxjOqewCvTI2hUms2H83mkc+38+OfR57UVZ+z4hBzVx7mm3uHVa/6rKzU\n/Lw7hVHdA7jcurhne0I2v+85gb6ipljY6oPpdA9y56M7BjH17XXc/r4xH/Xe9Fh83Ry5bVg4dy/Y\nzPc7jrMzMYfpw8Lq3PvuUV24Y3g4SVlFbE/M5sHPtjFn5SEen9izwe81Pr2Ax77Yzq7kXIrKjEX1\nXi4WRnUPYFxEJJN6t8fBbMIBuComlI/WHyGzoBRfN0cWrD/Ky7/sZ81B3+qx6KqJ4qp3t5f3CeGN\nZYe47+MthPu58cTNw1h3aAB3fbQZS7feDY51Ezbc+Kgt+lpAGxlZIx6BEY8QnwsfLN7Nos2J5JeU\nM6KbP49OGMjxnDH0WrCBdyc6M/LoG7DieYi5CVyatyHLWVVZaQSCruPZFPsiz2+O5LmgXwkKDoS+\nU8GvC98tXszonD9g8Uz4/iFyA4cysTyCR4JyaPf9IjA7GtlqKXFQcAKWP9v4HBMYvaxj689O4ch9\nS4zPPS89s+ucggSCNqSUoleIF71CvPjz+IYnjsdHBrHpifFsPJLJ2oMZ7EzK4f8m9iDQo+FxyjuG\nh3PLe3/w4bqj/GlEOH+dFNHku/Ep/UOZs+IQSVlF3Dok7KTj0wZ25I1lB/n0jwQevbimB5JTWMbM\nT7bS3suZ/17Tu3p89ZJe7enkt485Kw5Vj+H3r/WOuzkigz3pFeLJvNXxAFjMinE9A3nt+n7V13z9\nhhgue2019y7czBs39qO9l5Gi9/6aeJ790Rg+WrjhKH+ZFAHA9sRsUnNLuDiqpns9PiKIn3ensud4\nHpHBnhSXVbAhPpObB3fCy8XCB7cP5KZ5GxjVPaC653FRZBChPi78+/vdlFZU1t2LwspiNhHm70aY\nvxvL9p5g/pp4pg8NI6iBuZ5/freLfSl5TBvYgZ7tPOjRzpNewZ4NTpRfN8AIkN9sTWJYV3/+88Me\n3BzNbIjPJCm7iBBvF+KSc1AKItobdW16tPOgZzsPEjILmXtLfzydLUyIDKJvB2/+98sBrugb0mQq\nc35JOb/uTmVydHss0ddB72tBKZbtPcGdH27CpODS6GBuHRpWvfNWrxBNB38vnt9pZsSUf6PeHmms\ndh/zl1P85s+eikrNd9uTGOt+DK/8FCp7TObJxbvI8Iwh6N4HodYbni0He/Kv7clsm9EO8+6vMW34\nhBcsq9DxjsY8zIiHa4ZlinPgyz/BDw8bk/QT/m2Uham+cbmRsZa02dhnZPJLdY+31J7vISCiJvXZ\nRiQQnAc8nC2M7RnU6DBIbSO6+XN5n2C6B7lz35iupxySsZhNzJ4WQ2JWUYOTYCHeLozpEcinGxP4\n8/huWMwmtNbM+moHqbnFfHH3kDrF1swmxYyRnfnb13G8ufwQDiZFn2ZuzVfb3JtjOZJeQAdfV4K9\nXU6ayOsS4M7zU6J55PPtjHtpBQ+O74aHs4UnF+/mokjj5/T5pgQeuqg7zhYzP+1KxcGkGFfrZzi6\np7HW4/e9qUQGe/JHfCal5ZUM72aktAZ5OvPzQyPr/AzNJsX0oWH854c9AA0GgtoevqgH3+84zqu/\nHeCZq+rWm1m+7wQr96fxxOQI7hxx6jTDnu086RPqxacbj/HZxgQ8nC3MvaU/V7+5lm+3JXHv6K7E\nJeXS2d+tzkrWd26Jpayiks4BRtaPUorHJ/bk+nfW8+G6I8wY2fiLzFPf7eKLzYks33eCl6/ri8mk\niE8vYOanW+kR5MH7tw04KZnBbFL8aWRn/vLVTtYVRDC056VGIBh8T4O9ghN5xSjUSQXbzsQvu1N4\n6LPtPOH0ObebHPgiN5K4pARenda3Tq8XjLmdT/5IYJcOo9PwJxi6fBCP9i5ixiWDT57fcPaCaR/D\nz38zSrxkHYEp79YsrFzzihEEuk2AzfONultXv1OzK2FLbJpvrKQf9X+n90NogbYuQy3OMqUUs6+P\n4f6xza9r1KeDN5OjG8+jvnFwR9LzS/hldyrHc4q4Z8EWfowzMpZiGni3f02/UPzdndh8NIuoEK/T\nWjwX7O3C0K7+dPA9uXZTlUujg/nloVEM7eLHM0v28pevdjKimz+v3xDD9KFhZBWWsWTncbTW1UNV\ntVNhAz2c6RPqxa/WEr+rDqThaDYxKLwmzbKhn+F1Azrg5mgm3N/tlDXiO/q5csOgjny2MYH49JqV\np+UVlTz9wx7C/Fy5pYGeWGOuje3A/tR89qXm8dJ1fejX0YfYTj58vSUJrTW7knNOKnPRwde1OghU\nGdLFj5HdA3hz+SHyS8ppyKYjmXyxOZHI9p58sy2ZJ76NI6+4jBkfbsLBpHj75v4NZrQBXBUTgr+7\nE2+vPGxUyi3JMYJBA2Z8uJlb3/uDs5nKvvJAOm6OZiY5bmFNeU8eX2JsE3l5AzV/Bnc2dgZcfziD\npXHHKa3QDBo2rvFJbrODUYH3kueNoZsFU4yeQspOY9go6mojw2rCf4ysq7ljjLLyL0fCfzvAB5cZ\nw2WHVxg1sgoz65Z3r6yEX/5pZI91HQdDZ561n0tjpEcgTmlU90BCvF14bule0vNKKK/U/N/EHvyp\nkXexzhYztw8P4/ml+4g9xTvmM9XRz5V5tw7g192prD+cwcMTuuPkYGZoFz86+7uxYP1Reod4cTi9\ngNuGh5/0/HERQfzv1/2k5ZWw6kA6sWE+J71jrM/T2cJ/r4nG0dy8QPvA2G4s2pzIf5fsYfb1MThb\nzHy6MYEDJ/KZc1P/xtNgG3B532Bm/3aAKf1Dq1evXxkTwhPfxLHqQDrHc4rpFdy8ekcPje/GVW+u\nZeH6o9w1qm6voLyikie+iSPYy5lF9wzh9d8P8ubyQ6zYl8bxnCI+umNQk3vvOlvM3DYsjBd+2see\nS0YQ0UivICWnmG0Jxp4b2xNzmr2xe1O01qzcn8ZVHQoJTjpGwaB/MiEziEcv7tFgYA/ydKazvxvr\nD2dSXFZBmJ8r0aHN+BkOustY9/H1XfD+pVBZYWRwTX7JOD70AaPu1rrXjcnq8FHGYsCEjbDsGeoU\naVRmo3CkX1eoKIH4lcZq+EteMAKPjUkgEKdkNiluGNSRF34ySmf8+4pe1ZOwjblpcCdWH0jn0iZ6\nGmfT+MggxkfWDPsopbhxcCf+/f1u/vfrfsDY5KO+sT0DefmX/Xy+KYG9KXmNTurW19A7y8YEeDhx\n18gu/O/X/fT/9y+Mjwxi9YF0Bob7cnFUy/Zh8nS2sPrxsXWCx+Te7Xlq8S6eWWIMV0WFNK/ufUxH\nH4Z39eedVfHcOjSsTs/t/bVH2JuSx5yb+uPq6MBjF/egsLSC99ce4W+TIurU42/MTYM68drvB/hs\nYwJPjnrcyIdf9ZIxrm61bJ/RG3MwKT7941jLA0FxLhz8BSKuqH7BPJpRSGJWEVeHbgWg24jrmOvV\ndMLCoM5+fLM1ieLyCma2oDdN7ynGmpDPbjIW+F3/Wd2Fe32m1VklXq0w01jEV5BuDB8VpEHmYWPB\nYV6KkSU0dGar7fshgUA0y10jOzOqewBR1rTEU/F0tvDxnwa3QssaN6VfKC/8tJclO1OI6ejd4GRt\nVLAn7TydjdRHjDkWW5g5riuxYT58vyOZH+NSyCsu54nJTU/kN6Z+D8LHzZHRPQL5Zbexn1NUM3sE\nAPeP7cq0uev5bGNCdWXc4zlF/O+X/YzpEVAdqJRS/POySG4e0onOzaxn5eVqYUCYL2sOpsPlo4w1\nB2tfMwoTWhec/bYnlVAfF4Z09uO77ck8cXE47uaK5mcYLZ4Ju742rjflPXD2YtVBY2fcyNzVxt4a\npwgCYAyVffLHMcDodbVIt/Fw+1JjD5Dmrqp29TWGfc4RMkcgmsXBbKJXiNd5tfGKl6uFy6KN/9S1\ns4VqU0oxNiKQ/JJyfN0ciWzf8l2kmkMpxbCu/vz36mg2/m08qx8fQ3To2UunvCrGGM/u6Ova5Mrt\n+gaF+zIgzIc5Kw5RWl7J/tQ8rp2zjkrNSWnHSim6BLi36G9gWFd/DpzI50RuMUx8DgIjjVpPOUkU\nl1Ww+mA643oGMm1gRxxKcymfMxreGGgU/DuVvUtqgsDh5fDuxZB1hMNxG3jM/SecU7c0O+1ysHVe\nqHeIF10CGiilcSrBfY3KtecpCQTigjZjZGd6h3g1OZQzPsJY/DO8q/9Ji9BswWI2Vae7ni1jewbi\n6exQvQaluZRS3D+2G8dzivnHt3Fc8+ZaSsor+eyuwdVbMJ6JYV2MHtbaQxnGfhnXfWDUJVp0O+sP\npFBcVsm4iCD6hbjyodts3PKPGOXGP7ne+Fwl87BR86jSuoFhcQ788DAVAZEs7f0ylTd+CXnJ6Ff7\n8s/EP3Ff+QfGbnl9bzi5UQ0I9HTmzuHhPNhIGveFTgKBuKB1C/Jg8QPDmywFPLSLP/06erd44du5\nxNli5rO7hvD3yS0ruwEwsps/0aFefLoxgRAfF769b9hZ661EBnvi5WIxhofA2Evi8tmQsJ7gH29j\nouN2BnXyQH03k74VO3msdAbHxrwGKTsp+mIGyelZRobNG4Nh4RSYOwriVxlZNfmpzPN5mLs/ieO9\n5I5w52+kRt7OY2Uz+P2SZXDPaqNKbTM9cWkk4yJaNmdzoZDqo0IIdibm8O22JB68qDvuZ3lf3Ls/\n2szOpBxWPz6melhJr36V7F9fxIdccPSA0jyKhs+iz/K+hPu5cXnBF9xX/iFZeBrnRF1tjKkvfxZy\nEgAo6H83MRtGY1LGE5IWZAAACR9JREFUArKv7x3Gz7tTef33A2z5+0VnXEL9QiPVR4UQTeod6tXi\nYaXmGtrVj6W7UjiWWVg93LSn821cURzG/GHZDC/6DXzCcBk3i+sLdhnlOzrdyua8XHxS17Nr8PMM\nn3S9cbFe18C6N+D4NmZXXEdFZSqL7h3KjA83M/PTrTg7mIkO9ZYg0EISCIQQNjXUOk+w5mBGdSD4\nfW8qZTjQY/Q08Li1+tynrujFU1f0AkDr9xn/8grc4y1UVz6yuMDIRzmRV8z7zy3j6pgQokO9eXlq\nH26ctwGtYeZY2ae5pWSOQAhhU10C3AjydGLNIWOeIL+knK+2JtEn1KvJshJKKW4e3IntCdnsSMyu\nc+ydlYcpq6isLsM+tIs/91gXxY06W5U/7YgEAiGETSmlGNbFn3WHMigpr+CeBZs5mlHIgxedervy\nq/uH4upoZsH6o9WPpeeXsGD9Ma7sG0JYrTUNj0zowbf3DTtl/SdxMpsGAqXUEaXUTqXUNqXUSTO8\nSikvpdRipdR2pdQupdRttmyPEKJtDO3qT2ZBKTfP+4NVB9L579W9GdOMd+6ezpb/b+9eg62s6jiO\nf39BErcEDRg5yEUgjJoAcYyiGpAmL2XygsYKzHGYYRqY0mpKnW5TvWqmyWrGBEdLLLyMBMHYjKOS\ng+MLQTRCBC8IogdQaBDMLnLx34u1aLb7nM317LPjWb/PzJ6zn/Us9l6L/zn7v5/1PM9azJzUxor1\nO9n/r4Ps2Pdv5i5ex9uHDjN/+ruHgHqc5ASH1j3nCKZHxN8b7FsAbIqIKyQNAp6XtCQiDjSob2an\noalj0sRua1/ey3cuGddh5b6jmfOxEdy95hV+uHIjj72wh4OHg9/MnsyYwSdx45d1qtUniwPor3RN\nWT9gL9D5VIhmdto658zeXDFhKCPP7sP8aSc2t/74oe/nwhEDWbF+Jx8c0o+FcyZ3mE3VTk1T7yOQ\ntA14g/SBvygibqvb3x9YCZwP9Aeuiog/d/I684B5AMOHD5+8ffv2+ipmVmEb2vfx0LOvM3/66GPO\nDmudO9p9BM1OBG0RsUPSYOBh4OsR8VjN/lnAVOBbwOhcZ0JEvNnoNX1DmZnZiTtaImjqyeKI2JF/\n7gaWAxfVVbkWWBbJFmAb6ejAzMy6SdMSgaS+eegHSX2BzwIb66q9AszIdYYA44CtzWqTmZl11MzB\ntiHA8jy3SE/g7oh4UNLXACJiIfBT4E5JzwACbjjKFUZmZtYETUsEEbEVmNBJ+cKa5ztJRwpmZtYi\nvrPYzKxwTgRmZoVzIjAzK5wTgZlZ4U67Fcok7QFO9tbiDwAlXpVUYr9L7DOU2e8S+wwn3u8RETGo\nsx2nXSI4FZLWNbqzrspK7HeJfYYy+11in6Fr++2hITOzwjkRmJkVrrREcNuxq1RSif0usc9QZr9L\n7DN0Yb+LOkdgZmYdlXZEYGZmdZwIzMwKV0wikHSppOclbZF0Y6vb0wySzpX0qKRNkp6VdF0uP0vS\nw5JezD8HtrqtzSCph6S/Snogb4+StCbH/D5JZ7S6jV1J0gBJSyU9J2mzpI+XEGtJ38y/3xsl3SPp\nfVWMtaTfStotaWNNWafxVfLr3P8Nki44kfcqIhFI6gHcAlwGjAe+LGl8a1vVFIeAb0fEeGAKsCD3\n80ZgVUSMBVbl7Sq6Dthcs/0z4OaIGENaMnVuS1rVPL8CHoyI80kz/W6m4rGW1AZ8A7gwIj4C9AC+\nRDVjfSdwaV1Zo/heBozNj3nArSfyRkUkAtLKaFsiYmtEHADuBa5scZu6XETsioin8/N/kD4Y2kh9\nXZyrLQZmtqaFzSNpGPA54Pa8LeBiYGmuUql+SzoT+DRwB0BEHIiIfRQQa9L0+b0l9QT6ALuoYKzz\nsr5764obxfdK4K682uMTwABJ5xzve5WSCNqAV2u223NZZUkaCUwC1gBDImJX3vUaadGgqvkl8F3g\nnbx9NrAvIg7l7arFfBSwB/hdHg67Pa8EWOlY5+Vvf05a3XAXsB94imrHulaj+J7SZ1wpiaAokvoB\nfwSuj4g3a/dFul64UtcMS/o8sDsinmp1W7pRT+AC4NaImAT8k7phoIrGeiDp2+8oYCjQl47DJ0Xo\nyviWkgh2AOfWbA/LZZUj6b2kJLAkIpbl4tePHCbmn7tb1b4mmQp8QdLLpGG/i0nj5wPy8AFUL+bt\nQHtErMnbS0mJoeqx/gywLSL2RMRBYBkp/lWOda1G8T2lz7hSEsGTwNh8ZcEZpJNLK1vcpi6Xx8Xv\nADZHxC9qdq0ErsnPrwFWdHfbmikiboqIYRExkhTbv0TEbOBRYFauVql+R8RrwKuSxuWiGcAmKh5r\n0pDQFEl98u/7kX5XNtZ1GsV3JfDVfPXQFGB/zRDSsUVEEQ/gcuAF4CXge61uT5P6+EnSoeIGYH1+\nXE4aL18FvAg8ApzV6rY28f9gGvBAfn4esBbYAtwP9Gp1+7q4rxOBdTnefwIGlhBr4MfAc8BG4PdA\nryrGGriHdB7kIOkIcG6j+AIiXRn5EvAM6aqq434vTzFhZla4UoaGzMysAScCM7PCORGYmRXOicDM\nrHBOBGZmhXMiMOtGkqYdmR3V7P+FE4GZWeGcCMw6IWmOpLWS1ktalNc6eEvSzXku/FWSBuW6EyU9\nkeeBX14zR/wYSY9I+pukpyWNzi/fr2YdgSX5DlmzlnEiMKsj6UPAVcDUiJgIHAZmkyY4WxcRHwZW\nAz/K/+Qu4IaI+Cjprs4j5UuAWyJiAvAJ0l2ikGaFvZ60NsZ5pLlyzFqm57GrmBVnBjAZeDJ/We9N\nmtzrHeC+XOcPwLK8LsCAiFidyxcD90vqD7RFxHKAiPgPQH69tRHRnrfXAyOBx5vfLbPOORGYdSRg\ncUTc9K5C6Qd19U52fpa3a54fxn+H1mIeGjLraBUwS9Jg+N86sSNIfy9HZrj8CvB4ROwH3pD0qVx+\nNbA60gpx7ZJm5tfoJalPt/bC7Dj5m4hZnYjYJOn7wEOS3kOa/XEBafGXi/K+3aTzCJCmA16YP+i3\nAtfm8quBRZJ+kl/ji93YDbPj5tlHzY6TpLciol+r22HW1Tw0ZGZWOB8RmJkVzkcEZmaFcyIwMyuc\nE4GZWeGcCMzMCudEYGZWuP8CZ6UZJoJkjm4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjr9R_wDjwHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training\n",
        "max_epochs = 100\n",
        "n_iter = 0\n",
        "\n",
        "# each 50 iterations we are going to compare the losses\n",
        "total_train_loss = []\n",
        "total_valid_loss = []\n",
        "\n",
        "for e in range(max_epochs):\n",
        "    model.train()\n",
        "    random.shuffle(train_data)\n",
        "    train_loss = []\n",
        "    for b_train, batch in enumerate(batched_learning(train_data, batch_size=batch_size)):        \n",
        "        \n",
        "        n_iter += 1\n",
        "        # train phase\n",
        "        # feed data into the network and get outputs.\n",
        "        inputs, target = batch\n",
        "        \n",
        "        #print(inputs.shape, target.shape)\n",
        "\n",
        "        # Train on GPU\n",
        "        inputs = (tensorFromSequence(inputs)).to(device)\n",
        "        target = (tensorFromSequence(target)).to(device)\n",
        "\n",
        "        # Create mask for both input and target sequences\n",
        "        input_mask, target_mask = create_masks(inputs, target, pad_token)\n",
        "\n",
        "        inputs = inputs[:,0,:]\n",
        "        target = target[:,0,:] \n",
        "\n",
        "        ys = target.contiguous().view(-1)              \n",
        "\n",
        "        # ys = labels.contiguous().view(-1).to(torch.long).to(device)\n",
        "        \n",
        "        # feed data into the network and get outputs.\n",
        "        preds_idx = model(inputs, target, input_mask, target_mask)\n",
        "        \n",
        "        # Flush out gradients computed at the previous step before computing gradients at the current step. \n",
        "        #       Otherwise, gradients would accumulate.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # calculate loss\n",
        "        loss = F.cross_entropy(preds_idx.contiguous().view(preds_idx.size(-1), -1).transpose(0,1), ys, ignore_index = pad_token, size_average = False) / (count_nonpad_tokens(ys))\n",
        "\n",
        "        # accumulates the gradient and backprogate loss.\n",
        "        loss.backward()\n",
        "\n",
        "        # performs a parameter update based on the current gradient\n",
        "        scheduler.step()    \n",
        "        \n",
        "        print('\\n====================================================')\n",
        "        print('Epoch/Batch: {}/{}'.format(e, b_train))\n",
        "        print('Train >>>> Loss: {:6.6}'.format(loss))\n",
        "\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "    print('\\n**************************************************')\n",
        "    print(\"\\n*** Test *** \")\n",
        "    \n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    valid_loss = []\n",
        "    with torch.no_grad():\n",
        "        pair = valid_data\n",
        "        inputs = tensorFromSequence(pair[0]).to(device)\n",
        "        target = tensorFromSequence(pair[1]).to(device)\n",
        "        \n",
        "        # Create mask for both input and target sequences\n",
        "        input_mask, target_mask = create_masks(inputs, target, pad_token)\n",
        "\n",
        "        inputs = inputs[:,0,:]\n",
        "        target = target[:,0,:] \n",
        "        ys = target.contiguous().view(-1)\n",
        "\n",
        "        preds_validate = model(inputs, target, input_mask, target_mask)\n",
        "        loss = F.cross_entropy(preds_validate.contiguous().view(preds_validate.size(-1), -1).transpose(0,1), ys, \\\n",
        "                                ignore_index = pad_token, size_average = False) / (count_nonpad_tokens(ys))\n",
        "        valid_loss.append(loss.item())\n",
        "\n",
        "    avg_train_loss = np.mean(train_loss)\n",
        "    avg_valid_loss = np.mean(valid_loss)\n",
        "\n",
        "    total_train_loss.append(avg_train_loss)\n",
        "    total_valid_loss.append(avg_valid_loss)\n",
        "\n",
        "    print(\"[Average Train Loss]: {:6.6}\".format(avg_train_loss))\n",
        "    print(\"[Average Testing Loss]: {:6.6}\".format(avg_valid_loss))\n",
        "\n",
        "    # save checkpoint whenever there is improvement in performance\n",
        "    if avg_valid_loss < best_loss:\n",
        "        best_loss = avg_valid_loss\n",
        "        # Note: optimizer also has states ! don't forget to save them as well.\n",
        "        ckpt = {'my_model':model.state_dict(),\n",
        "                'optimizer':optimizer.state_dict(),\n",
        "                'best_loss':best_loss}\n",
        "        torch.save(ckpt, ckpt_path)\n",
        "        print('checkpoint is saved !')\n",
        "\n",
        "    train_writer.add_scalar('loss/train', avg_train_loss, global_step=n_iter)\n",
        "    test_writer.add_scalar('loss/valid', avg_valid_loss, global_step=n_iter)\n",
        "    print('\\n**************************************************')\n",
        "    # torch.cuda.empty_cache()\n",
        "    # torch.save(model.state_dict(), '/gdrive/My Drive/my_data/library/checkpoints/train-{}.pt'.format(e))\n",
        "    ckpt = {'my_model':model.state_dict(),\n",
        "            'optimizer':optimizer.state_dict(),\n",
        "            'best_loss':best_loss}\n",
        "    torch.save(model.state_dict(), ckpt_backup_path)\n",
        "    torch.save(ckpt, ckpt_fullbackup_path)\n",
        "\n",
        "train_writer = SummaryWriter()\n",
        "test_writer = SummaryWriter()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm8OK2lRUtg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(total_train_loss, label='train loss')\n",
        "plt.plot(total_valid_loss, label='validation loss')\n",
        "plt.legend()\n",
        "\n",
        "img_path = os.path.join('/gdrive/My Drive/my_data/library/checkpoints/', \"losses_5-6.png\")\n",
        "plt.savefig(img_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKBnMBy_XfMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch =         \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fn6lBx9XlT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for batch in batched_learning(train_data, batch_size=batch_size):\n",
        "    inputs, target = batch\n",
        "    inputs = tensorFromSequence(inputs).to(device)\n",
        "    target = tensorFromSequence(target).to(device)\n",
        "\n",
        "    print(max(max((inputs[0]))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKSPCsLfXmmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l58u9TWLCsp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filename: Beam.py\n",
        "# Date Created: 15-Mar-2019 2:42:12 pm\n",
        "# Description: Functions used for beam search.\n",
        "\n",
        "def init_vars(src, model):\n",
        "    # outputs used for the decoder is the starting pitches\n",
        "    outputs = src\n",
        "\n",
        "    # encoder pass\n",
        "    src_mask = (src != pad_token).unsqueeze(-2).to(device)\n",
        "    e_output = model.encoder(src, src_mask)\n",
        "\n",
        "    # decoder pass\n",
        "    trg_mask = nopeak_mask(src.shape[1])\n",
        "    print(src_mask.size(), trg_mask.size())\n",
        "    out = model.decoder(outputs, e_output, src_mask, trg_mask)\n",
        "    out = model.linear(out)\n",
        "\n",
        "    # final fc layer\n",
        "    out = F.softmax(out, dim=-1)\n",
        "\n",
        "    # calculate probablites for beam search\n",
        "    # takes the last output from the model, hence out[:, -1]\n",
        "    probs, ix = out[:, -1].data.topk(3)\n",
        "    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n",
        "\n",
        "    # store the model outputs\n",
        "    outputs = torch.zeros(3, sequence_length).long().to(device)\n",
        "    outputs[:, 0:src.shape[1]] = src\n",
        "    outputs[:, src.shape[1]] = ix[0]\n",
        "\n",
        "    # store the encoder output to be used later\n",
        "    e_outputs = torch.zeros(3, e_output.size(-2),e_output.size(-1)).to(device)\n",
        "    e_outputs[:, :] = e_output[0]\n",
        "\n",
        "    return outputs, e_outputs, log_scores\n",
        "\n",
        "def k_best_outputs(outputs, out, log_scores, i, k):\n",
        "    # calculate probablities for each step in the sequence\n",
        "    probs, ix = out[:, -1].data.topk(k)\n",
        "    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0,1)\n",
        "    k_probs, k_ix = log_probs.view(-1).topk(k)\n",
        "\n",
        "    row = k_ix // k\n",
        "    col = k_ix % k\n",
        "\n",
        "    # update outputs\n",
        "    outputs[:, :i] = outputs[row, :i]\n",
        "    outputs[:, i] = ix[row, col]\n",
        "\n",
        "    log_scores = k_probs.unsqueeze(0)\n",
        "\n",
        "    return outputs, log_scores\n",
        "\n",
        "def beam_search(src, model):\n",
        "    outputs, e_outputs, log_scores = init_vars(src, model)\n",
        "    init_start_len = outputs.shape[0]\n",
        "    src_mask = (src != pad_token).unsqueeze(-2).to(device)\n",
        "\n",
        "    for i in range(init_start_len, sequence_length):\n",
        "        # Just comment this block of code if only use encoder once at the start\n",
        "        src_mask = (outputs[0,:i].unsqueeze(-2) != pad_token).unsqueeze(-2).to(device)\n",
        "        e_output = model.encoder(outputs[0,:i].unsqueeze(-2), src_mask)\n",
        "        e_outputs = torch.zeros(3, e_output.size(-2),e_output.size(-1)).to(device)\n",
        "        e_outputs[:, :] = e_output[0]\n",
        "        print(outputs.size())\n",
        "        trg_mask = nopeak_mask(i, )\n",
        "        out = model.linear(model.decoder(outputs[:,:i], e_outputs, src_mask, trg_mask))\n",
        "        out = F.softmax(out, dim=-1)\n",
        "\n",
        "        outputs, log_scores = k_best_outputs(outputs, out, log_scores, i, 3)\n",
        "\n",
        "    # return the one with the largest log_scores\n",
        "    return outputs[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aENrd6Lcbi0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filename: Process.py\n",
        "# Date Created: 17-Mar-2019 5:43:02 pm\n",
        "# Description: Functions used for basic processes.\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "def IndexToPitch(input, vocab):\n",
        "    \"\"\"\n",
        "    Converts the index values from model's output back to pitches from vocab.\n",
        "    \"\"\"\n",
        "    index_vocab = np.arange(len(vocab))\n",
        "    output = input.clone()\n",
        "\n",
        "    for i, val in reversed(list(enumerate(index_vocab))):\n",
        "        output[output==val] = vocab[i]\n",
        "\n",
        "    return output\n",
        "\n",
        "def ProcessModelOutput(model_output):\n",
        "    \"\"\"\n",
        "    Remove custom tokens and set rest tokens to NaN values\n",
        "    Converts the model's output into numpy format similar to JSB dataset.\n",
        "    \"\"\"\n",
        "    # Values for the custom tokens\n",
        "    rest_token = 0\n",
        "    pad_token = 1\n",
        "    sos_token = 2\n",
        "    eos_token = 3\n",
        "\n",
        "    # Convert tensor to numpy\n",
        "    output = model_output.cpu().detach().numpy()\n",
        "\n",
        "    # Replace all pad tokens with rest tokens\n",
        "    output[output==pad_token] = rest_token\n",
        "\n",
        "    # Change rest tokens to NaN values\n",
        "    output = np.where(output==rest_token, np.nan, output)\n",
        "\n",
        "    # Reshape output to match JSB dataset\n",
        "    output = output.reshape(round(output.shape[0]/4),4)\n",
        "\n",
        "    return output\n",
        "\n",
        "def get_len(train):\n",
        "    for i, b in enumerate(train):\n",
        "        pass\n",
        "\n",
        "    return i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XJlesoxZUdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate(model):\n",
        "    print(\"generating music using beam search...\")\n",
        "    model.eval()\n",
        "\n",
        "    # choose 2 random pitches within the vocab (except rest/pad token) to start the sequence\n",
        "    starting_pitch = torch.randint(2, len(vocab)-1, (2,)).unsqueeze(1).transpose(0,1).to(device)\n",
        "\n",
        "    # generate the sequence using beam search\n",
        "    generated_seq = beam_search(starting_pitch, model)\n",
        "\n",
        "    # Make the index values back to original pitch\n",
        "    output_seq = IndexToPitch(generated_seq, vocab)\n",
        "\n",
        "    # Process the output format such that it is the same as our dataset\n",
        "    processed = ProcessModelOutput(output_seq)\n",
        "\n",
        "    return processed\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewrQBaAzZeV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqxc-lp89_kS",
        "colab_type": "code",
        "outputId": "925c0a7b-9309-4f8e-9db6-3153bd765bff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "d_model = 512\n",
        "nhead = 8\n",
        "dim_feedforward = 2048\n",
        "dropout = 0.2\n",
        "num_layer = 6\n",
        "batch_size = 8\n",
        "sequence_length = 1024\n",
        "warmup_steps = 4000\n",
        "pad_token = 1   \n",
        "vocabulary_size = 48\n",
        "\n",
        "normalization = torch.nn.LayerNorm(d_model)\n",
        "\n",
        "custom_encoder_layer = MusicTransformerEncoderLayer(d_model=d_model, nhead=nhead, \n",
        "                                               dim_feedforward=dim_feedforward, \n",
        "                                               dropout=dropout, activation=\"relu\")\n",
        "\n",
        "custom_decoder_layer = MusicTransformerDecoderLayer(d_model=d_model, nhead=nhead, \n",
        "                                               dim_feedforward=dim_feedforward, \n",
        "                                               dropout=dropout, activation=\"relu\")\n",
        "\n",
        "custom_encoder = MusicTransformerEncoder(custom_encoder_layer, vocabulary_size, num_layer, normalization)\n",
        "custom_decoder = MusicTransformerDecoder(custom_decoder_layer, vocabulary_size, num_layer, normalization)\n",
        "\n",
        "model = MusicTransformer(d_model=d_model, nhead=nhead, \n",
        "                         vocabulary_size=vocabulary_size, \n",
        "                         num_encoder_layers=num_layer, \n",
        "                         num_decoder_layers=num_layer, \n",
        "                         dim_feedforward=dim_feedforward, \n",
        "                         dropout=dropout, activation=\"relu\", \n",
        "                         custom_encoder=custom_encoder, \n",
        "                         custom_decoder=custom_decoder)\n",
        "# Give model to the current device (hopefully cuda)\n",
        "model.to(device)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MusicTransformer(\n",
              "  (encoder): MusicTransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): MusicTransformerEncoderLayer(\n",
              "        (self_attn): MusicMultiheadAttention(\n",
              "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): MusicTransformerEncoderLayer(\n",
              "        (self_attn): MusicMultiheadAttention(\n",
              "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): MusicTransformerEncoderLayer(\n",
              "        (self_attn): MusicMultiheadAttention(\n",
              "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): MusicTransformerEncoderLayer(\n",
              "        (self_attn): MusicMultiheadAttention(\n",
              "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): MusicTransformerEncoderLayer(\n",
              "        (self_attn): MusicMultiheadAttention(\n",
              "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): MusicTransformerEncoderLayer(\n",
              "        (self_attn): MusicMultiheadAttention(\n",
              "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (embedding): Embedding(48, 512)\n",
              "  )\n",
              "  (decoder): MusicTransformerDecoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): MusicTransformerDecoderLayer(\n",
              "        (self_attn): MusicMultiheadAttention(\n",
              "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): MusicTransformerDecoderLayer(\n",
              "        (self_attn): MusicMultiheadAttention(\n",
              "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): MusicTransformerDecoderLayer(\n",
              "        (self_attn): MusicMultiheadAttention(\n",
              "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): MusicTransformerDecoderLayer(\n",
              "        (self_attn): MusicMultiheadAttention(\n",
              "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): MusicTransformerDecoderLayer(\n",
              "        (self_attn): MusicMultiheadAttention(\n",
              "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): MusicTransformerDecoderLayer(\n",
              "        (self_attn): MusicMultiheadAttention(\n",
              "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (weights_o): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (embedding): Embedding(48, 512)\n",
              "  )\n",
              "  (fc): Linear(in_features=512, out_features=48, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE3sTSjg-aI6",
        "colab_type": "code",
        "outputId": "db320309-99db-4cfc-d3be-f46fc7876a00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_name = '6-4'\n",
        "ckpt_path = '/gdrive/My Drive/my_data/library/checkpoints/train-nlayer_'+model_name+'.pt'\n",
        "ckpt_backup_path = '/gdrive/My Drive/my_data/library/checkpoints/train-backup-nlayer_'+model_name+'.pt'\n",
        "ckpt_fullbackup_path = '/gdrive/My Drive/my_data/library/checkpoints/train-fullbackup-nlayer_'+model_name+'.pt'\n",
        "if os.path.exists(ckpt_path):\n",
        "    ckpt = torch.load(ckpt_path)\n",
        "    try:\n",
        "      model.load_state_dict(ckpt['my_model'])\n",
        "      optimizer.load_state_dict(ckpt['optimizer'])\n",
        "      best_acc = ckpt['best_loss']\n",
        "    except RuntimeError as e:\n",
        "        print('wrong checkpoint')\n",
        "    else:    \n",
        "      print('checkpoint is loaded !')\n",
        "      print('current best loss : %.2f' % best_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wrong checkpoint\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLzjioOU-BRR",
        "colab_type": "code",
        "outputId": "bc17aafd-0836-4a28-f909-01804236d923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "# Generate the vocabulary from the data\n",
        "vocab = GenerateVocab(src_data)\n",
        "pad_token = 1\n",
        "\n",
        "# counter to keep track of how many outputs have been saved\n",
        "save_counter = 0\n",
        "\n",
        "# Now lets generate some music\n",
        "generated_music = generate(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generating music using beam search...\n",
            "torch.Size([1, 2])\n",
            "torch.Size([1, 1, 2]) torch.Size([1, 2, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-cc7887b921bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Now lets generate some music\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgenerated_music\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-9f85554440a8>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstarting_pitch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# generate the sequence using beam search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mgenerated_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstarting_pitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Make the index values back to original pitch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-d07c00971cae>\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(src, model)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0minit_start_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpad_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-d07c00971cae>\u001b[0m in \u001b[0;36minit_vars\u001b[0;34m(src, model)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrg_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnopeak_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-2282d53f2add>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    140\u001b[0m         return super().forward(tgt, memory, tgt_mask=tgt_mask, \n\u001b[1;32m    141\u001b[0m                 \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 memory_key_padding_mask=memory_key_padding_mask)\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    226\u001b[0m                                     \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                                     \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                                     memory_key_padding_mask=memory_key_padding_mask)\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n\u001b[0;32m--> 355\u001b[0;31m                                    key_padding_mask=memory_key_padding_mask)[0]\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    781\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   3350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mattn_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3351\u001b[0m         \u001b[0mattn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3352\u001b[0;31m         \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3354\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkey_padding_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: output with shape [16, 1, 1] doesn't match the broadcast shape [1, 16, 2, 2]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIYg2W2b-6_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}